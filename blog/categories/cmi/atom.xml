<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: cmi | Zarrar's Journaling]]></title>
  <link href="http://czarrar.github.io/blog/categories/cmi/atom.xml" rel="self"/>
  <link href="http://czarrar.github.io/"/>
  <updated>2013-12-21T18:38:40-05:00</updated>
  <id>http://czarrar.github.io/</id>
  <author>
    <name><![CDATA[Zarrar Shehzad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Derivative Class]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/20/derivative-class/"/>
    <updated>2013-12-20T21:36:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/20/derivative-class</id>
    <content type="html"><![CDATA[<p><strong>Mission:</strong><br />
» Refractor current derivative workflows in <code>cpac_pipeline.py</code> (if you choose to accept it)</p>

<p><strong>Why:</strong><br />
» Easier to read code<br />
» Easier to implement quick packs
» Testing should be much more straightforward
» Potential for an extension-like system for users to integrate their own derivatives in the future (plug-in-play)</p>

<p>I personally have two objectives with these changes based on working with the ABIDE dataset. First, it is to make it easier to use other preprocessed data as inputs to CPAC. Second, it is to add a new derivative (functional density mapping; Tomasi et al., 2010) to the current pipeline system.</p>

<h2 id="how-does-it-currently-work-using-reho-as-an-example">How does it currently work? Using REHO as an example.</h2>

<p>I will be focusing on the <code>CPAC/pipeline/cpac_pipeline.py</code> file. This is a bit of a gargantuan file and contains both high-level (in terms of the pipeline) and low-level code, so it can get a bit confusing to go through. With that said, having everything in one place like this also makes it easier to go through after you know your way around.</p>

<p>I’ll skip past most of this code till about line 1796<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> where the REHO workflow starts. Then, I’ll go line by line (somewhat) to understand what’s happening until line 1833 when this part of the REHO code ends. Let’s begin shall we? I might suggest a good whiskey, it goes well while reading certain code.</p>

<h3 id="getting-our-toes-wet">Getting our toes wet</h3>

<p><code>python
new_stops_list = []
new_strat_list = []
num_strat = 0
</code></p>

<p>The <code>num_strat</code> stores the current strategy number (or index in a list, in this case). A strategy is one while run of the pipeline (i think), and anytime there is a fork in the pipeline (e.g., using versus not using bandpass filtering), a new strategy is created. This is all determined before as the data is preprocessed.</p>

<p>And it seems like the <code>new_stops_list</code> and <code>new_strat_list</code> do not get used in this section. So kindly ignore them. The <code>new_strat_list</code> is usually for when you add a new strategy to the list (i.e., there is a fork in the pipeline).</p>

<h3 id="getting-our-feet-wet">Getting our feet wet</h3>

<p>Ok so we haven’t even scratched the surface yet. Now I’ll throw all the code at you here for this section and then go through it gradually.</p>

<p>``` python
if 1 in c.runReHo:
    for i,strat in enumerate(strat_list):</p>

<pre><code>    preproc = create_reho()
    preproc.inputs.inputspec.cluster_size = c.clusterSize
    reho = preproc.clone('reho_%d' % num_strat)

    try:
        node, out_file = strat.get_leaf_properties()
        workflow.connect(node, out_file,
                         reho, 'inputspec.rest_res_filt')

        node, out_file = strat.get_node_from_resource_pool('functional_brain_mask')
        workflow.connect(node, out_file,
                         reho, 'inputspec.rest_mask')
    except:
        print 'Invalid Connection: REHO:', num_strat, ' resource_pool: ', strat.get_resource_pool()
        raise
</code></pre>

<p>strat.update_resource_pool({‘raw_reho_map’:(reho, ‘outputspec.raw_reho_map’)})
        strat.update_resource_pool({‘reho_Z_img’:(reho, ‘outputspec.z_score’)})
        strat.append_name(reho.name)</p>

<pre><code>    create_log_node(reho, 'outputspec.raw_reho_map', num_strat)
    
    num_strat += 1
</code></pre>

<p>```</p>

<h4 id="lines-1-2">Lines 1-2</h4>

<p><code>python
if 1 in c.runReHo:
    for i,strat in enumerate(strat_list):
</code></p>

<p>For all derivatives, we usually start with an if statement that checks if the user wants to run this particular derivative. Then we loop through each strategy to access the preprocessed functional data. The node information for these files are kept in the <code>strat_list</code>, which is a list of strategy objects. </p>

<h4 id="lines-4-6">Lines 4-6</h4>

<p><code>python
        preproc = create_reho()
        preproc.inputs.inputspec.cluster_size = c.clusterSize
        reho = preproc.clone('reho_%d' % num_strat)
</code></p>

<p>This is a little confusing but <code>preproc</code> is actually a workflow object for running REHO. It needs one input cluster size for the number of neighboring voxels to examine when measuring the regional homogeneity of the time-series. We then clone this reho workflow so that we can have a unique one for each reho named <code>'reho_%d' % num_strat</code>.</p>

<p>Maybe you like me are wondering why you can’t pass the cluster size and the name of the workflow as an argument to <code>create_reho</code>, which would eliminate those two additional lines. Not sure.</p>

<h4 id="lines-9-15">Lines 9-15</h4>

<p>``` python</p>

<p>node, out_file = strat.get_leaf_properties()
workflow.connect(node, out_file, reho, ‘inputspec.rest_res_filt’)</p>

<p>node, out_file = strat.get_node_from_resource_pool(‘functional_brain_mask’)
workflow.connect(node, out_file, reho, ‘inputspec.rest_mask’)</p>

<p>``` </p>

<p>One key step to any derivative is it’s inputs. As I mentioned before the <code>start</code> object holds the node information for the preprocessed functional data. First, we get the filtered functional data <code>inputspec.rest_res_filt</code>. Then, we get the functional brain mask. We will extract REHO estimates from our functional data within our brain mask.</p>

<p>Now you might be wondering, what is the difference between <code>strat.get_leaf_properties()</code> vs. <code>strat.get_node_from_resource_pool('functional_brain_mask')</code>? The former (for the time series) is when one node and output name get added to the <code>start</code> object as a leaf. It appears that there is only one leaf of it’s kind and this is used specifically for functional preprocessed data. This file can be the functional data at different stages of preprocessing depending on either the stage in the pipeline or the preferences of the user. This could be the file preprocessed data, preprocessed + nuisance regression, or preprocess + nuisance regression + filtering. The latter (for the brain mask) is a bit more straightforward and simply gets the information for ‘functional_brain_mask’ in the strat object (an element in a dictionary).</p>

<h4 id="lines-20-24">Lines 20-24</h4>

<p>``` python
        strat.update_resource_pool({‘raw_reho_map’:(reho, ‘outputspec.raw_reho_map’)})
        strat.update_resource_pool({‘reho_Z_img’:(reho, ‘outputspec.z_score’)})
        strat.append_name(reho.name)</p>

<pre><code>    create_log_node(reho, 'outputspec.raw_reho_map', num_strat)
</code></pre>

<p>```</p>

<p>These functions all have to do with adding the files generated by our reho workflow into the <code>start</code> object. Specifically, we will be adding the raw reho map and the Z-score transformed reho map. The last line here is related to keeping a log of this REHO workflow by creating a new log node.</p>

<p>Also I have no idea what the <code>strat.append_name</code> is about. Will need to find out.</p>

<h4 id="lines-26-27">Lines 26-27</h4>

<p><code>python
        num_strat += 1
stops_list += new_stops_list
strat_list += new_strat_list
</code></p>

<p>I’ve sort-of gone through these lines before so I’ll spare you the repetition.</p>

<h2 id="improving-this-code">Improving this code?</h2>

<p>I picked a pretty easy derivative. I mentioned some avenues of improvement. The function that generates the derivative workflow should be able to take in arguments. The <code>get_leaf_properties</code> is a little confusing and should be replaced.</p>

<p>The other avenue of improvement is the possibility that much of this is repeated in the other derivatives. You have some input functional data from the <code>strat</code> object and you need to save some output data into the <code>strat</code> object. We should be able to create an abstract <code>Derivative</code> class.</p>

<p>So we take in some inputs that are in the strat class and we give them to the reho class.</p>

<p>Below is a very rough and quick switch of this process.</p>

<p>``` python
class Derivative(object):
    “"”For adding a derivative to your workflow”””
    def <strong>init</strong>(self, fun, workflow, strat, <em>args, **kwrds):
        super(Derivative, self).<strong>init</strong>()
        self.workflow = workflow
        self.strat = strat
        self.args = args
        self.kwrds = kwrds
        self.deriv = func(</em>args, **kwrds)</p>

<pre><code>def connect_inputs(**kwrds):
    for deriv_in,strat_out in kwrds.iteritems():
        node,out_file = self.strat.get_node_from_resource_pool(strat_out)
        self.workflow.connect(node, out_file, 
                              self.deriv, 'inputspec.%s' % deriv_in)

def connect_outputs(**kwrds):
    for strat_in,deriv_out in kwrds.iteritems():
        strat.update_resource_pool({strat_in: (self.deriv, deriv_out)})
</code></pre>

<p>```</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The exact line number (1796) will vary depending on your version. Just search ‘Inserting REHO’ and you’ll be golden.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DPARSF Quick Pack]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/18/dparsf-quick-pack/"/>
    <updated>2013-12-18T17:14:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/18/dparsf-quick-pack</id>
    <content type="html"><![CDATA[<p>I want to first get down what files from the DPARSF preprocessed output correspond to inputs needed by CPAC for running the derivatives.</p>

<p>A big thing here is that I am running DPARSF in native space. Then on the this derivative data, I would normalize it using DARTELS. And I guess then apply smoothing using DPARSF. How to I apply smoothing with Chao-Gan’s script (emailed him to find out)?</p>

<h2 id="naming-conventions">Naming Conventions</h2>

<p><code>
A=slice timing
R=renormalization
C=covariate regressed
W=spatially normalized
S=smoothed
F=filtered
Sym=for VMH
global=GSR
</code></p>

<h2 id="anatomical">Anatomical</h2>

<p>I don’t need any of these since they are only for registration</p>

<h2 id="functional">Functional</h2>

<ul>
  <li>functional_brain_mask</li>
  <li>functional_nuisance_residuals</li>
  <li>functional_freq_filtered</li>
  <li></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CPAC Derivative Inputs]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/17/cpac-derivative-inputs/"/>
    <updated>2013-12-17T19:24:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/17/cpac-derivative-inputs</id>
    <content type="html"><![CDATA[<p>I want to try to organize what inputs are needed for what derivates for CPAC. First, I have the list of template inputs for quick pack for the CCS pipeline:</p>

<p>``` python
anat_suffixes = {
    “anatomical_brain”: “mprage_sanlm.nii.gz”,     # check
    “anatomical_reorient”: “reg/highres_rpi.nii.gz”, 
    “anatomical_to_mni_nonlinear_xfm”: “reg/highres2standard_warp.nii.gz”, 
    “mni_normalized_anatomical”: “reg/fnirt_highres2standard.nii.gz”, 
}</p>

<p>func_suffixes = {
    “preprocessed”: “rest_res.nii.gz”, 
    “mean_functional”: “rest_pp_mean.nii.gz”, 
    “functional_brain_mask”: “rest_pp_mask.nii.gz”, 
    “functional_nuisance_residuals”: “rest_pp_nofilt_sm0.nii.gz”,<br />
    “functional_freq_filtered”: [”%(strategy)srest_pp<em>%(pipeline)s_sm0.nii.gz”, “%(strategy)s/rest_pp</em>%(pipeline)s_sm0.nii.gz”],
    “functional_mni”: [”%(strategy)srest.%(pipeline)s.sm0.mni152.nii.gz”, “%(strategy)s/rest.%(pipeline)s.sm0.mni152.nii.gz”],
    “functional_brain_mask_to_standard”: “” # I would need to make this myself
}</p>

<p>reg_suffixes = {
    “functional_to_anat_linear_xfm”: “reg/example_func2highres.mat” 
#    “functional_to_mni_linear_xfm”: “functional_to_mni_linear_xfm.mat”
} </p>

<p>```</p>

<p>Now I go through each of the files to see if they are really needed. If not, then I will want to make sure that it is also processed as optional in my ghetto quick pack code.</p>

<h2 id="anatomicals">Anatomicals</h2>

<h3 id="anatomicalbrain">anatomical_brain</h3>

<p><strong>Yes</strong> it is needed for <strong>all the steps</strong> but <strong>only with ANTS</strong>. It is used by the applywarp sections of code and I think particularly for transforming FSL’s registration matrix to ANTs format (again i think).</p>

<h3 id="anatomicalreorient">anatomical_reorient</h3>

<p>Only <strong>VMHC?</strong>.</p>

<h3 id="anatomicaltomninonlinearxfm">anatomical_to_mni_nonlinear_xfm</h3>

<p><strong>Yes</strong> for <strong>all the steps</strong> but <strong>only for applying the registration</strong>.</p>

<h3 id="mninormalizedanatomical">mni_normalized_anatomical</h3>

<p><strong>No</strong>. This is only used in QC and so this can be removed from the set of inputs that I generate.</p>

<h2 id="functional">Functional</h2>

<h3 id="preprocessed">preprocessed</h3>

<p><strong>No</strong>. This is used in scrubbing and QC so it too can be removed from the set of inputs that I generate.</p>

<h3 id="meanfunctional">mean_functional</h3>

<p><strong>No but in VMHC</strong> it is used. Not sure how long that will last.</p>

<h3 id="functionalbrainmask">functional_brain_mask</h3>

<p><strong>Yes</strong> it is used in <strong>many (all?) steps</strong>.</p>

<h3 id="functionalnuisanceresiduals">functional_nuisance_residuals</h3>

<p><strong>Yes</strong> for <strong>ALFF and fALFF</strong>. Note that this uses the set_leaf_properties and get_leaf_properties function to dynamically get so it won’t be so obvious. This option can also be used for the other derivatives if you don’t want them to be filtered.</p>

<h3 id="functionalfreqfiltered">functional_freq_filtered</h3>

<p><strong>Yes</strong> for <strong>everything where want filtering</strong>.</p>

<h3 id="functionalmni">functional_mni</h3>

<p><strong>Yes</strong> for <strong>Spatial Regression, ROI time series, Voxel time series, Temporal Regression for SCA, Network Centrality</strong></p>

<h3 id="functionalbrainmasktostandard">functional_brain_mask_to_standard</h3>

<p><strong>Yes</strong> but <strong>only when apply registration</strong>. Although should also need it for the above ones…</p>

<h2 id="final">Final</h2>

<p>Ok so the final list, here again. Note that I didn’t discuss the reg one.</p>

<p>``` python
anat_suffixes = {
    “anatomical_brain”: “mprage_sanlm.nii.gz”,          # REGISTRATION
    “anatomical_reorient”: “reg/highres_rpi.nii.gz”,    # VMHC?
    “anatomical_to_mni_nonlinear_xfm”: “reg/highres2standard_warp.nii.gz”,  # REGISTRATION
#    “mni_normalized_anatomical”: “reg/fnirt_highres2standard.nii.gz”,      # NOT NEEDED 
}</p>

<p>func_suffixes = {
#    “preprocessed”: “rest_res.nii.gz”,             # NOT NEEDED
#    “mean_functional”: “rest_pp_mean.nii.gz”,      # VMHC?
    “functional_brain_mask”: “rest_pp_mask.nii.gz”, # YES 
    “functional_nuisance_residuals”: “rest_pp_nofilt_sm0.nii.gz”,  # YES (although not really used)
    “functional_freq_filtered”: [”%(strategy)srest_pp<em>%(pipeline)s_sm0.nii.gz”, “%(strategy)s/rest_pp</em>%(pipeline)s_sm0.nii.gz”],    # YES
    “functional_mni”: [”%(strategy)srest.%(pipeline)s.sm0.mni152.nii.gz”, “%(strategy)s/rest.%(pipeline)s.sm0.mni152.nii.gz”],      # YES
    “functional_brain_mask_to_standard”: “”                                                                                         # Yes
}</p>

<p>reg_suffixes = {
    “functional_to_anat_linear_xfm”: “reg/example_func2highres.mat” # REGISTRATION 
#    “functional_to_mni_linear_xfm”: “functional_to_mni_linear_xfm.mat”
} </p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Week 50 - Friday]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/13/week-50-friday/"/>
    <updated>2013-12-13T12:01:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/13/week-50-friday</id>
    <content type="html"><![CDATA[<p>There’s been a bit of a lull in writing these entries. Hoping to get back on the train.</p>

<p><strong>What are the dominant issues for today?</strong></p>

<ul>
  <li>
    <p>I found that yesterday that the fast eigencentrality code doesn’t work when the inputs are normalized data (mean=0, sd=1). I am not totally sure what’s going on but I can try to use the matlab code directly to see if my python port is missing something. I might first try with random data and then with some imaging data if possible. The imaging data could be used to directly compare with the python code. I might keep these test code blocks as gists.</p>
  </li>
  <li>
    <p>I should make a table of all the processing steps that are needed for the ABIDE dataset. I want to run some of these things in the background as I can so as not to pile up.</p>
  </li>
</ul>

<h2 id="testing-fast-eigenvector-code">Testing Fast Eigenvector Code</h2>

<p>When I ran both the python and the fast code without the normalization of the time-series. I get the same results in both datasets. However, with normalization of the timeseries, I get different results. One suggestion from Steve was that there might be a difference in some normalization.</p>

<p>Details of my simplified comparison can be found on https://gist.github.com/czarrar/7950474 and at the bottom of the post.</p>

<h2 id="abide">ABIDE</h2>

<p>Let’s first make a table and workflow explaining everything. I’ll add it here first and then move it to another spot later.</p>

<p>I was able to start the processing for degree centrality with our dataset. I should be able to…</p>

<h2 id="gist">Gist</h2>

<p><div><script src='https://gist.github.com/7950474.js'></script>
<noscript><pre><code>import numpy as np
from CPAC.cwas.subdist import norm_cols
from fast_ecm import fast_eigenvector_centrality # this is from the other gist; can ignore this and paste in the other function

print 'Compare with non-normalized matrices'

m  = np.random.random((200,1000))
cm = m.T.dot(m)

# Let's first call a basic approach
# This actually doesn't work, not sure what I'm setting wrong
from scipy import linalg as LA
w01,v01 = LA.eigh(cm, eigvals=(0,0))
e01     = cm.dot(np.abs(v01))/w01[0]

# Let's call a second basic approach (used currently)
from scipy.sparse import linalg as sLA
w02,v02 = sLA.eigsh(cm, k=1, which='LM', maxiter=1000)
e02     = cm.dot(np.abs(v02))/w02[0]

# Finally let's call the fast eigenvector (power approach)
v03     = fast_eigenvector_centrality(m, verbose=False)

# How different are the second and third ones?
print 'mean absolute diff: ', np.abs(e02-v03).mean()
print 'correlation: ', np.corrcoef(e02.T, v03.T)[0,1]



print 'Compare with normalized matrices'

n  = norm_cols(m)
cn = n.T.dot(n)

# Let's first call a basic approach
from scipy import linalg as LA
w11,v11 = LA.eigh(cn, eigvals=(0,0))
e11     = cn.dot(np.abs(v11))/w11[0]

# Let's call a second basic approach (used currently)
from scipy.sparse import linalg as sLA
w12,v12 = sLA.eigsh(cn, k=1, which='LM', maxiter=1000)
e12     = cn.dot(np.abs(v12))/w12[0]

# Finally let's call the fast eigenvector (power approach)
v13     = fast_eigenvector_centrality(n, verbose=False)

# How different are the second and third ones?
print 'mean absolute diff: ', np.abs(e12-v13).mean()
print 'correlation: ', np.corrcoef(e12.T, v13.T)[0,1]
</code></pre></noscript></div>
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Week 50 - Monday]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/09/week-50-monday/"/>
    <updated>2013-12-09T20:03:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/09/week-50-monday</id>
    <content type="html"><![CDATA[<h2 id="plansummary">Plan/Summary</h2>

<ul>
  <li>[x] Email Maarten with the results</li>
  <li>[x] Add running Xinian’s filtering script to my Basecamp TODO</li>
  <li>[x] Integrate updated CWAS code into master CPAC branch</li>
  <li>[] Tagging (e 1 hr)</li>
  <li>[] QC (e 1 hr)</li>
  <li>[] Finish fast eigenvector centrality code (not workflow)</li>
</ul>

<p>While above were items that I had planned in the morning, items below reflect additions to this plan during the day.</p>

<ul>
  <li>[x] cleaned up the code to use regression test data to run a python CPAC and R version of CWAS.</li>
  <li>[x] wrote code to smooth 4D functional data in standard space</li>
  <li>[x] write code to use smoothed data for CWAS</li>
  <li>[x] write some additional code to use internal CPAC filepaths for CWAS</li>
  <li>[x] write code to parse group analysis model inputs for CWAS</li>
</ul>

<p>The coding for CWAS in CPAC went surprisingly well. Hopefully, this streak might continue tomorrow when testing.</p>

<h2 id="integrating-cwas-code-with-cpac">Integrating CWAS Code with CPAC</h2>

<p>My issue in using the main CPAC repository was that although I was a member, I needed to be made an owner. After figuring this out, I created a ‘cwas’ branch and added the relevant changes to the cwas folder.</p>
]]></content>
  </entry>
  
</feed>
