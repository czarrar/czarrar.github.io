<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: cmi | Zarrar's Journaling]]></title>
  <link href="http://czarrar.github.io/blog/categories/cmi/atom.xml" rel="self"/>
  <link href="http://czarrar.github.io/"/>
  <updated>2013-12-19T00:39:52-05:00</updated>
  <id>http://czarrar.github.io/</id>
  <author>
    <name><![CDATA[Zarrar Shehzad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[DPARSF Quick Pack]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/18/dparsf-quick-pack/"/>
    <updated>2013-12-18T17:14:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/18/dparsf-quick-pack</id>
    <content type="html"><![CDATA[<p>I want to first get down what files from the DPARSF preprocessed output correspond to inputs needed by CPAC for running the derivatives.</p>

<p>A big thing here is that I am running DPARSF in native space. Then on the this derivative data, I would normalize it using DARTELS. And I guess then apply smoothing using DPARSF. How to I apply smoothing with Chao-Gan’s script (emailed him to find out)?</p>

<h2 id="naming-conventions">Naming Conventions</h2>

<p><code>
A=slice timing
R=renormalization
C=covariate regressed
W=spatially normalized
S=smoothed
F=filtered
Sym=for VMH
global=GSR
</code></p>

<h2 id="anatomical">Anatomical</h2>

<p>I don’t need any of these since they are only for registration</p>

<h2 id="functional">Functional</h2>

<ul>
  <li>functional_brain_mask</li>
  <li>functional_nuisance_residuals</li>
  <li>functional_freq_filtered</li>
  <li></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CPAC Derivative Inputs]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/17/cpac-derivative-inputs/"/>
    <updated>2013-12-17T19:24:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/17/cpac-derivative-inputs</id>
    <content type="html"><![CDATA[<p>I want to try to organize what inputs are needed for what derivates for CPAC. First, I have the list of template inputs for quick pack for the CCS pipeline:</p>

<p>``` python
anat_suffixes = {
    “anatomical_brain”: “mprage_sanlm.nii.gz”,     # check
    “anatomical_reorient”: “reg/highres_rpi.nii.gz”, 
    “anatomical_to_mni_nonlinear_xfm”: “reg/highres2standard_warp.nii.gz”, 
    “mni_normalized_anatomical”: “reg/fnirt_highres2standard.nii.gz”, 
}</p>

<p>func_suffixes = {
    “preprocessed”: “rest_res.nii.gz”, 
    “mean_functional”: “rest_pp_mean.nii.gz”, 
    “functional_brain_mask”: “rest_pp_mask.nii.gz”, 
    “functional_nuisance_residuals”: “rest_pp_nofilt_sm0.nii.gz”,<br />
    “functional_freq_filtered”: [”%(strategy)srest_pp<em>%(pipeline)s_sm0.nii.gz”, “%(strategy)s/rest_pp</em>%(pipeline)s_sm0.nii.gz”],
    “functional_mni”: [”%(strategy)srest.%(pipeline)s.sm0.mni152.nii.gz”, “%(strategy)s/rest.%(pipeline)s.sm0.mni152.nii.gz”],
    “functional_brain_mask_to_standard”: “” # I would need to make this myself
}</p>

<p>reg_suffixes = {
    “functional_to_anat_linear_xfm”: “reg/example_func2highres.mat” 
#    “functional_to_mni_linear_xfm”: “functional_to_mni_linear_xfm.mat”
} </p>

<p>```</p>

<p>Now I go through each of the files to see if they are really needed. If not, then I will want to make sure that it is also processed as optional in my ghetto quick pack code.</p>

<h2 id="anatomicals">Anatomicals</h2>

<h3 id="anatomicalbrain">anatomical_brain</h3>

<p><strong>Yes</strong> it is needed for <strong>all the steps</strong> but <strong>only with ANTS</strong>. It is used by the applywarp sections of code and I think particularly for transforming FSL’s registration matrix to ANTs format (again i think).</p>

<h3 id="anatomicalreorient">anatomical_reorient</h3>

<p>Only <strong>VMHC?</strong>.</p>

<h3 id="anatomicaltomninonlinearxfm">anatomical_to_mni_nonlinear_xfm</h3>

<p><strong>Yes</strong> for <strong>all the steps</strong> but <strong>only for applying the registration</strong>.</p>

<h3 id="mninormalizedanatomical">mni_normalized_anatomical</h3>

<p><strong>No</strong>. This is only used in QC and so this can be removed from the set of inputs that I generate.</p>

<h2 id="functional">Functional</h2>

<h3 id="preprocessed">preprocessed</h3>

<p><strong>No</strong>. This is used in scrubbing and QC so it too can be removed from the set of inputs that I generate.</p>

<h3 id="meanfunctional">mean_functional</h3>

<p><strong>No but in VMHC</strong> it is used. Not sure how long that will last.</p>

<h3 id="functionalbrainmask">functional_brain_mask</h3>

<p><strong>Yes</strong> it is used in <strong>many (all?) steps</strong>.</p>

<h3 id="functionalnuisanceresiduals">functional_nuisance_residuals</h3>

<p><strong>Yes</strong> for <strong>ALFF and fALFF</strong>. Note that this uses the set_leaf_properties and get_leaf_properties function to dynamically get so it won’t be so obvious. This option can also be used for the other derivatives if you don’t want them to be filtered.</p>

<h3 id="functionalfreqfiltered">functional_freq_filtered</h3>

<p><strong>Yes</strong> for <strong>everything where want filtering</strong>.</p>

<h3 id="functionalmni">functional_mni</h3>

<p><strong>Yes</strong> for <strong>Spatial Regression, ROI time series, Voxel time series, Temporal Regression for SCA, Network Centrality</strong></p>

<h3 id="functionalbrainmasktostandard">functional_brain_mask_to_standard</h3>

<p><strong>Yes</strong> but <strong>only when apply registration</strong>. Although should also need it for the above ones…</p>

<h2 id="final">Final</h2>

<p>Ok so the final list, here again. Note that I didn’t discuss the reg one.</p>

<p>``` python
anat_suffixes = {
    “anatomical_brain”: “mprage_sanlm.nii.gz”,          # REGISTRATION
    “anatomical_reorient”: “reg/highres_rpi.nii.gz”,    # VMHC?
    “anatomical_to_mni_nonlinear_xfm”: “reg/highres2standard_warp.nii.gz”,  # REGISTRATION
#    “mni_normalized_anatomical”: “reg/fnirt_highres2standard.nii.gz”,      # NOT NEEDED 
}</p>

<p>func_suffixes = {
#    “preprocessed”: “rest_res.nii.gz”,             # NOT NEEDED
#    “mean_functional”: “rest_pp_mean.nii.gz”,      # VMHC?
    “functional_brain_mask”: “rest_pp_mask.nii.gz”, # YES 
    “functional_nuisance_residuals”: “rest_pp_nofilt_sm0.nii.gz”,  # YES (although not really used)
    “functional_freq_filtered”: [”%(strategy)srest_pp<em>%(pipeline)s_sm0.nii.gz”, “%(strategy)s/rest_pp</em>%(pipeline)s_sm0.nii.gz”],    # YES
    “functional_mni”: [”%(strategy)srest.%(pipeline)s.sm0.mni152.nii.gz”, “%(strategy)s/rest.%(pipeline)s.sm0.mni152.nii.gz”],      # YES
    “functional_brain_mask_to_standard”: “”                                                                                         # Yes
}</p>

<p>reg_suffixes = {
    “functional_to_anat_linear_xfm”: “reg/example_func2highres.mat” # REGISTRATION 
#    “functional_to_mni_linear_xfm”: “functional_to_mni_linear_xfm.mat”
} </p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Week 50 - Friday]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/13/week-50-friday/"/>
    <updated>2013-12-13T12:01:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/13/week-50-friday</id>
    <content type="html"><![CDATA[<p>There’s been a bit of a lull in writing these entries. Hoping to get back on the train.</p>

<p><strong>What are the dominant issues for today?</strong></p>

<ul>
  <li>
    <p>I found that yesterday that the fast eigencentrality code doesn’t work when the inputs are normalized data (mean=0, sd=1). I am not totally sure what’s going on but I can try to use the matlab code directly to see if my python port is missing something. I might first try with random data and then with some imaging data if possible. The imaging data could be used to directly compare with the python code. I might keep these test code blocks as gists.</p>
  </li>
  <li>
    <p>I should make a table of all the processing steps that are needed for the ABIDE dataset. I want to run some of these things in the background as I can so as not to pile up.</p>
  </li>
</ul>

<h2 id="testing-fast-eigenvector-code">Testing Fast Eigenvector Code</h2>

<p>When I ran both the python and the fast code without the normalization of the time-series. I get the same results in both datasets. However, with normalization of the timeseries, I get different results. One suggestion from Steve was that there might be a difference in some normalization.</p>

<p>Details of my simplified comparison can be found on https://gist.github.com/czarrar/7950474 and at the bottom of the post.</p>

<h2 id="abide">ABIDE</h2>

<p>Let’s first make a table and workflow explaining everything. I’ll add it here first and then move it to another spot later.</p>

<p>I was able to start the processing for degree centrality with our dataset. I should be able to…</p>

<h2 id="gist">Gist</h2>

<p><div><script src='https://gist.github.com/7950474.js'></script>
<noscript><pre><code>import numpy as np
from CPAC.cwas.subdist import norm_cols
from fast_ecm import fast_eigenvector_centrality # this is from the other gist; can ignore this and paste in the other function

print 'Compare with non-normalized matrices'

m  = np.random.random((200,1000))
cm = m.T.dot(m)

# Let's first call a basic approach
# This actually doesn't work, not sure what I'm setting wrong
from scipy import linalg as LA
w01,v01 = LA.eigh(cm, eigvals=(0,0))
e01     = cm.dot(np.abs(v01))/w01[0]

# Let's call a second basic approach (used currently)
from scipy.sparse import linalg as sLA
w02,v02 = sLA.eigsh(cm, k=1, which='LM', maxiter=1000)
e02     = cm.dot(np.abs(v02))/w02[0]

# Finally let's call the fast eigenvector (power approach)
v03     = fast_eigenvector_centrality(m, verbose=False)

# How different are the second and third ones?
print 'mean absolute diff: ', np.abs(e02-v03).mean()
print 'correlation: ', np.corrcoef(e02.T, v03.T)[0,1]



print 'Compare with normalized matrices'

n  = norm_cols(m)
cn = n.T.dot(n)

# Let's first call a basic approach
from scipy import linalg as LA
w11,v11 = LA.eigh(cn, eigvals=(0,0))
e11     = cn.dot(np.abs(v11))/w11[0]

# Let's call a second basic approach (used currently)
from scipy.sparse import linalg as sLA
w12,v12 = sLA.eigsh(cn, k=1, which='LM', maxiter=1000)
e12     = cn.dot(np.abs(v12))/w12[0]

# Finally let's call the fast eigenvector (power approach)
v13     = fast_eigenvector_centrality(n, verbose=False)

# How different are the second and third ones?
print 'mean absolute diff: ', np.abs(e12-v13).mean()
print 'correlation: ', np.corrcoef(e12.T, v13.T)[0,1]
</code></pre></noscript></div>
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Week 50 - Monday]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/09/week-50-monday/"/>
    <updated>2013-12-09T20:03:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/09/week-50-monday</id>
    <content type="html"><![CDATA[<h2 id="plansummary">Plan/Summary</h2>

<ul>
  <li>[x] Email Maarten with the results</li>
  <li>[x] Add running Xinian’s filtering script to my Basecamp TODO</li>
  <li>[x] Integrate updated CWAS code into master CPAC branch</li>
  <li>[] Tagging (e 1 hr)</li>
  <li>[] QC (e 1 hr)</li>
  <li>[] Finish fast eigenvector centrality code (not workflow)</li>
</ul>

<p>While above were items that I had planned in the morning, items below reflect additions to this plan during the day.</p>

<ul>
  <li>[x] cleaned up the code to use regression test data to run a python CPAC and R version of CWAS.</li>
  <li>[x] wrote code to smooth 4D functional data in standard space</li>
  <li>[x] write code to use smoothed data for CWAS</li>
  <li>[x] write some additional code to use internal CPAC filepaths for CWAS</li>
  <li>[x] write code to parse group analysis model inputs for CWAS</li>
</ul>

<p>The coding for CWAS in CPAC went surprisingly well. Hopefully, this streak might continue tomorrow when testing.</p>

<h2 id="integrating-cwas-code-with-cpac">Integrating CWAS Code with CPAC</h2>

<p>My issue in using the main CPAC repository was that although I was a member, I needed to be made an owner. After figuring this out, I created a ‘cwas’ branch and added the relevant changes to the cwas folder.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Week 49 - Friday]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/06/week-49-friday/"/>
    <updated>2013-12-06T10:35:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/06/week-49-friday</id>
    <content type="html"><![CDATA[<h2 id="plan">Plan</h2>

<ul>
  <li>ABIDE
    <ul>
      <li>[] QC (1 hour)</li>
    </ul>
  </li>
  <li>CPAC
    <ul>
      <li>[] Finish adapting fast eigenvector code (30 mins)</li>
      <li>[] Write SVD version of eigenvector (1 hour)</li>
      <li>[] Test new eigenvector code (2 hours)</li>
      <li>[] @Steve CWAS Regression Test (1 hour)</li>
    </ul>
  </li>
  <li>Tagging
    <ul>
      <li>[] Meeting to discuss criteria (1 hour)</li>
    </ul>
  </li>
</ul>

<p>Look into SVD (20 mins). But it’s slower.</p>

<p><code>python
import numpy as np
from numpy.linalg import svd
from scipy.sparse.linalg import eigsh
mat = np.random.random((100,1000))
%timeit u,s,v = svd(mat.T, False)
%timeit evalue, evector = eigsh(mat.T.dot(mat), k=1, which='LM', maxiter=1000) 
</code></p>

<p>In this case the current approach took 45.2ms and the SVD approach took 135ms.</p>

<h2 id="fast-eigenvector-centrality">Fast Eigenvector Centrality</h2>

<p>The plan for testing would be to first make sure that the code results are close to the original approach and possibly also compare to an SVD approach.</p>

<h2 id="brain-size-ala-maarten">Brain Size ala Maarten</h2>

<p>We ended up dropping the ball in getting Maarten some of these scripts.</p>

<p>I need to create an input file with <code>T1file,age,sex</code>. If I am using the NKI Rockland pilot dataset, then I believe I should be able to find the relevant path. For the most recent data, there have been additions so let me confirm that I have access to everything. Ok so first, pilot dataset.</p>

<p>``` r
rawdir    &lt;- ‘/home2/data/Originals/Rockland/raw’
phenofile &lt;- ‘/home2/data/Originals/Rockland/NKI.1-39_phenotypic.csv’</p>

<p>pheno     &lt;- read.csv(phenofile)
df        &lt;- pheno[,c(1,3,12)]
df$paths  &lt;- file.path(rawdir, df$Subject, “touse”, “anat”, “mprage.nii.gz”)
df$exists &lt;- file.exists(df$paths)*1</p>

<p>df.txt &lt;- df[df$exists==1,c(4,3,2)]
write.table(df.txt, file=”rockland_pilot.txt”, row.names=F, col.names=F, quote=F)
```</p>

<p>I want to now look at the Enhanced dataset.</p>

<h2 id="thoughts">Thoughts</h2>

<h3 id="plan-better">Plan Better</h3>

<p>Last few days I feel that I haven’t gotten through as much as I could have. Possibly due to not sleeping as much and partly due to more socializing like with the speaker’s visit and a new office mate. Let’s try differently today.</p>

<h3 id="link-to-pages-by-week-and-month">Link to pages by week and month</h3>

<p>Another nice thing would be to have a summary page at the end of the week with links to all the other pages that week (might want to check if those exist?) and similarly at the end of the month could have a page with the lists for that month. I wonder if this could be within the pages section?</p>
]]></content>
  </entry>
  
</feed>
