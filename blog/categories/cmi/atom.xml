<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: cmi | Zarrar's Journaling]]></title>
  <link href="http://czarrar.github.io/blog/categories/cmi/atom.xml" rel="self"/>
  <link href="http://czarrar.github.io/"/>
  <updated>2013-12-02T19:34:09-05:00</updated>
  <id>http://czarrar.github.io/</id>
  <author>
    <name><![CDATA[Zarrar Shehzad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Week 49 - Monday]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/02/week-49-monday/"/>
    <updated>2013-12-02T13:25:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/02/week-49-monday</id>
    <content type="html"><![CDATA[<h2 id="summary">Summary</h2>

<ul>
  <li>Checked and finalized fixing a subject.</li>
  <li>Resolved questions with QC</li>
  <li>Discussed issue with VMHC</li>
  <li>Setup quick pack for Xinian’s dataset</li>
  <li>Read through the eigenvector centrality paper</li>
  <li>Went through a few articles for tagging with updated instructions</li>
</ul>

<h2 id="todos">TODOs</h2>

<ul>
  <li>ABIDE
    <ul>
      <li>Check on fixed subject</li>
      <li>Email about QC</li>
      <li>Look into VMHC issue on github</li>
      <li>Create quick packs for everyone</li>
      <li>Email to ask about eigenvector centrality</li>
    </ul>
  </li>
  <li>Check fixed subject</li>
  <li>Find out about tagging</li>
</ul>

<h2 id="abide---fixed-subject">ABIDE - Fixed Subject</h2>

<p>I reran one participant with missing derivatives. It appears that everything is good.</p>

<h2 id="abide---quick-packs">ABIDE - Quick Packs</h2>

<p>We are looking to have the following derivatives:</p>

<ul>
  <li>Timeseries for different regions of interest</li>
  <li>Dual Regression maps</li>
  <li>ALFF/fALFF</li>
  <li>REHO</li>
  <li>Degree/Eigen Centrality</li>
  <li>VMHC</li>
</ul>

<h3 id="cameron">Cameron</h3>

<p>For our data, we already have most of the above derivatives and need the following:</p>

<ul>
  <li>Degree/Eigen Centrality</li>
  <li>VMHC</li>
</ul>

<p>Degree I should be able to run now but need to wait on Eigen and VMHC.</p>

<h3 id="xinian">Xinian</h3>

<p>I started to create the quick pack for this dataset. The one issue is that it’s unclear whether the global signal correction was applied to either the filtered or non-filtered data. It should be applied to both so will need to contact some people in order to figure out how to resolve this issue. Hopefully no heads will roll.</p>

<p>The scripts and what not for this dataset quick pack are located in <code>/data/Projects/ABIDE_Initiative/CPAC/abide/config/32_xinian</code>. I still need to write some code to generate the standard brain mask and of course test it all as well.</p>

<h3 id="pierre">Pierre</h3>

<p>Pierre’s is already in standard space so I believe I should simply be able to turn off <code>runRegisterFuncToMNI</code> and also my custom option <code>applyRegisterFuncToMNI</code>. The regular outputs should be good enough.</p>

<h3 id="chao-gan">Chao-Gan</h3>

<p>Check with Chao-Gan.</p>

<h2 id="abide---qc">ABIDE - QC</h2>

<p>I should do a similar approach as Yang and reference an email from Pierre with some guidelines.</p>

<h2 id="eigenvector-centrality">Eigenvector Centrality</h2>

<p>I did a bit of reading last time about a faster approach to eigenvector centrality. Now I went through the main paper again. It appears that their speed-up is primarily by not having to compute the cross product between a matrix of time series (voxels x time points). They split this step up into two parts that reduces the complexity from N^2 to 2NT where N = voxels and T = time points. They make a claim that for voxelwise data, they see a speedup of 1000x.</p>

<h2 id="tagging">Tagging</h2>

<p>Spoke to Cameron. We discussed some potential changes and I emailed these out to him and Matt.</p>

<p>In terms of tagging, here were some important points to note:</p>

<ul>
  <li>use mike’s opinions as the standard</li>
  <li>basic neuroscience is a superset of brain/behavior</li>
  <li>only include resting-state fMRI studies!</li>
</ul>

<h2 id="cwas">CWAS</h2>

<p>I got emails from Xavier and Phil with comments. I should look to integrate them.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wednesday - Week 48]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/27/wednesday-week-48/"/>
    <updated>2013-11-27T11:42:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/27/wednesday-week-48</id>
    <content type="html"><![CDATA[<h2 id="summary">Summary</h2>

<p>I was able to get through at least at a partial level through the items in my TOOD list below. More details are of course below.</p>

<ul>
  <li>I checked the CPAC output and found that 1 subject had some missing preprocessing data due to a crash during nuisance. I am rerunning this subject on rocky so I can examine the working directory (it said a tissue prior was missing in the error). Everything else is good. It should be noted that there are 2 participants in OHSU that are missing the 1st scan but have 2 other scans.</li>
  <li>I reran the QC and the output pages now seem good. Need to decide the procedure for QC now.</li>
  <li>Looked into eigenvector centrality speedup. There is code to do the power iteration approach that is advocated in the Brain Connectivity paper with 1000x speedup in a fast eigenvector centrality approach. Could also email the people from that paper to ask for their code.</li>
  <li>Discussed updating VMHC to have Cameron’s fixes (register regular MNI brain to symmetric MNI brain) with Steve. Also, discussed having the registration step be done in the assigned standard anatomical resolution and then apply the warp from this registration to the native functional image into the assigned standard functional resolution. This is not currently done.</li>
  <li>Did tagging for about 30 minutes.</li>
  <li>Created TODO items for ABIDE.</li>
</ul>

<h2 id="plans">Plans</h2>

<ul>
  <li>ABIDE
    <ul>
      <li>Create TODOs</li>
      <li>Finish checking CPAC output</li>
      <li>Fix QC pages</li>
      <li>Look into fixing VMHC error when running at 3mm</li>
      <li>Look into eigenvector centrality</li>
      <li>Create quick packs for CPAC and Xinian</li>
    </ul>
  </li>
  <li>Do 1 hour of tagging</li>
</ul>

<h2 id="abide---todo">ABIDE - TODO</h2>

<p>This is a partial list and the next time I will need to make a more thorough list.</p>

<ul>
  <li>Do QC</li>
  <li>Check all outputs there for Pierre, Xinian, Chao-Gan</li>
  <li>Optimize the eigenvector centrality</li>
  <li>Make sure VMHC works properly when using 3mm output resolution</li>
  <li>Setup Quick Packs</li>
  <li>Run Quick Packs</li>
</ul>

<h2 id="abide---check-outputs">ABIDE - Check Outputs</h2>

<p>I made a script <code>20_check_subjects.R</code> in <code>.../config/24_check</code> that checks which subject is missing some preprocessing output given that the raw data exists.</p>

<p>For two subjects, we are missing the first rest scan. These subjects are from OHSU and are <code>0050155</code> and <code>0050165</code>.</p>

<p>We need to redo <code>0051275</code> since there was a failure in calculating the nuisance regression. I ran this subject again on rocky with the relevant info/scripts in <code>/data/Projects/ABIDE_Initiative/CPAC/abide/config/26_reprocess_3mm</code>.</p>

<h2 id="abide---qc">ABIDE - QC</h2>

<p>Simply rerunning the QC resolved some broken links and the QC pages appear fine. Need to set some system of getting through all these QC pages and confirm the protocol for checking.</p>

<p><code>python
import CPAC
CPAC.utils.create_all_qc.run('/path/to/output_directory')
</code></p>

<h2 id="vmhc">VMHC</h2>

<p>Currently there is a bit of an issue when we try to run VMHC at 3mm. Although it now works, the registration appears to to be done at that resolution (3mm). Talking to Steve, it seems a fix proposed by Cameron would resolve my issue as well. That is, the registration between the regular MNI to the symmetric MNI would be done in the anatomical resolution, which is usually specified at 1mm. Then, this warp would be combined with the previous one to standard space, and we would apply this warp to the functional in order to get to the specified standard functional space, which is in 3mm.</p>

<h2 id="eigen-vector-centrality">Eigen-Vector Centrality</h2>

<p>I did a brief search of faster implementations of eigenvector centrality, which is also sometimes referred to as google’s page rank.</p>

<h3 id="paper-with-fast-implementation">Paper with Fast Implementation</h3>

<p>The most relevant item is a paper recommended by Cameron: http://online.liebertpub.com.ezproxy.med.nyu.edu/doi/full/10.1089/brain.2012.0087. It appears their analyses were done in matlab so if we could also get their code, it would be fairly easy to integrate with CPAC. Below is a quote from the paper that indicates this approach is very fast, potentially faster than degree centrality! Note that they used a voxel size of 2mm.</p>

<blockquote>
  <p>The RS-fMRI data of each subject contain 195,704 in-brain voxels per volume and the time-series length is 200. The approximate gain in efficiency compared to the standard algorithm is a factor 1000. The computation times for ECM of the fMRI (excluding file I/O) are 39 sec on average (std. dev. 14 sec) on an Intel Xeon.</p>
</blockquote>

<p>So figure out if we should contact them or not?</p>

<h3 id="related-links">Related Links</h3>

<p>The above paper used the power iteration method to calculate the dominant eigenvector. I found various links on the code to run this power iteration method.</p>

<ul>
  <li>Some slides with well explained code. http://homepages.math.uic.edu/~jan/mcs507/numpyveclinalg.pdf</li>
  <li>Easy to follow function although needs to be fixed: https://github.com/seckcoder/mmd/blob/9aba8a224c69daebc6f3f800e3847362911f2225/dimension_reduction.py</li>
  <li>Super straightforward code: http://stackoverflow.com/questions/13739186/compute-eigenvector-using-a-dominant-eigenvalue</li>
  <li>4 ways to compute: http://glowingpython.blogspot.com/2011/05/four-ways-to-compute-google-pagerank.html</li>
</ul>

<h2 id="tagging">Tagging</h2>

<p>I think I should go through and check the first 45 that I did. I believe that I may not have properly added the type of connectivity (seed-based, unsupervised, etc) properly.</p>

<p>Sent out an email with some issues/guidance on tagging.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tuesday - Week 48]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/26/tuesday-week-48/"/>
    <updated>2013-11-26T10:41:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/26/tuesday-week-48</id>
    <content type="html"><![CDATA[<h2 id="summary">Summary</h2>

<p>I was able to get through a fair amount of what I planned (next section). Here is what I did do:</p>

<ul>
  <li>Finished updating centrality code to be memory efficient</li>
  <li>Created a command-line tool to run said centrality code</li>
  <li>Ran memory test on final centrality code. The memory limit that a user sets is roughly (within 50MB) the amount that is actually used.</li>
  <li>Found path to Yang’s QC report for ABIDE</li>
  <li>Finished writing script to check what CPAC outputs exist. Most of the data appear to have been written with only a few missing data.</li>
</ul>

<h2 id="plans">Plans</h2>

<ul>
  <li>Do final memory test for centrality code</li>
  <li>Create command-line centrality wrapper</li>
  <li>Finish 10mins (from yesterday) for CMI librarian tagging</li>
  <li>Find Yang’s old QCing and related PDF</li>
  <li>Get details on ABIDE preprocessing output (what completed and what didn’t)</li>
  <li>Start creating quick packs for different strategies (CPAC, Pierre, Xinian, and Chao-Gan).</li>
</ul>

<h3 id="questions">Questions</h3>

<ul>
  <li>How will quick pack work for Pierre’s data? Do we provide the identity matrix?</li>
</ul>

<h2 id="abide-processing-output">ABIDE Processing Output</h2>

<p>One thing to check is if all scans have been preprocessed (i.e., if a subject has 3 scans as is the case with OHSU). I could check to see what other’s have done. Hmm not a bad idea. First find subjects with more than one scan and then see what happens with these subjects. Yes, the OHSU data has more than 1 scan per participant (can be up to 3 scans). Our CPAC processing did produce output for all scans. Ok so Xinian and I think Chao-Gan did not actually preprocess the additional scans, but Pierre did. Also Xinian and Chao-Gan did not concatenate their data for those subjects as the number of time-points was the same.</p>

<p>I also finished creating a script that checks on the filtered and globally corrected data.</p>

<p><code>r
df &lt;- read.csv("zcheck_exists_filt_global.csv")
rownames(df) &lt;- df$X
df &lt;- df[,-1]
nrow(df) - colSums(df)
</code></p>

<p>Here is the output below. We can see that we have most of the data but there are a couple of bad (possibly very bad) subject data.</p>

<p><code>
                             alff                  anatomical_brain 
                                3                                 0 
              anatomical_reorient   anatomical_to_mni_nonlinear_xfm 
                                0                                 0 
                  ants_affine_xfm                                dr 
                                0                                 3 
                            falff             functional_brain_mask 
                                3                                 2 
functional_brain_mask_to_standard          functional_freq_filtered 
                                2                                 3 
                   functional_mni     functional_nuisance_residuals 
                                3                                 3 
    functional_to_anat_linear_xfm                   mean_functional 
                                2                                 2 
        mni_normalized_anatomical                      preprocessed 
                                0                                 2 
                             reho 
                                3
</code></p>

<h2 id="abide-qc">ABIDE QC</h2>

<p>I was able to find Yang’s QC report in <code>/data/Projects/ABIDE_Initiative/CPAC/QC</code>.</p>

<p>Didn’t get a chance to regenerate the QC outputs but I think I might want to do this after I resolve that all the subject’s have data output with CPAC.</p>

<h2 id="centrality">Centrality</h2>

<p>Details for the military tactics taken to resolving the memory overage issues in the centrality, see <a href="/blog/2013/11/25/updating-centrality">this page</a>.</p>

<h3 id="memory-test">Memory Test</h3>

<p>Below are the results of running <code>./memtopX ./x_test_memprof.py</code> in <code>.../config/test_quick_pack</code>. I set a 1GB memory limit here, which led to two blocks being used. It seems further down that it came close to the this limit with 1.05GB being used.</p>

<p>Note that I’ve used 4mm data in standard space here, although later I also test with similar results using 3mm data.</p>

<p><code>
sorted nodes [0.0, 1.0]
threshold_option --&gt; 0
p_value -&gt; 0.266037302736
r_value --&gt;  0.0913878943802
inside optimized_centraltity, r_value -&gt; 0.0913878943802
block_size -&gt;  14236
Setup Degree Output
Normalize TimeSeries
Computing centrality across 18091 voxels
running block -&gt; 14236 0
...correlating
...calculating degree
...removing correlation matrix
running block -&gt; 18091 14236
...correlating
...calculating degree
...removing correlation matrix
timing: 32.47
</code></p>

<p>Here is the corresponding output of <code>mem.log</code> that recorded the memory usage for my script every 30 seconds.</p>

<p>File /Users/zarrar/Dropbox/Journal/octopress/source/downloads/code/cpac_centrality_memory.log could not be found</p>

<p>With the 3mm data, I set a memory limit of 4GB at first. This leads to a maximum usage of 4.03GB in the first block (it’s lower in the next block). Pretty nice! If I set a memory limit instead of 1GB, then I get a maximum usage of 1.03GB (consistent in most blocks). Score.</p>

<h3 id="command-line-tool">Command-Line Tool</h3>

<p>To make it easier to directly call the centrality code, I created a command-line interface. This is in <code>C-PAC/tools</code> and the script is called <code>cpac_centrality.py</code>. The auto-generated help is shown below.</p>

<p>``` 
usage: cpac_centrality.py [-h] -i INPUT [-m MASK] [–degree] [–eigen]
                          [–binarize] [–weighted] [–sparsity SPARSITY]
                          [–pvalue PVALUE] [–rho RHO] [–memlimit MEMLIMIT]
                          [-o OUTDIR]</p>

<p>Compute centrality for a given timeseries.</p>

<p>optional arguments:
  -h, –help            show this help message and exit
  -i INPUT, –input INPUT
                        Input timeseries data file
  -m MASK, –mask MASK  Brain mask (by default the program will create a mask
                        where voxels have non-zero variance regardeless of the
                        user specified mask).
  –degree              Calculate degree centrality
  –eigen               Calculate eigen centrality
  –binarize            For a given voxel, save the number of connections that
                        pass a threshold
  –weighted            For a given voxel, save the sum of all connection
                        weights that pass a threshold.
  –sparsity SPARSITY   Sparsity based threshold. (Only one threshold option
                        can be specified.)
  –pvalue PVALUE       P-value threshold for each connection. (Only one
                        threshold option can be specified.)
  –rho RHO             Regular correlation threshold. (Only one threshold
                        option can be specified.)
  –memlimit MEMLIMIT   Memory limit that should be set.
  -o OUTDIR, –outdir OUTDIR
                        Output directory
```</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Monday - Week 48]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/25/monday-week-48/"/>
    <updated>2013-11-25T09:59:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/25/monday-week-48</id>
    <content type="html"><![CDATA[<h2 id="plans">Plans</h2>

<p>Today, I planned to work on…see summary section for what actually got done.</p>

<ul>
  <li>finish memory fixes to centrality code</li>
  <li>checking on the 3mm registration of ABIDE</li>
  <li>setup quick pack for Pierre &amp; Xinian’s data</li>
  <li>setup quick pack for ABIDE data (centrality and VMHC)</li>
</ul>

<h2 id="summary">Summary</h2>

<p>I wasn’t able to get through a lot that was planned. I worked on the following (described more later):</p>

<ul>
  <li>Centrality Coding</li>
  <li>Checking paths for ABIDE preprocessing</li>
</ul>

<h2 id="centrality-code">Centrality Code</h2>

<p>I updated the centrality code with several fixes and cleaned up some of the functions. Details of this work can be found on this <a href="/blog/2013/11/25/update-centrality">other post</a>.</p>

<h2 id="qc">QC</h2>

<p>The 3mm registration is done. I want to now take a closer look at the data. I want to take a look at a few things:</p>

<ul>
  <li>Are all the outputs there? Maybe make some type of table or record of what is good with what?</li>
  <li>Can we fix up the QC pages?</li>
  <li>Do the images look good?</li>
</ul>

<p>I began to create the code to find the needed paths in the preprocessing directory. This can be found in <code>/data/Projects/ABIDE_Initiative/CPAC/abide/config/24_check/10_check_path.py</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Another Quick Pack Journey]]></title>
    <link href="http://czarrar.github.io/blog/2013/10/30/another-look-at-quick-pack-journey/"/>
    <updated>2013-10-30T15:55:00-04:00</updated>
    <id>http://czarrar.github.io/blog/2013/10/30/another-look-at-quick-pack-journey</id>
    <content type="html"><![CDATA[<p>Today, I’ve got through a few things. First, I finished compiling the <code>regressors.csv</code> file for </p>

<p>Some minor things, I helped Krishna with some ROI error he had in CPAC. It turned out that his issue was due to incorrectly binarizing the mask.</p>

<h1 id="quickpack">QuickPack</h1>

<p>Trying my hand again on quick pack today. Last time I got a bunch of different errors that I really couldn’t track. I am re-running the complete run to see what errors I get again and try to figure those out. I’m also rerunning the alff to see what happens there.</p>

<h2 id="complete-run">Complete Run</h2>

<p>This appears to be re-running smoothly (knock on wood). The last time I ran a complete (preprocessing + derivatives) run, it led to some unclear problems.</p>

<h2 id="alff-qp">ALFF QP</h2>

<p>I get the same link error as I did before. The error occurs on line 562 in <code>utils.py</code> within the <code>create_symbolic_links</code>. The line and error are</p>

<pre><code>ext = fname.split('.', 1)[1]
IndexError: list index out of range
</code></pre>

<p>Basically <code>fname</code> should be a filename but a directory is passed instead and so it fails. Specifically, it appears to be passing <code>/home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/ALFF_Output/pipeline_0/0051466_session_1/functional_freq_filtered/_scan_rest_1_rest/sinker_16</code>. I don’t have much sense about what is going on with this file.</p>

<p>Other bits of information:</p>

<ul>
  <li>The crash file is located <code>/home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/crash/crash-20131030-145001-milham-link_16.a0.np
z</code>.</li>
  <li>The directory in the working directory is <code>/data/Projects/ABIDE_Initiative/CPAC/test_qp/ALFF_Working/resting_preproc_0051466_session_1/_scan_rest_1_rest/link_16</code></li>
</ul>

<h3 id="steve-talk">Steve Talk</h3>

<p>I discussed this issue with Steve and it seems I will need to get down and dirty with nipype to understand the origin of this problem.</p>

<h2 id="reho-qp">REHO QP</h2>

<p>Here I got two errors. One of the errors was the same as with ALFF except instead of link<em>16 it is link</em>5. The other error is new and specific to REHO with the crash file: <code>/home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/crash/crash-20131030-164123-milham-reho_map.a0.a0.npz</code>. It appears the error is an empty input filename being passed to the <code>compute_reho</code> in <code>reho/utils</code>.</p>

<p>A snapshot of the error is given below:</p>

<pre><code>/data/Projects/ABIDE_Initiative/CPAC/test_qp/Reho_Working/resting_preproc_0051466_session_1/reho_0/_scan_rest_1_rest/_scan_rest_1_rest/reho_map/&lt;string&gt; in compute_reho(in_file, mask_file, cluster_size)

/home/data/PublicProgram/epd-7.2-2-rh5-x86_64/lib/python2.7/site-packages/nibabel/loadsave.pyc in load(filename)
     37     except KeyError:
     38         raise ImageFileError('Cannot work out file type of "%s"' %
---&gt; 39                              filename)
     40     if ext in ('.nii', '.mnc', '.mgh', '.mgz'):
     41         klass = class_map[img_type]['class']

ImageFileError: Cannot work out file type of ""
Interface Function failed to run. 
</code></pre>

<p>To run the crash file, see the code below.</p>

<p>``` python
import sys
sys.path.insert(0, ‘/home2/data/Projects/CPAC_Regression_Test/nipype-installs/fcp-indi-nipype/running-install/lib/python2.7/site-packages’)
sys.path.insert(1, “/home2/data/Projects/CPAC_Regression_Test/2013-05-30_cwas/C-PAC”)
import CPAC
import CPAC.reho.utils
from nipype.utils.filemanip import loadflat</p>

<p>crashinfo = loadflat(“/home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/crash/crash-20131030-164123-milham-reho_map.a0.a0.npz”)
crashinfo[‘node’].run()
```</p>

<h1 id="preprocessing-emotionalbs">Preprocessing EmotionalBS</h1>

<p>I am going to preprocess the emotionalBS data again with CPAC+ANTS. Some relevant parameters are:</p>

<ul>
  <li>Used ANTS for registration</li>
  <li>Two nuisance correction strategies
      * compcor+motion+linear+quadratic
      * compcor+global+motion+linear+quadratic</li>
  <li>Both frequency filtering (0.01-0.1 Hz) and no filtering</li>
  <li>Smoothing of 4.5mm (only for derivatives)</li>
  <li>Generated some SCA and drSCA based derivates using ROIs/maps from the ABIDE preprocessing. Otherwise no other derivatives were created.</li>
</ul>

<h2 id="install-fsl-5-for-gelert">Install FSL 5 for Gelert</h2>

<p>I was a little confused on how to install fsl 5 to a custom path (<code>/home2/data/PublicProgram</code>). It seemed to go through neurodebian and apt-get, I needed certain admin privileges that even Mike’s account didn’t have. And even if I had those, it wasn’t clear how I would install to a custom directory. Then I realized I could just copy the current fsl5 in <code>/usr/share/fsl/5.o</code> into that directory. The reason for this (<code>/home2/data/PublicProgram</code>) directory is so </p>

<h1 id="new-journal">New Journal</h1>

<p>I also worked on setting up this blog/journal with github user account. The goal is to chronicle my work efforts with particular thought on any issues I encounter. A major side benefit will be easier communication with others in the lab (e.g., was able to easily show the error for ALFF quick pack to Steve). Previously, I had been using different journal pages for each of my projects. I ended up being a big confused where to post stuff and often lost track of the markdown pages that I had created before they were published. With one journal site, this should all be easier.</p>

<p>I’m also testing out this slightly different jekyll wrapper (octopress). I unfortunately spent a lot longer (2 hours) setting it up cuz of a git error then I had wanted. So I hope some of its benefits over jekyll bootstrap such as easier syncing, simpler interface, and simpler integration of more advanced plugins, will be useful in the long run. That way I can justify my time spent installing it!</p>
]]></content>
  </entry>
  
</feed>
