<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: cpac | Zarrar's Journaling]]></title>
  <link href="http://czarrar.github.io/blog/categories/cpac/atom.xml" rel="self"/>
  <link href="http://czarrar.github.io/"/>
  <updated>2013-12-21T18:38:40-05:00</updated>
  <id>http://czarrar.github.io/</id>
  <author>
    <name><![CDATA[Zarrar Shehzad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Derivative Class]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/20/derivative-class/"/>
    <updated>2013-12-20T21:36:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/20/derivative-class</id>
    <content type="html"><![CDATA[<p><strong>Mission:</strong><br />
» Refractor current derivative workflows in <code>cpac_pipeline.py</code> (if you choose to accept it)</p>

<p><strong>Why:</strong><br />
» Easier to read code<br />
» Easier to implement quick packs
» Testing should be much more straightforward
» Potential for an extension-like system for users to integrate their own derivatives in the future (plug-in-play)</p>

<p>I personally have two objectives with these changes based on working with the ABIDE dataset. First, it is to make it easier to use other preprocessed data as inputs to CPAC. Second, it is to add a new derivative (functional density mapping; Tomasi et al., 2010) to the current pipeline system.</p>

<h2 id="how-does-it-currently-work-using-reho-as-an-example">How does it currently work? Using REHO as an example.</h2>

<p>I will be focusing on the <code>CPAC/pipeline/cpac_pipeline.py</code> file. This is a bit of a gargantuan file and contains both high-level (in terms of the pipeline) and low-level code, so it can get a bit confusing to go through. With that said, having everything in one place like this also makes it easier to go through after you know your way around.</p>

<p>I’ll skip past most of this code till about line 1796<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> where the REHO workflow starts. Then, I’ll go line by line (somewhat) to understand what’s happening until line 1833 when this part of the REHO code ends. Let’s begin shall we? I might suggest a good whiskey, it goes well while reading certain code.</p>

<h3 id="getting-our-toes-wet">Getting our toes wet</h3>

<p><code>python
new_stops_list = []
new_strat_list = []
num_strat = 0
</code></p>

<p>The <code>num_strat</code> stores the current strategy number (or index in a list, in this case). A strategy is one while run of the pipeline (i think), and anytime there is a fork in the pipeline (e.g., using versus not using bandpass filtering), a new strategy is created. This is all determined before as the data is preprocessed.</p>

<p>And it seems like the <code>new_stops_list</code> and <code>new_strat_list</code> do not get used in this section. So kindly ignore them. The <code>new_strat_list</code> is usually for when you add a new strategy to the list (i.e., there is a fork in the pipeline).</p>

<h3 id="getting-our-feet-wet">Getting our feet wet</h3>

<p>Ok so we haven’t even scratched the surface yet. Now I’ll throw all the code at you here for this section and then go through it gradually.</p>

<p>``` python
if 1 in c.runReHo:
    for i,strat in enumerate(strat_list):</p>

<pre><code>    preproc = create_reho()
    preproc.inputs.inputspec.cluster_size = c.clusterSize
    reho = preproc.clone('reho_%d' % num_strat)

    try:
        node, out_file = strat.get_leaf_properties()
        workflow.connect(node, out_file,
                         reho, 'inputspec.rest_res_filt')

        node, out_file = strat.get_node_from_resource_pool('functional_brain_mask')
        workflow.connect(node, out_file,
                         reho, 'inputspec.rest_mask')
    except:
        print 'Invalid Connection: REHO:', num_strat, ' resource_pool: ', strat.get_resource_pool()
        raise
</code></pre>

<p>strat.update_resource_pool({‘raw_reho_map’:(reho, ‘outputspec.raw_reho_map’)})
        strat.update_resource_pool({‘reho_Z_img’:(reho, ‘outputspec.z_score’)})
        strat.append_name(reho.name)</p>

<pre><code>    create_log_node(reho, 'outputspec.raw_reho_map', num_strat)
    
    num_strat += 1
</code></pre>

<p>```</p>

<h4 id="lines-1-2">Lines 1-2</h4>

<p><code>python
if 1 in c.runReHo:
    for i,strat in enumerate(strat_list):
</code></p>

<p>For all derivatives, we usually start with an if statement that checks if the user wants to run this particular derivative. Then we loop through each strategy to access the preprocessed functional data. The node information for these files are kept in the <code>strat_list</code>, which is a list of strategy objects. </p>

<h4 id="lines-4-6">Lines 4-6</h4>

<p><code>python
        preproc = create_reho()
        preproc.inputs.inputspec.cluster_size = c.clusterSize
        reho = preproc.clone('reho_%d' % num_strat)
</code></p>

<p>This is a little confusing but <code>preproc</code> is actually a workflow object for running REHO. It needs one input cluster size for the number of neighboring voxels to examine when measuring the regional homogeneity of the time-series. We then clone this reho workflow so that we can have a unique one for each reho named <code>'reho_%d' % num_strat</code>.</p>

<p>Maybe you like me are wondering why you can’t pass the cluster size and the name of the workflow as an argument to <code>create_reho</code>, which would eliminate those two additional lines. Not sure.</p>

<h4 id="lines-9-15">Lines 9-15</h4>

<p>``` python</p>

<p>node, out_file = strat.get_leaf_properties()
workflow.connect(node, out_file, reho, ‘inputspec.rest_res_filt’)</p>

<p>node, out_file = strat.get_node_from_resource_pool(‘functional_brain_mask’)
workflow.connect(node, out_file, reho, ‘inputspec.rest_mask’)</p>

<p>``` </p>

<p>One key step to any derivative is it’s inputs. As I mentioned before the <code>start</code> object holds the node information for the preprocessed functional data. First, we get the filtered functional data <code>inputspec.rest_res_filt</code>. Then, we get the functional brain mask. We will extract REHO estimates from our functional data within our brain mask.</p>

<p>Now you might be wondering, what is the difference between <code>strat.get_leaf_properties()</code> vs. <code>strat.get_node_from_resource_pool('functional_brain_mask')</code>? The former (for the time series) is when one node and output name get added to the <code>start</code> object as a leaf. It appears that there is only one leaf of it’s kind and this is used specifically for functional preprocessed data. This file can be the functional data at different stages of preprocessing depending on either the stage in the pipeline or the preferences of the user. This could be the file preprocessed data, preprocessed + nuisance regression, or preprocess + nuisance regression + filtering. The latter (for the brain mask) is a bit more straightforward and simply gets the information for ‘functional_brain_mask’ in the strat object (an element in a dictionary).</p>

<h4 id="lines-20-24">Lines 20-24</h4>

<p>``` python
        strat.update_resource_pool({‘raw_reho_map’:(reho, ‘outputspec.raw_reho_map’)})
        strat.update_resource_pool({‘reho_Z_img’:(reho, ‘outputspec.z_score’)})
        strat.append_name(reho.name)</p>

<pre><code>    create_log_node(reho, 'outputspec.raw_reho_map', num_strat)
</code></pre>

<p>```</p>

<p>These functions all have to do with adding the files generated by our reho workflow into the <code>start</code> object. Specifically, we will be adding the raw reho map and the Z-score transformed reho map. The last line here is related to keeping a log of this REHO workflow by creating a new log node.</p>

<p>Also I have no idea what the <code>strat.append_name</code> is about. Will need to find out.</p>

<h4 id="lines-26-27">Lines 26-27</h4>

<p><code>python
        num_strat += 1
stops_list += new_stops_list
strat_list += new_strat_list
</code></p>

<p>I’ve sort-of gone through these lines before so I’ll spare you the repetition.</p>

<h2 id="improving-this-code">Improving this code?</h2>

<p>I picked a pretty easy derivative. I mentioned some avenues of improvement. The function that generates the derivative workflow should be able to take in arguments. The <code>get_leaf_properties</code> is a little confusing and should be replaced.</p>

<p>The other avenue of improvement is the possibility that much of this is repeated in the other derivatives. You have some input functional data from the <code>strat</code> object and you need to save some output data into the <code>strat</code> object. We should be able to create an abstract <code>Derivative</code> class.</p>

<p>So we take in some inputs that are in the strat class and we give them to the reho class.</p>

<p>Below is a very rough and quick switch of this process.</p>

<p>``` python
class Derivative(object):
    “"”For adding a derivative to your workflow”””
    def <strong>init</strong>(self, fun, workflow, strat, <em>args, **kwrds):
        super(Derivative, self).<strong>init</strong>()
        self.workflow = workflow
        self.strat = strat
        self.args = args
        self.kwrds = kwrds
        self.deriv = func(</em>args, **kwrds)</p>

<pre><code>def connect_inputs(**kwrds):
    for deriv_in,strat_out in kwrds.iteritems():
        node,out_file = self.strat.get_node_from_resource_pool(strat_out)
        self.workflow.connect(node, out_file, 
                              self.deriv, 'inputspec.%s' % deriv_in)

def connect_outputs(**kwrds):
    for strat_in,deriv_out in kwrds.iteritems():
        strat.update_resource_pool({strat_in: (self.deriv, deriv_out)})
</code></pre>

<p>```</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The exact line number (1796) will vary depending on your version. Just search ‘Inserting REHO’ and you’ll be golden.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DPARSF Quick Pack]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/18/dparsf-quick-pack/"/>
    <updated>2013-12-18T17:14:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/18/dparsf-quick-pack</id>
    <content type="html"><![CDATA[<p>I want to first get down what files from the DPARSF preprocessed output correspond to inputs needed by CPAC for running the derivatives.</p>

<p>A big thing here is that I am running DPARSF in native space. Then on the this derivative data, I would normalize it using DARTELS. And I guess then apply smoothing using DPARSF. How to I apply smoothing with Chao-Gan’s script (emailed him to find out)?</p>

<h2 id="naming-conventions">Naming Conventions</h2>

<p><code>
A=slice timing
R=renormalization
C=covariate regressed
W=spatially normalized
S=smoothed
F=filtered
Sym=for VMH
global=GSR
</code></p>

<h2 id="anatomical">Anatomical</h2>

<p>I don’t need any of these since they are only for registration</p>

<h2 id="functional">Functional</h2>

<ul>
  <li>functional_brain_mask</li>
  <li>functional_nuisance_residuals</li>
  <li>functional_freq_filtered</li>
  <li></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CPAC Derivative Inputs]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/17/cpac-derivative-inputs/"/>
    <updated>2013-12-17T19:24:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/17/cpac-derivative-inputs</id>
    <content type="html"><![CDATA[<p>I want to try to organize what inputs are needed for what derivates for CPAC. First, I have the list of template inputs for quick pack for the CCS pipeline:</p>

<p>``` python
anat_suffixes = {
    “anatomical_brain”: “mprage_sanlm.nii.gz”,     # check
    “anatomical_reorient”: “reg/highres_rpi.nii.gz”, 
    “anatomical_to_mni_nonlinear_xfm”: “reg/highres2standard_warp.nii.gz”, 
    “mni_normalized_anatomical”: “reg/fnirt_highres2standard.nii.gz”, 
}</p>

<p>func_suffixes = {
    “preprocessed”: “rest_res.nii.gz”, 
    “mean_functional”: “rest_pp_mean.nii.gz”, 
    “functional_brain_mask”: “rest_pp_mask.nii.gz”, 
    “functional_nuisance_residuals”: “rest_pp_nofilt_sm0.nii.gz”,<br />
    “functional_freq_filtered”: [”%(strategy)srest_pp<em>%(pipeline)s_sm0.nii.gz”, “%(strategy)s/rest_pp</em>%(pipeline)s_sm0.nii.gz”],
    “functional_mni”: [”%(strategy)srest.%(pipeline)s.sm0.mni152.nii.gz”, “%(strategy)s/rest.%(pipeline)s.sm0.mni152.nii.gz”],
    “functional_brain_mask_to_standard”: “” # I would need to make this myself
}</p>

<p>reg_suffixes = {
    “functional_to_anat_linear_xfm”: “reg/example_func2highres.mat” 
#    “functional_to_mni_linear_xfm”: “functional_to_mni_linear_xfm.mat”
} </p>

<p>```</p>

<p>Now I go through each of the files to see if they are really needed. If not, then I will want to make sure that it is also processed as optional in my ghetto quick pack code.</p>

<h2 id="anatomicals">Anatomicals</h2>

<h3 id="anatomicalbrain">anatomical_brain</h3>

<p><strong>Yes</strong> it is needed for <strong>all the steps</strong> but <strong>only with ANTS</strong>. It is used by the applywarp sections of code and I think particularly for transforming FSL’s registration matrix to ANTs format (again i think).</p>

<h3 id="anatomicalreorient">anatomical_reorient</h3>

<p>Only <strong>VMHC?</strong>.</p>

<h3 id="anatomicaltomninonlinearxfm">anatomical_to_mni_nonlinear_xfm</h3>

<p><strong>Yes</strong> for <strong>all the steps</strong> but <strong>only for applying the registration</strong>.</p>

<h3 id="mninormalizedanatomical">mni_normalized_anatomical</h3>

<p><strong>No</strong>. This is only used in QC and so this can be removed from the set of inputs that I generate.</p>

<h2 id="functional">Functional</h2>

<h3 id="preprocessed">preprocessed</h3>

<p><strong>No</strong>. This is used in scrubbing and QC so it too can be removed from the set of inputs that I generate.</p>

<h3 id="meanfunctional">mean_functional</h3>

<p><strong>No but in VMHC</strong> it is used. Not sure how long that will last.</p>

<h3 id="functionalbrainmask">functional_brain_mask</h3>

<p><strong>Yes</strong> it is used in <strong>many (all?) steps</strong>.</p>

<h3 id="functionalnuisanceresiduals">functional_nuisance_residuals</h3>

<p><strong>Yes</strong> for <strong>ALFF and fALFF</strong>. Note that this uses the set_leaf_properties and get_leaf_properties function to dynamically get so it won’t be so obvious. This option can also be used for the other derivatives if you don’t want them to be filtered.</p>

<h3 id="functionalfreqfiltered">functional_freq_filtered</h3>

<p><strong>Yes</strong> for <strong>everything where want filtering</strong>.</p>

<h3 id="functionalmni">functional_mni</h3>

<p><strong>Yes</strong> for <strong>Spatial Regression, ROI time series, Voxel time series, Temporal Regression for SCA, Network Centrality</strong></p>

<h3 id="functionalbrainmasktostandard">functional_brain_mask_to_standard</h3>

<p><strong>Yes</strong> but <strong>only when apply registration</strong>. Although should also need it for the above ones…</p>

<h2 id="final">Final</h2>

<p>Ok so the final list, here again. Note that I didn’t discuss the reg one.</p>

<p>``` python
anat_suffixes = {
    “anatomical_brain”: “mprage_sanlm.nii.gz”,          # REGISTRATION
    “anatomical_reorient”: “reg/highres_rpi.nii.gz”,    # VMHC?
    “anatomical_to_mni_nonlinear_xfm”: “reg/highres2standard_warp.nii.gz”,  # REGISTRATION
#    “mni_normalized_anatomical”: “reg/fnirt_highres2standard.nii.gz”,      # NOT NEEDED 
}</p>

<p>func_suffixes = {
#    “preprocessed”: “rest_res.nii.gz”,             # NOT NEEDED
#    “mean_functional”: “rest_pp_mean.nii.gz”,      # VMHC?
    “functional_brain_mask”: “rest_pp_mask.nii.gz”, # YES 
    “functional_nuisance_residuals”: “rest_pp_nofilt_sm0.nii.gz”,  # YES (although not really used)
    “functional_freq_filtered”: [”%(strategy)srest_pp<em>%(pipeline)s_sm0.nii.gz”, “%(strategy)s/rest_pp</em>%(pipeline)s_sm0.nii.gz”],    # YES
    “functional_mni”: [”%(strategy)srest.%(pipeline)s.sm0.mni152.nii.gz”, “%(strategy)s/rest.%(pipeline)s.sm0.mni152.nii.gz”],      # YES
    “functional_brain_mask_to_standard”: “”                                                                                         # Yes
}</p>

<p>reg_suffixes = {
    “functional_to_anat_linear_xfm”: “reg/example_func2highres.mat” # REGISTRATION 
#    “functional_to_mni_linear_xfm”: “functional_to_mni_linear_xfm.mat”
} </p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Updating Centrality]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/25/updating-centrality/"/>
    <updated>2013-11-25T10:26:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/25/updating-centrality</id>
    <content type="html"><![CDATA[<p>I’ll go through the different changes in the CPAC centrality code that I’ve implemented.</p>

<h2 id="normalized-timeseries">Normalized Timeseries</h2>

<p>The easiest change was to use normalized time series, which then allows one to compute the crossproduct of the time series to calculate the correlations. Below are the relevant code slices that call on functions used in CWAS.</p>

<p><code>python
timeseries = norm_cols(timeseries.T)
corr_matrix = timeseries[:,j:i].T.dot(timeseries)
</code></p>

<h2 id="block-size">Block Size</h2>

<p>We changed the block size to more accurately reflect the underlying operations. Although it should be noted that numpy has some poor memory handling as certain temporary objects will still be maintained in memory throwing off this calculation. However, later sections will show attempts to rectify this issue.</p>

<p>``` python
nvoxs   = timeseries.shape[0]
ntpts   = timeseries.shape[1]
nbytes  = timeseries.dtype.itemsize</p>

<p>if memory_allocated:
    memory_in_bytes = memory_allocated * 1024.0<em>*3    # assume it is in GB
    block_size = int( memory_in_bytes/(nvoxs * nbytes) - 2 - ntpts</em>nbytes )</p>

<p>if block_size &gt; nvoxs:
    block_size = nvoxs
elif block_size &lt; 1:
    raise MemoryError(“ Not enough memory available to perform degree centrality”)
```</p>

<p>Note that the calculation is also dynamic with the data type of the time series.</p>

<h2 id="no-more-nan-to-number">No More NaN to Number</h2>

<p>This function <code>np.nan_to_num</code> is applied on the correlation matrix to turn any NaNs to 0s. This will occur when a time-series is all 0s leading to an error of sorts when calculating the correlation as you will be dividing by zeros and what not. The easiest way to avoid this error is to remove any flat time-series. I do this in the <code>load</code> function that initially reads in the time-series and the mask data.</p>

<p>I create an additional mask that is specific to the data. This is then combined with the externally defined template mask for a final mask at a later step.</p>

<p><code>python
datmask = (data!=0).any(axis=3)
mask    = nib.load(template).get_data().astype(np.float32)
</code></p>

<h2 id="more-efficient-sum-of-thresholded-correlations">More Efficient Sum of Thresholded Correlations</h2>

<p>This particular step (below) is both slow and takes up a lot of memory. For instance the <code>corr_matrix &gt; r_value</code> must create a new copy of the matrix.</p>

<p><code>python
 np.sum((corr_matrix &gt; r_value).astype(np.float32), axis = 1)
</code></p>

<p>One solution is to use cython. For now, I will do this without compiling the whole package and use <code>pyxthon</code> that will dynamically compile the new function at runtime. Below is the cython code.</p>

<p><code>python
def centrality_binarize_double(np.ndarray[double, ndim=2] cmat, np.ndarray[double, ndim=1] cent, double thresh):
    cdef unsigned int i,j
    for i in xrange(cmat.shape[0]):
        for j in xrange(cmat.shape[1]):
            cent[i] = cent[i] + 1*(cmat[i,j] &gt; thresh)
#
def centrality_weighted_double(np.ndarray[double, ndim=2] cmat, np.ndarray[double, ndim=1] cent, double thresh):
    cdef unsigned int i,j
    for i in xrange(cmat.shape[0]):
        for j in xrange(cmat.shape[1]):
            cent[i] = cent[i] + cmat[i,j]*(cmat[i,j] &gt; thresh)
</code></p>

<p>I created a wrapper centrality function that is fairly straightforward. It allows one to access all the different C functions through one python function. I also wrote 2 tests that both passed. Here’s one example to illustrate the new functionality (<code>comp</code>) relative to the old (<code>ref</code>).</p>

<p>``` python
def test_centrality_binarize():
    print “testing centrality binarize”</p>

<pre><code>method      = "binarize"
nblock      = 20
nvoxs       = 100
r_value     = 0.2
corr_matrix = np.random.random((nblock, nvoxs))

ref  = np.sum(corr_matrix&gt;r_value, axis=1)
comp = centrality(corr_matrix, r_value, method)

assert_equal(ref, comp) ```
</code></pre>

<h2 id="deleting-temporary-correlation-matrix">Deleting Temporary Correlation Matrix</h2>

<p>The correlation matrix created for each block loop will not be freed, hence there is a build up of these matrices in memory. A simple fix for this is to delete the correlation matrix at the end of each loop. It is then freed when the next loop iteration begins.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Friday - Week 47]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/22/friday-week-47/"/>
    <updated>2013-11-22T15:44:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/22/friday-week-47</id>
    <content type="html"><![CDATA[<h2 id="summary">Summary</h2>

<h3 id="centrality-and-memory">Centrality and Memory</h3>

<p>The conclusion from today’s work is that at three steps in the computation of centrality there is a huge spike in memory load that then stays there.</p>

<ol>
  <li>When computing the correlations, additional memory is allocated then that used for the output (up to 0.5x more).</li>
  <li>When converting any NaNs to 0s, more than 2x more than the current memory is used (I guess because a copy of the data is created but not wiped from memory at a later point).</li>
  <li>When calculating the degree centrality from the connectivity about 2x the amount of memory is used at this step.</li>
</ol>

<p>We actually can deal with step 2, since this only occurs because some time-series are flat or all zeros. If we mask those time series out in advance, then we don’t need to worry about that particular issue.</p>

<p>The third step might also be solved semi-easily with some cython. There is one nice tutorial/comparison here: http://technicaldiscovery.blogspot.com/2011/06/speeding-up-python-numpy-cython-and.html. We could then write a function that loops through the array, thresholds each element, and adds each element to a vector.</p>

<p>Further details are given below on examining memory issues in centrality using (1) 3mm brain and breakpoints and (2) a toy example to get more details.</p>

<h3 id="abide">ABIDE</h3>

<p>I began running CPAC to register the participant functional data and derivatives to 3mm space.</p>

<h2 id="memory-issues">Memory Issues</h2>

<p>There are a couple of nice pages on some memory issues with numpy. One suggestion was to upgrade to version 1.7.1+ where some memory leaks were fixed. I’m currently using version 1.6.1 so it’s possible using a more advanced version could help…however, I did check with a version of numpy Dan has and this didn’t solve my particular issues.</p>

<h2 id="detailed-memory-test">Detailed Memory Test</h2>

<p>I have put a few breakpoints in the centrality function to see where the memory blows up. I will record the memory usage based on <code>ps</code> at one of 4 points below. I really run the following command <code>./memtopX ./x_test_memprof.py</code>. I will be using 3mm data here.</p>

<p><code>python
TODO: breaks pigments will compile elsewhere
</code></p>

<p>block -&gt; 34382 0</p>

<ol>
  <li>93.88 MB</li>
  <li>139.12 MB</li>
  <li>6.45 GB (but note that before this point, memory goes up to 18.1GB)</li>
  <li>6.45 GB</li>
  <li>6.45 GB (but note that before this point, memory goes up to 13.75GB)</li>
</ol>

<p>block -&gt; 45262 34382</p>

<p>Also there was a second block so points 3-4 repeat. They both had 2.49 GB as that used.</p>

<p>Took about 324.71 secs or 5mins.</p>

<h2 id="for-our-sanity">For our Sanity</h2>

<p>So to confirm that we get sane numbers in python, one can run the following code and then monitor the memory externally with <code>./memtop</code>. Here, we will create a <code>corr_matrix</code> identical in size as in block 1 above. We find generally that our own calculation of this matrix is the same as that within python, which is the same as that using <code>memtop</code> (depends on <code>ps</code>). To get <code>memtop</code>, please follow this link: http://justinfranks.com/code/scripts/memtop.</p>

<p>``` python
import numpy as np</p>

<p>nvoxs = 34382
corr_matrix = np.ones((nvoxs, nvoxs)).astype(‘float32’)</p>

<h1 id="our-assumption-44gb">Our assumption (4.4GB)</h1>
<p>size_us = 4 * nvoxs<strong>2 / float(1024</strong>2)</p>

<h1 id="using-numpy-44gb">Using numpy (4.4GB)</h1>
<p>size_np = corr_matrix.nbytes / float(1024**2)</p>

<h1 id="from-the-command-line-445gb">From the command-line (4.45GB)</h1>
<p>import os
os.system(“./memtop | grep ipython”)
```</p>

<h2 id="understanding-memory-for-dot-product">Understanding memory for dot product</h2>

<p>``` python
import sys
sys.path = [’’, ‘/home2/dlurie/Enthought/Canopy<em>64bit/User/lib/python2.7/site-packages/nibabel-1.3.0-py2.7.egg’, ‘/home2/dlurie/Enthought/Canopy</em>64bit/User/lib/python2.7/site-packages/pydicom-0.9.8-py2.7.egg’, ‘/home2/dlurie/Enthought/Canopy<em>64bit/User/lib/python2.7/site-packages/networkx-1.7-py2.7.egg’, ‘/home2/dlurie/Enthought/Canopy</em>64bit/User/lib/python2.7/site-packages/pysurfer-0.3.1-py2.7.egg’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86<em>64/lib/python27.zip’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86</em>64/lib/python2.7’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86<em>64/lib/python2.7/plat-linux2’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86</em>64/lib/python2.7/lib-tk’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86<em>64/lib/python2.7/lib-old’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86</em>64/lib/python2.7/lib-dynload’, ‘/home2/dlurie/Enthought/Canopy<em>64bit/User/lib/python2.7/site-packages’, ‘/home2/dlurie/Enthought/Canopy</em>64bit/System/lib/python2.7/site-packages’, ‘/home2/dlurie/Enthought/Canopy<em>64bit/System/lib/python2.7/site-packages/PIL’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86</em>64/lib/python2.7/site-packages’]</p>

<h1 id="note-there-is-a-base-memory-of-8486--mb">Note there is a base memory of 84.86  MB</h1>
<p>import numpy as np
npsize = lambda x: x.nbytes/float(1024**3)</p>

<p>nvoxs = 10000
ntpts = 150</p>

<h1 id="there-is-an-increase-to-1007-mb-with-the-time-series">There is an increase to 100.7 MB with the time series</h1>
<p># this is a little larger then it’s 11.5MB size
timeseries = np.random.random((nvoxs,ntpts)).T
timeseries[:,100:104] = 0</p>

<h1 id="to-86373-mb">to 863.73 MB</h1>
<p>corr_matrix = np.zeros((nvoxs,nvoxs))</p>

<h1 id="to-135-gb-even-after-it-is-done">to 1.35 GB (even after it is done)</h1>
<p>np.dot(timeseries.T[:,0:nvoxs], timeseries, out=corr_matrix)</p>

<h1 id="to-247-gb-but-stays-at-209-gb">to 2.47 GB but stays at 2.09 GB</h1>
<p>corr_matrix = np.nan_to_num(corr_matrix)</p>

<h1 id="still-at-209-gb">still at 2.09 GB</h1>
<p>degree_centrality_weighted = np.zeros(nvoxs)</p>

<h1 id="to-368-gb-but-stays-at-284-gb">to 3.68 GB but stays at 2.84 GB</h1>
<p>r_value = 0.2
np.sum(corr_matrix*(corr_matrix &gt; r_value), axis=1, out=degree_centrality_weighted)</p>

<h1 id="note-that-theoretically-we-might-have-thought-a-limit-of-075-gb">Note that theoretically, we might have thought a limit of 0.75 GB</h1>
<p>(nvoxs<em>nvoxs + nvoxs + nvoxs</em>ntpts)*8/float(1024**3)
```</p>

<p>Below is the parallel commands that I ran to monitor the memory usage above.</p>

<p><code>bash
./memtop | grep ipython # get the pid
PID=7873
while [[ True ]]; do
	./memtop -s -p $PID
	sleep 0.5
done
</code></p>

<h2 id="memory-leakage-links">Memory Leakage Links</h2>

<ul>
  <li>http://stackoverflow.com/questions/12422307/reducing-numpy-memory-footprint-in-long-running-application</li>
  <li>http://stackoverflow.com/questions/12461413/why-does-comparison-of-a-numpy-array-with-a-list-consume-so-much-memory</li>
  <li>http://stackoverflow.com/questions/15191391/how-to-avoid-this-four-line-memory-leak-with-numpymkl</li>
</ul>

<h2 id="running-centrality-on-abide">Running Centrality on ABIDE</h2>

<p>The usage approaches 18GB when I set a 12GB limit. On gelert, each node has about 16GB of RAM. The easiest thing might be to run one process on each node with a limit of 6GB to keep well within range.</p>

<h2 id="registering-data-to-3mm-space">Registering Data to 3mm Space</h2>

<p>I also want to redo the registration to be in 3mm standard space, whereas it currently is in 2mm. I’m working on the script for that task and have started running this on gelert now.</p>
]]></content>
  </entry>
  
</feed>
