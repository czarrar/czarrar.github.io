<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: cpac | Zarrar's Journaling]]></title>
  <link href="http://czarrar.github.io/blog/categories/cpac/atom.xml" rel="self"/>
  <link href="http://czarrar.github.io/"/>
  <updated>2013-12-19T00:39:52-05:00</updated>
  <id>http://czarrar.github.io/</id>
  <author>
    <name><![CDATA[Zarrar Shehzad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[DPARSF Quick Pack]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/18/dparsf-quick-pack/"/>
    <updated>2013-12-18T17:14:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/18/dparsf-quick-pack</id>
    <content type="html"><![CDATA[<p>I want to first get down what files from the DPARSF preprocessed output correspond to inputs needed by CPAC for running the derivatives.</p>

<p>A big thing here is that I am running DPARSF in native space. Then on the this derivative data, I would normalize it using DARTELS. And I guess then apply smoothing using DPARSF. How to I apply smoothing with Chao-Gan’s script (emailed him to find out)?</p>

<h2 id="naming-conventions">Naming Conventions</h2>

<p><code>
A=slice timing
R=renormalization
C=covariate regressed
W=spatially normalized
S=smoothed
F=filtered
Sym=for VMH
global=GSR
</code></p>

<h2 id="anatomical">Anatomical</h2>

<p>I don’t need any of these since they are only for registration</p>

<h2 id="functional">Functional</h2>

<ul>
  <li>functional_brain_mask</li>
  <li>functional_nuisance_residuals</li>
  <li>functional_freq_filtered</li>
  <li></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CPAC Derivative Inputs]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/17/cpac-derivative-inputs/"/>
    <updated>2013-12-17T19:24:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/17/cpac-derivative-inputs</id>
    <content type="html"><![CDATA[<p>I want to try to organize what inputs are needed for what derivates for CPAC. First, I have the list of template inputs for quick pack for the CCS pipeline:</p>

<p>``` python
anat_suffixes = {
    “anatomical_brain”: “mprage_sanlm.nii.gz”,     # check
    “anatomical_reorient”: “reg/highres_rpi.nii.gz”, 
    “anatomical_to_mni_nonlinear_xfm”: “reg/highres2standard_warp.nii.gz”, 
    “mni_normalized_anatomical”: “reg/fnirt_highres2standard.nii.gz”, 
}</p>

<p>func_suffixes = {
    “preprocessed”: “rest_res.nii.gz”, 
    “mean_functional”: “rest_pp_mean.nii.gz”, 
    “functional_brain_mask”: “rest_pp_mask.nii.gz”, 
    “functional_nuisance_residuals”: “rest_pp_nofilt_sm0.nii.gz”,<br />
    “functional_freq_filtered”: [”%(strategy)srest_pp<em>%(pipeline)s_sm0.nii.gz”, “%(strategy)s/rest_pp</em>%(pipeline)s_sm0.nii.gz”],
    “functional_mni”: [”%(strategy)srest.%(pipeline)s.sm0.mni152.nii.gz”, “%(strategy)s/rest.%(pipeline)s.sm0.mni152.nii.gz”],
    “functional_brain_mask_to_standard”: “” # I would need to make this myself
}</p>

<p>reg_suffixes = {
    “functional_to_anat_linear_xfm”: “reg/example_func2highres.mat” 
#    “functional_to_mni_linear_xfm”: “functional_to_mni_linear_xfm.mat”
} </p>

<p>```</p>

<p>Now I go through each of the files to see if they are really needed. If not, then I will want to make sure that it is also processed as optional in my ghetto quick pack code.</p>

<h2 id="anatomicals">Anatomicals</h2>

<h3 id="anatomicalbrain">anatomical_brain</h3>

<p><strong>Yes</strong> it is needed for <strong>all the steps</strong> but <strong>only with ANTS</strong>. It is used by the applywarp sections of code and I think particularly for transforming FSL’s registration matrix to ANTs format (again i think).</p>

<h3 id="anatomicalreorient">anatomical_reorient</h3>

<p>Only <strong>VMHC?</strong>.</p>

<h3 id="anatomicaltomninonlinearxfm">anatomical_to_mni_nonlinear_xfm</h3>

<p><strong>Yes</strong> for <strong>all the steps</strong> but <strong>only for applying the registration</strong>.</p>

<h3 id="mninormalizedanatomical">mni_normalized_anatomical</h3>

<p><strong>No</strong>. This is only used in QC and so this can be removed from the set of inputs that I generate.</p>

<h2 id="functional">Functional</h2>

<h3 id="preprocessed">preprocessed</h3>

<p><strong>No</strong>. This is used in scrubbing and QC so it too can be removed from the set of inputs that I generate.</p>

<h3 id="meanfunctional">mean_functional</h3>

<p><strong>No but in VMHC</strong> it is used. Not sure how long that will last.</p>

<h3 id="functionalbrainmask">functional_brain_mask</h3>

<p><strong>Yes</strong> it is used in <strong>many (all?) steps</strong>.</p>

<h3 id="functionalnuisanceresiduals">functional_nuisance_residuals</h3>

<p><strong>Yes</strong> for <strong>ALFF and fALFF</strong>. Note that this uses the set_leaf_properties and get_leaf_properties function to dynamically get so it won’t be so obvious. This option can also be used for the other derivatives if you don’t want them to be filtered.</p>

<h3 id="functionalfreqfiltered">functional_freq_filtered</h3>

<p><strong>Yes</strong> for <strong>everything where want filtering</strong>.</p>

<h3 id="functionalmni">functional_mni</h3>

<p><strong>Yes</strong> for <strong>Spatial Regression, ROI time series, Voxel time series, Temporal Regression for SCA, Network Centrality</strong></p>

<h3 id="functionalbrainmasktostandard">functional_brain_mask_to_standard</h3>

<p><strong>Yes</strong> but <strong>only when apply registration</strong>. Although should also need it for the above ones…</p>

<h2 id="final">Final</h2>

<p>Ok so the final list, here again. Note that I didn’t discuss the reg one.</p>

<p>``` python
anat_suffixes = {
    “anatomical_brain”: “mprage_sanlm.nii.gz”,          # REGISTRATION
    “anatomical_reorient”: “reg/highres_rpi.nii.gz”,    # VMHC?
    “anatomical_to_mni_nonlinear_xfm”: “reg/highres2standard_warp.nii.gz”,  # REGISTRATION
#    “mni_normalized_anatomical”: “reg/fnirt_highres2standard.nii.gz”,      # NOT NEEDED 
}</p>

<p>func_suffixes = {
#    “preprocessed”: “rest_res.nii.gz”,             # NOT NEEDED
#    “mean_functional”: “rest_pp_mean.nii.gz”,      # VMHC?
    “functional_brain_mask”: “rest_pp_mask.nii.gz”, # YES 
    “functional_nuisance_residuals”: “rest_pp_nofilt_sm0.nii.gz”,  # YES (although not really used)
    “functional_freq_filtered”: [”%(strategy)srest_pp<em>%(pipeline)s_sm0.nii.gz”, “%(strategy)s/rest_pp</em>%(pipeline)s_sm0.nii.gz”],    # YES
    “functional_mni”: [”%(strategy)srest.%(pipeline)s.sm0.mni152.nii.gz”, “%(strategy)s/rest.%(pipeline)s.sm0.mni152.nii.gz”],      # YES
    “functional_brain_mask_to_standard”: “”                                                                                         # Yes
}</p>

<p>reg_suffixes = {
    “functional_to_anat_linear_xfm”: “reg/example_func2highres.mat” # REGISTRATION 
#    “functional_to_mni_linear_xfm”: “functional_to_mni_linear_xfm.mat”
} </p>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Updating Centrality]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/25/updating-centrality/"/>
    <updated>2013-11-25T10:26:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/25/updating-centrality</id>
    <content type="html"><![CDATA[<p>I’ll go through the different changes in the CPAC centrality code that I’ve implemented.</p>

<h2 id="normalized-timeseries">Normalized Timeseries</h2>

<p>The easiest change was to use normalized time series, which then allows one to compute the crossproduct of the time series to calculate the correlations. Below are the relevant code slices that call on functions used in CWAS.</p>

<p><code>python
timeseries = norm_cols(timeseries.T)
corr_matrix = timeseries[:,j:i].T.dot(timeseries)
</code></p>

<h2 id="block-size">Block Size</h2>

<p>We changed the block size to more accurately reflect the underlying operations. Although it should be noted that numpy has some poor memory handling as certain temporary objects will still be maintained in memory throwing off this calculation. However, later sections will show attempts to rectify this issue.</p>

<p>``` python
nvoxs   = timeseries.shape[0]
ntpts   = timeseries.shape[1]
nbytes  = timeseries.dtype.itemsize</p>

<p>if memory_allocated:
    memory_in_bytes = memory_allocated * 1024.0<em>*3    # assume it is in GB
    block_size = int( memory_in_bytes/(nvoxs * nbytes) - 2 - ntpts</em>nbytes )</p>

<p>if block_size &gt; nvoxs:
    block_size = nvoxs
elif block_size &lt; 1:
    raise MemoryError(“ Not enough memory available to perform degree centrality”)
```</p>

<p>Note that the calculation is also dynamic with the data type of the time series.</p>

<h2 id="no-more-nan-to-number">No More NaN to Number</h2>

<p>This function <code>np.nan_to_num</code> is applied on the correlation matrix to turn any NaNs to 0s. This will occur when a time-series is all 0s leading to an error of sorts when calculating the correlation as you will be dividing by zeros and what not. The easiest way to avoid this error is to remove any flat time-series. I do this in the <code>load</code> function that initially reads in the time-series and the mask data.</p>

<p>I create an additional mask that is specific to the data. This is then combined with the externally defined template mask for a final mask at a later step.</p>

<p><code>python
datmask = (data!=0).any(axis=3)
mask    = nib.load(template).get_data().astype(np.float32)
</code></p>

<h2 id="more-efficient-sum-of-thresholded-correlations">More Efficient Sum of Thresholded Correlations</h2>

<p>This particular step (below) is both slow and takes up a lot of memory. For instance the <code>corr_matrix &gt; r_value</code> must create a new copy of the matrix.</p>

<p><code>python
 np.sum((corr_matrix &gt; r_value).astype(np.float32), axis = 1)
</code></p>

<p>One solution is to use cython. For now, I will do this without compiling the whole package and use <code>pyxthon</code> that will dynamically compile the new function at runtime. Below is the cython code.</p>

<p><code>python
def centrality_binarize_double(np.ndarray[double, ndim=2] cmat, np.ndarray[double, ndim=1] cent, double thresh):
    cdef unsigned int i,j
    for i in xrange(cmat.shape[0]):
        for j in xrange(cmat.shape[1]):
            cent[i] = cent[i] + 1*(cmat[i,j] &gt; thresh)
#
def centrality_weighted_double(np.ndarray[double, ndim=2] cmat, np.ndarray[double, ndim=1] cent, double thresh):
    cdef unsigned int i,j
    for i in xrange(cmat.shape[0]):
        for j in xrange(cmat.shape[1]):
            cent[i] = cent[i] + cmat[i,j]*(cmat[i,j] &gt; thresh)
</code></p>

<p>I created a wrapper centrality function that is fairly straightforward. It allows one to access all the different C functions through one python function. I also wrote 2 tests that both passed. Here’s one example to illustrate the new functionality (<code>comp</code>) relative to the old (<code>ref</code>).</p>

<p>``` python
def test_centrality_binarize():
    print “testing centrality binarize”</p>

<pre><code>method      = "binarize"
nblock      = 20
nvoxs       = 100
r_value     = 0.2
corr_matrix = np.random.random((nblock, nvoxs))

ref  = np.sum(corr_matrix&gt;r_value, axis=1)
comp = centrality(corr_matrix, r_value, method)

assert_equal(ref, comp) ```
</code></pre>

<h2 id="deleting-temporary-correlation-matrix">Deleting Temporary Correlation Matrix</h2>

<p>The correlation matrix created for each block loop will not be freed, hence there is a build up of these matrices in memory. A simple fix for this is to delete the correlation matrix at the end of each loop. It is then freed when the next loop iteration begins.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Friday - Week 47]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/22/friday-week-47/"/>
    <updated>2013-11-22T15:44:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/22/friday-week-47</id>
    <content type="html"><![CDATA[<h2 id="summary">Summary</h2>

<h3 id="centrality-and-memory">Centrality and Memory</h3>

<p>The conclusion from today’s work is that at three steps in the computation of centrality there is a huge spike in memory load that then stays there.</p>

<ol>
  <li>When computing the correlations, additional memory is allocated then that used for the output (up to 0.5x more).</li>
  <li>When converting any NaNs to 0s, more than 2x more than the current memory is used (I guess because a copy of the data is created but not wiped from memory at a later point).</li>
  <li>When calculating the degree centrality from the connectivity about 2x the amount of memory is used at this step.</li>
</ol>

<p>We actually can deal with step 2, since this only occurs because some time-series are flat or all zeros. If we mask those time series out in advance, then we don’t need to worry about that particular issue.</p>

<p>The third step might also be solved semi-easily with some cython. There is one nice tutorial/comparison here: http://technicaldiscovery.blogspot.com/2011/06/speeding-up-python-numpy-cython-and.html. We could then write a function that loops through the array, thresholds each element, and adds each element to a vector.</p>

<p>Further details are given below on examining memory issues in centrality using (1) 3mm brain and breakpoints and (2) a toy example to get more details.</p>

<h3 id="abide">ABIDE</h3>

<p>I began running CPAC to register the participant functional data and derivatives to 3mm space.</p>

<h2 id="memory-issues">Memory Issues</h2>

<p>There are a couple of nice pages on some memory issues with numpy. One suggestion was to upgrade to version 1.7.1+ where some memory leaks were fixed. I’m currently using version 1.6.1 so it’s possible using a more advanced version could help…however, I did check with a version of numpy Dan has and this didn’t solve my particular issues.</p>

<h2 id="detailed-memory-test">Detailed Memory Test</h2>

<p>I have put a few breakpoints in the centrality function to see where the memory blows up. I will record the memory usage based on <code>ps</code> at one of 4 points below. I really run the following command <code>./memtopX ./x_test_memprof.py</code>. I will be using 3mm data here.</p>

<p><code>python
TODO: breaks pigments will compile elsewhere
</code></p>

<p>block -&gt; 34382 0</p>

<ol>
  <li>93.88 MB</li>
  <li>139.12 MB</li>
  <li>6.45 GB (but note that before this point, memory goes up to 18.1GB)</li>
  <li>6.45 GB</li>
  <li>6.45 GB (but note that before this point, memory goes up to 13.75GB)</li>
</ol>

<p>block -&gt; 45262 34382</p>

<p>Also there was a second block so points 3-4 repeat. They both had 2.49 GB as that used.</p>

<p>Took about 324.71 secs or 5mins.</p>

<h2 id="for-our-sanity">For our Sanity</h2>

<p>So to confirm that we get sane numbers in python, one can run the following code and then monitor the memory externally with <code>./memtop</code>. Here, we will create a <code>corr_matrix</code> identical in size as in block 1 above. We find generally that our own calculation of this matrix is the same as that within python, which is the same as that using <code>memtop</code> (depends on <code>ps</code>). To get <code>memtop</code>, please follow this link: http://justinfranks.com/code/scripts/memtop.</p>

<p>``` python
import numpy as np</p>

<p>nvoxs = 34382
corr_matrix = np.ones((nvoxs, nvoxs)).astype(‘float32’)</p>

<h1 id="our-assumption-44gb">Our assumption (4.4GB)</h1>
<p>size_us = 4 * nvoxs<strong>2 / float(1024</strong>2)</p>

<h1 id="using-numpy-44gb">Using numpy (4.4GB)</h1>
<p>size_np = corr_matrix.nbytes / float(1024**2)</p>

<h1 id="from-the-command-line-445gb">From the command-line (4.45GB)</h1>
<p>import os
os.system(“./memtop | grep ipython”)
```</p>

<h2 id="understanding-memory-for-dot-product">Understanding memory for dot product</h2>

<p>``` python
import sys
sys.path = [’’, ‘/home2/dlurie/Enthought/Canopy<em>64bit/User/lib/python2.7/site-packages/nibabel-1.3.0-py2.7.egg’, ‘/home2/dlurie/Enthought/Canopy</em>64bit/User/lib/python2.7/site-packages/pydicom-0.9.8-py2.7.egg’, ‘/home2/dlurie/Enthought/Canopy<em>64bit/User/lib/python2.7/site-packages/networkx-1.7-py2.7.egg’, ‘/home2/dlurie/Enthought/Canopy</em>64bit/User/lib/python2.7/site-packages/pysurfer-0.3.1-py2.7.egg’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86<em>64/lib/python27.zip’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86</em>64/lib/python2.7’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86<em>64/lib/python2.7/plat-linux2’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86</em>64/lib/python2.7/lib-tk’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86<em>64/lib/python2.7/lib-old’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86</em>64/lib/python2.7/lib-dynload’, ‘/home2/dlurie/Enthought/Canopy<em>64bit/User/lib/python2.7/site-packages’, ‘/home2/dlurie/Enthought/Canopy</em>64bit/System/lib/python2.7/site-packages’, ‘/home2/dlurie/Enthought/Canopy<em>64bit/System/lib/python2.7/site-packages/PIL’, ‘/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86</em>64/lib/python2.7/site-packages’]</p>

<h1 id="note-there-is-a-base-memory-of-8486--mb">Note there is a base memory of 84.86  MB</h1>
<p>import numpy as np
npsize = lambda x: x.nbytes/float(1024**3)</p>

<p>nvoxs = 10000
ntpts = 150</p>

<h1 id="there-is-an-increase-to-1007-mb-with-the-time-series">There is an increase to 100.7 MB with the time series</h1>
<p># this is a little larger then it’s 11.5MB size
timeseries = np.random.random((nvoxs,ntpts)).T
timeseries[:,100:104] = 0</p>

<h1 id="to-86373-mb">to 863.73 MB</h1>
<p>corr_matrix = np.zeros((nvoxs,nvoxs))</p>

<h1 id="to-135-gb-even-after-it-is-done">to 1.35 GB (even after it is done)</h1>
<p>np.dot(timeseries.T[:,0:nvoxs], timeseries, out=corr_matrix)</p>

<h1 id="to-247-gb-but-stays-at-209-gb">to 2.47 GB but stays at 2.09 GB</h1>
<p>corr_matrix = np.nan_to_num(corr_matrix)</p>

<h1 id="still-at-209-gb">still at 2.09 GB</h1>
<p>degree_centrality_weighted = np.zeros(nvoxs)</p>

<h1 id="to-368-gb-but-stays-at-284-gb">to 3.68 GB but stays at 2.84 GB</h1>
<p>r_value = 0.2
np.sum(corr_matrix*(corr_matrix &gt; r_value), axis=1, out=degree_centrality_weighted)</p>

<h1 id="note-that-theoretically-we-might-have-thought-a-limit-of-075-gb">Note that theoretically, we might have thought a limit of 0.75 GB</h1>
<p>(nvoxs<em>nvoxs + nvoxs + nvoxs</em>ntpts)*8/float(1024**3)
```</p>

<p>Below is the parallel commands that I ran to monitor the memory usage above.</p>

<p><code>bash
./memtop | grep ipython # get the pid
PID=7873
while [[ True ]]; do
	./memtop -s -p $PID
	sleep 0.5
done
</code></p>

<h2 id="memory-leakage-links">Memory Leakage Links</h2>

<ul>
  <li>http://stackoverflow.com/questions/12422307/reducing-numpy-memory-footprint-in-long-running-application</li>
  <li>http://stackoverflow.com/questions/12461413/why-does-comparison-of-a-numpy-array-with-a-list-consume-so-much-memory</li>
  <li>http://stackoverflow.com/questions/15191391/how-to-avoid-this-four-line-memory-leak-with-numpymkl</li>
</ul>

<h2 id="running-centrality-on-abide">Running Centrality on ABIDE</h2>

<p>The usage approaches 18GB when I set a 12GB limit. On gelert, each node has about 16GB of RAM. The easiest thing might be to run one process on each node with a limit of 6GB to keep well within range.</p>

<h2 id="registering-data-to-3mm-space">Registering Data to 3mm Space</h2>

<p>I also want to redo the registration to be in 3mm standard space, whereas it currently is in 2mm. I’m working on the script for that task and have started running this on gelert now.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Thursday - Week 47]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/21/thursday-week-47/"/>
    <updated>2013-11-21T11:34:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/21/thursday-week-47</id>
    <content type="html"><![CDATA[<p>Today mostly revolved around memory profiling to work in python. Summary is as follows.</p>

<ul>
  <li>Looked into memory profiling
    <ul>
      <li>Had issues with memprof but got heapy to work (links below).</li>
      <li>Got memprof to work not through numpy</li>
      <li>Also applied memory_profiler (a little bit more accurate and useful than memprof).</li>
      <li>Finally also applied memtop (a wrapper around ps)</li>
    </ul>
  </li>
  <li>Attended lab meeting</li>
  <li>Discussion of ABIDE with Chao-Gan and then Cameron</li>
</ul>

<h2 id="memory-profiling">Memory Profiling</h2>

<p>Want to examine the memory usage of centrality in CPAC.</p>

<h3 id="choosing-a-package">Choosing a Package</h3>

<p>There are several python packages available with an extensive list available on stackoverflow: http://stackoverflow.com/questions/110259/which-python-memory-profiler-is-recommended.</p>

<p>Some that do not seem appropriate for our purposes:</p>

<ul>
  <li><a href="https://launchpad.net/meliae">Meliae</a>: No documentation.</li>
  <li><a href="http://pythonhosted.org/Pympler/muppy.html">Muppy</a>: Not that useful.</li>
</ul>

<p>One particular issue that I’ll be dealing with are how well it will work with nipype.</p>

<h3 id="path-issue">Path Issue</h3>

<p>The first problem I faced was the memory profiling not being on the path. This wasn’t so obvious because in the interactive shell of python and python, the memory profiling packages that were recently installed were available. However, when running python (the same python) from the command-line, then it couldn’t find it. The fix was simply to add my python site-packages to the path. Should know in the future that the paths for the interactive vs command-line python can be different.</p>

<h3 id="heapy">Heapy</h3>

<p>So when I actually ran memprof, it ended up failing. I went through all the other packages and was able to get <a href="http://smira.ru/wp-content/uploads/2011/08/heapy.html">heapy</a> to work. The approach taken with heapy is to call an individual function to measure the memory usage, whereas the other approaches all use a decorator of some kind. It worked, but the RAM values all seemed a bit low and not very convenient to read, so I tried another approach.</p>

<h3 id="memprof">Memprof</h3>

<p>Now I’ll be looking at <a href="http://jmdana.github.io/memprof">Memprof</a>, recommended by Cameron and also on the <a href="http://stackoverflow.com/questions/110259/which-python-memory-profiler-is-recommended">stackoverflow page</a>. Since it won’t work with nipype, I manually ran the <code>calc_centrality</code> function. This approach gave me some memory read outs. However, because it doesn’t actually record the memory usage of a particular command (when all the correlations are calculated), the maximum memory is about 160 + 11 or 171MB in total. This is a lot lower than I calculated and much lower than what I saw in top, which maxed out at around 4GB.</p>

<p>[add pics]</p>

<h3 id="memory-profiler">Memory Profiler</h3>

<p>The other approach <a href="https://pypi.python.org/pypi/memory_profiler">memory_profiler</a> was a bit more useful. It showed about 1.4gb being used at the peak (around the correlation calculation).</p>

<h3 id="memtop">Memtop</h3>

<p>I also used a command-line tool <a href="http://www.webhostingtalk.com/showthread.php?t=1256200">memtop</a> that wraps around the command-line tool <code>ps</code>. This showed 4.2gb being used at the peak.</p>
]]></content>
  </entry>
  
</feed>
