<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: abide | Zarrar's Journaling]]></title>
  <link href="http://czarrar.github.io/blog/categories/abide/atom.xml" rel="self"/>
  <link href="http://czarrar.github.io/"/>
  <updated>2013-11-11T18:24:33-05:00</updated>
  <id>http://czarrar.github.io/</id>
  <author>
    <name><![CDATA[Zarrar Shehzad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Monday for Veterans]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/11/monday-for-veterans/"/>
    <updated>2013-11-11T16:26:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/11/monday-for-veterans</id>
    <content type="html"><![CDATA[<h1>ABIDE</h1>

<p>I started running the ABIDE dataset once again! I wonder if a separate post that is an aggregate of my progress would be good? I need to also check up on the mounted data on rocky.</p>

<h2>Pierre Preprocessed Data</h2>

<p>There are 4 different preprocessed data types.</p>

<ol>
<li>No GSR and low-pass filter</li>
<li>No GSR and no low-pass filter</li>
<li>GSR and low-pass filter</li>
<li>GSR and no low-pass filter</li>
</ol>


<p>A json file is included with each preprocessed data. This has regressor information and time-points (which I guess are the time-points that were kept for the analysisâ€¦so there was scrubbing).</p>

<h2>Xinian Preprocessed Data</h2>

<p>Subjects here are labeled by their site. Data includes freesurfer information. For the functional data, I am not totally sure which of the different preprocessed outputs to use. Also will we be making use of the freesurfer registered functional output?</p>

<p>A brief overview shows the following preprocessing factors that were varied:</p>

<ul>
<li>Temporal Filtering (guessing same way as Pierre&rsquo;s)</li>
<li>Smoothnesss (0 or 6 mm FWHM)</li>
</ul>


<p>So for this and the other preprocessed data, I will need to have a working Quick Pack.</p>

<h1>CPAC</h1>

<p>To help better understand why some working directories are being deleted but not others, we made a change to the file for standard output and standard error. Previously, this file held information for all participants. Here are the changes I made to running with SGE that did away with this issue:</p>

<ul>
<li>Run each subject separately with the grid engine (instead of as a job array).</li>
<li>Use a different standard output and error for each subject</li>
<li>Save the list of job ids and corresponding subjects</li>
</ul>


<p>Pretty simple change but big in allowing me to figure out what&rsquo;s going on. Additionally, I can manually check job ids that have completed and knowing the corresponding subjects, I could manually delete the working directories if they have not been deleted already.</p>

<h1>QuickPack</h1>

<p>So where did we leave off. It appears like it will run if I use the GUI but throws an error.</p>

<h1>Other Projects</h1>

<p>I&rsquo;m also still looking at EmotionalBS and is there anything else?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Friday - Save the Date]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/08/friday-save-the-date/"/>
    <updated>2013-11-08T09:28:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/08/friday-save-the-date</id>
    <content type="html"><![CDATA[<h1>ABIDE</h1>

<p>I am continuing to look into the &lsquo;no space left on device&rsquo; error on gelert. First, I&rsquo;ll attempt to understand what subject&rsquo;s were deleted if at all.</p>

<h2>Any Working Directories Deleted?</h2>

<p>I want to first figure out if the working directories are never deleted on gelert, which is potentially an easy fix, or if it is more sporadic. To test this, I will look at the subjects in the output directory but not in any of the working directories on gelert. Below are the 46 subjects that fall in that category indicating that some subjects are at least deleted.</p>

<p><code>
 [1] "0050642" "0050644" "0050650" "0050651" "0050652" "0050653" "0050654"
 [8] "0050655" "0050656" "0050657" "0050660" "0050661" "0050664" "0050666"
[15] "0050669" "0050682" "0050774" "0050776" "0050781" "0050785" "0050789"
[22] "0050791" "0050792" "0050794" "0050799" "0050802" "0050807" "0050808"
[29] "0050815" "0050825" "0050826" "0051456" "0051458" "0051461" "0051463"
[36] "0051464" "0051471" "0051475" "0051478" "0051480" "0051483" "0051485"
[43] "0051487" "0051491" "0051492" "0051493"
</code></p>

<h3>A mix of working directories were removed</h3>

<p>Are these deleted working directories belong to participants with one of the first subject ids? That is are these the first subjects that were run? The answer appears both a yes and no, as you can see from the subject indices.</p>

<p><code>
 [1]   1   3   9  10  11  12  13  14  15  16  19  20  22  24  27  28  94  96 101
[20] 105 109 111 112 114 119 122 127 128 135 145 146 149 151 154 156 157 164 168
[39] 171 173 176 178 180 184 185 186
</code></p>

<h3>The removed directories completed functional preprocessing</h3>

<p>Now I wonder if participant&rsquo;s with deleted working directories have completed workflows. I checked and all the 46 participants had a <code>functional_mni</code> that exists. Here is a sample path for the first participant: <code>/home2/data/Projects/ABIDE_Initiative/CPAC/Output_2013-11-05/sym_links/pipeline_MerrittIsland/_compcor_ncomponents_5_linear1.global1.motion1.quadratic1.compcor1.CSF_0.96_GM_0.7_WM_0.96/0050642_session_1/scan_rest_1_rest/func/functional_mni.nii.gz</code>.</p>

<h3>The kept directories also completed functional preprocessing</h3>

<p>I thought some of these might have failed at generating the <code>functional_mni</code>, however they are all there.</p>

<blockquote><p>Note: the relevant script is <code>x_find_tmps.R</code>.</p></blockquote>

<hr />

<p>You regress out the covariates and</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[thursday lib cmi nyu]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/07/thursday-lib-cmi-nyu/"/>
    <updated>2013-11-07T11:17:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/07/thursday-lib-cmi-nyu</id>
    <content type="html"><![CDATA[<h1>ABIDE</h1>

<p>On gelert, it has been over 2 days and it is now (morning of the 7th) on the 125th to 186th participant. However, it appears that there were many crashes for some participants.</p>

<p>Below are some crash files that I tried. Ultimately, it appears that there is no space left on the device for /tmp. This likely implies that the working directory folder is not being removed when it should. Ugh.</p>

<p>On rocky, the removal of the working directory works fine. I thought I had tried it for gelert too and found it was fine. I see a few possibilities.</p>

<ol>
<li>Some error might occur as CPAC runs, which might lead it to skip removing the directory.</li>
<li>It doesn&rsquo;t actually remove the directory when running through SGE (maybe because it is using a different function). Note that removal of the working directory occurs at the very end in the cpac runner run function.</li>
<li>There&rsquo;s some issue with permissions and so a /tmp directory can be created but not removed. This can be easily tested.</li>
</ol>


<h2>Sample Crash File A</h2>

<p>``` python
import os
os.chdir(&lsquo;/data/Projects/ABIDE_Initiative/CPAC/crash&rsquo;)</p>

<p>from nipype.utils.filemanip import loadflat
crashfile = loadflat(&lsquo;crash-20131107-103920-qli-_apply_ants_3d_warp149.npz&rsquo;)
```</p>

<p>This gives the following error</p>

<p><code>
TraitError: Each element of the 'transformation_series' trait of a WarpImageMultiTransformInputSpec instance must be an existing file name, but a value of '/tmp/resting_preproc_00
50699_session_1/anat_mni_ants_register_0/warp_brain/ants_Warp.nii.gz' &lt;type 'str'&gt; was specified.
</code></p>

<p>It seems like a working directory file for this particular subject does not exist from this error. However, looking at the various functional/derivative outputs in subject&rsquo;s folder that are in standard space, stuff appears the fine:</p>

<ul>
<li>functional: <strong>good</strong></li>
<li>alff: <strong>good</strong></li>
<li>centrality: <strong>good</strong></li>
<li>reho: <strong>good</strong></li>
<li>sca_roi: Is there no standard space SCA map?</li>
<li>dr: Is there no standard space SCA map?</li>
<li>vmhc: <strong>good</strong></li>
</ul>


<p>I believe I might be missing some other relevant outputs.</p>

<h2>Sample Crash File B</h2>

<p>I get the following error for this crash file <code>crash-20131107-114514-qli-_network_centrality_smooth_10.npz</code>.</p>

<p><code>
TraitError: The 'in_file' trait of a MultiImageMathsInput instance must be an existing file name, but a value of '/tmp/resting_preproc_0050702_session_1/centrality_zscore_1/_scan_rest_1_rest/_csf_threshold_0.96/_gm_threshold_0.7/_wm_threshold_0.96/_compcor_ncomponents_5_selector_pc10.linear1.wm0.global0.motion1.quadratic1.gm0.compcor1.csf0/_bandpass_freqs_0.01.0.1/_mask_mask_abide_90percent_gm/z_score/mapflow/_z_score0/degree_centrality_binarize_maths.nii.gz' &lt;type 'str'&gt; was specified.
</code></p>

<p>So it seems like for this participant the fslmaths command was not run on the centrality outputs and later the smoothing was not applied.</p>

<h1>Quick Pack</h1>

<p>Let&rsquo;s recre</p>

<h1>Emotional-BS</h1>

<p>I want to focus on the QC to get that out of the way.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[wednesday nki]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/06/wednesday-nki/"/>
    <updated>2013-11-06T14:59:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/06/wednesday-nki</id>
    <content type="html"><![CDATA[<h1>ABIDE</h1>

<p>We brought over the two drives with preprocessed ABIDE data from Pierre and Xinian to NKI. We attempted to mount the drives to rocky but got a &ldquo;Not Authorized&rdquo; error. I sent an email to Stan with Cameron cc'ed to see if we could deal with this issue (thinks we need sudo privileges).</p>

<p>Cameron will be getting sudo access and will figure out mounting the drive.</p>

<h1>NiLearn</h1>

<p>The subjects for nilearn (ADHD40) appear to be done. I modified my prior compile scripts to take from the new preprocessed output (without nuisance correction and bandpass filtering). I re-uploaded the files to connectir and sent an email to Gael et al. So should be all done here.</p>

<p>I believe I do want to get access to ADHD200 nitrc sftp so I can upload the data to that site. Right now it is on my own connectir nitrc site.</p>

<h1>Emotional-BS</h1>

<p>Read some articles. Check the QC.</p>

<h1>Quick-Pack</h1>

<p>Look into the GUI error&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tuesday]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/05/tuesday/"/>
    <updated>2013-11-05T15:26:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/05/tuesday</id>
    <content type="html"><![CDATA[<h1>NiLearn</h1>

<p>I need to regenerate the 4D output. I previously used data with nuisance co-variates removed, however I should have used the data prior to nuisance correction. Remember the data&rsquo;s location<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>.</p>

<p>So again I want to turn off nuisance correction and bandpass filtering. In order to do this, I&rsquo;m rerunning (on top of the old) the participants with nuisance and bandpass turned off.</p>

<p>Checking on the preprocessing, it initially skipped a bunch of things but now seems to be running some initial steps again. If this takes too long (i.e., is still going into tomorrow), then I might try to apply the warp myself tomorrow.</p>

<hr />

<h1>ABIDE</h1>

<p>Since the QC pages have been fixed, I started to re-run the ABIDE analysis. This time I used 6mm of smoothing, added back the degree centrality, and running each subject with 4 cores on gelert. It is running strong (for now). There are 1102 subjects to run and currently 62 are being run in parallel.</p>

<hr />

<h1>Emotional BS</h1>

<p>I ran the preprocessing for these participants before the QC issue for CPAC was fixed. Now I want to rebuild just the QC as follows:</p>

<p>``` python
import sys
sys.path.insert(0, &lsquo;/home2/data/Projects/CPAC_Regression_Test/nipype-installs/fcp-indi-nipype/running-install/lib/python2.7/site-packages&rsquo;)
sys.path.insert(1, &ldquo;/home/milham/Downloads/cpac_master&rdquo;)</p>

<p>import CPAC
CPAC.utils.create_all_qc.run(&lsquo;/home2/data/Projects/Emotional-BS/processed_data&rsquo;)
```</p>

<p>This does work but I then later noticed that I used 4.5mm as the smoothness instead of 6mm. I may need to rerun this down the line? Since for now this only effects derivatives, which I&rsquo;m not really using, then this shouldn&rsquo;t be an issue.</p>

<p>I also looked into co-registering the two anatomicals. I ended up dropping this endeavor for now since the gain in SNR and benefit to any results should not be too great. In looking at the QC, one possibility is that we may want to replace one of our anatomicals with the other anatomical.</p>

<h2>Multi-Instance Learning</h2>

<p>I did a basic search for relevant reading material. I found the following:</p>

<p><em>A paper outlining the approach in a seemingly easy manner.</em>
<a href="http://pages.ucsd.edu/~ztu/courses/2013_CS_spring/reading/bbabenko_re.pdf">http://pages.ucsd.edu/~ztu/courses/2013_CS_spring/reading/bbabenko_re.pdf</a></p>

<p><em>Very detailed slides that appear to give a good overview.</em>
<a href="http://www.cs.columbia.edu/~andrews/pub/talk-proposal-july8-clean.pdf">http://www.cs.columbia.edu/~andrews/pub/talk-proposal-july8-clean.pdf</a></p>

<p><em>Two links with relevant python code</em><br/>
<a href="http://engr.case.edu/doran_gary/code.html  ">http://engr.case.edu/doran_gary/code.html  </a>
<a href="https://github.com/garydoranjr/misvm">https://github.com/garydoranjr/misvm</a></p>

<hr />

<h1>TODO</h1>

<p>```
[] NiLearn Test Data</p>

<pre><code>[x] Sent email confirming that band-pass filtering is required
[] Get the pre-nuisance variables
</code></pre>

<p>[x] ABIDE Preprocessing</p>

<pre><code>[x] Restart the preprocessing using 4 cores (everything except eigen)
</code></pre>

<p>[x] Emotional BS</p>

<pre><code>[x] Try to regenerate the QC pages based on current output
[x] Lookup some multi-instance learning stuff
[] QC
</code></pre>

<p>[] QuickPack</p>

<pre><code>[] ? (not enough time today to make substantial progress)
</code></pre>

<p>```</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p><code>/home2/data/Projects/CPAC_Regression_Test/2013-10-04_v0-3-2/run/gael-o/package</code><a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
</feed>
