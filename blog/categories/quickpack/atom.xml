<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: quickpack | Zarrar's Journaling]]></title>
  <link href="http://czarrar.github.io/blog/categories/quickpack/atom.xml" rel="self"/>
  <link href="http://czarrar.github.io/"/>
  <updated>2013-11-17T22:44:56-05:00</updated>
  <id>http://czarrar.github.io/</id>
  <author>
    <name><![CDATA[Zarrar Shehzad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Testing Quick Pack]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/15/testing-quick-pack/"/>
    <updated>2013-11-15T15:41:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/15/testing-quick-pack</id>
    <content type="html"><![CDATA[<p>Today, I want to test my different quick pack results with the combined preprocessing + derivates run.</p>

<p>Note that the files for running the preprocessing and derivatives are located in <code>/home2/data/Projects/ABIDE_Initiative/CPAC/abide/config/test_qp</code>.</p>

<p>The files for testing the outputs are in <code>/home2/data/Projects/ABIDE_Initiative/CPAC/abide/tests/quickpack</code>.</p>

<h2 id="failed-comparison">Failed Comparison</h2>

<p>My first run at comparing the ALFF/REHO results failed. So now I’m figuring out what was different in their inputs (assuming that actual computation was the same).</p>

<p>The complete CPAC run uses that following as input for REHO:</p>

<p><code>/data/Projects/ABIDE_Initiative/CPAC/test_qp/All_Working/resting_preproc_0051466_session_1/nuisance_0/_scan_rest_1_rest/_csf_threshold_0.96/_gm_threshold_0.7/_wm_threshold_0.96/_compcor_ncomponents_5_selector_pc10.linear1.wm0.global1.motion1.quadratic1.gm0.compcor1.csf0/residuals/residual.nii.gz</code></p>

<p>So here it appears there is no frequency filtering that was applied. However, it appears the input used in the reho quick pack is not the same although the path looks similar:</p>

<p><code>/home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/All_Output/sym_links/pipeline_MerrittIsland/_compcor_ncomponents_5_linear1.global1.motion1.quadratic1.compcor1.CSF_0.96_GM_0.7_WM_0.96/0051466_session_1/scan_rest_1_rest/func/functional_nuisance_residuals.nii.gz</code></p>

<p>Now since the above file is in the symlinks directory, I found out the original file and it appears it points to a nuisance residuals that has not had global correction.</p>

<p><code>/home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/All_Output/pipeline_MerrittIsland/0051466_session_1/functional_nuisance_residuals/_scan_rest_1_rest/_csf_threshold_0.96/_gm_threshold_0.7/_wm_threshold_0.96/_compcor_ncomponents_5_selector_pc10.linear1.wm0.global0.motion1.quadratic1.gm0.compcor1.csf0/residual.nii.gz</code></p>

<p>If I use the global1 version of things, the files used with All_Output and Reho_Output are now the same. This implies that the symlinks for All_Output should not be used. So for now use only the working directory outputs!!! I should be able to guess these from the sink outputs?</p>

<h2 id="test-1">Test 1</h2>

<script type="math/tex; mode=display"> \sqrt{2*(1-r)} </script>

<h3 id="greater-test">Greater Test</h3>

<p>Match - Game - Point</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Solving Quick Pack]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/13/solving-quick-pack/"/>
    <updated>2013-11-13T12:04:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/13/solving-quick-pack</id>
    <content type="html"><![CDATA[<h1 id="running-alff">Running ALFF</h1>

<p>It all appears to run fine except one link node.</p>

<p><code>
131113-12:02:04,683 workflow ERROR:                                                                                                                                     [575/9353]
         ['Node link_16.a0 failed to run on host rocky.']
131113-12:02:04,684 workflow INFO:
         Saving crash info to /home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/crash/crash-20131113-120204-milham-link_16.a0.npz
131113-12:02:04,685 workflow INFO:
         Traceback (most recent call last):
  File "/home2/data/Projects/CPAC_Regression_Test/nipype-installs/fcp-indi-nipype/running-install/lib/python2.7/site-packages/nipype/pipeline/plugins/multiproc.py", line 18, in r
un_node
    result['result'] = node.run(updatehash=updatehash)
  File "/home2/data/Projects/CPAC_Regression_Test/nipype-installs/fcp-indi-nipype/running-install/lib/python2.7/site-packages/nipype/pipeline/engine.py", line 1282, in run
    self._run_interface()
  File "/home2/data/Projects/CPAC_Regression_Test/nipype-installs/fcp-indi-nipype/running-install/lib/python2.7/site-packages/nipype/pipeline/engine.py", line 1380, in _run_inter
face
    self._result = self._run_command(execute)
  File "/home2/data/Projects/CPAC_Regression_Test/nipype-installs/fcp-indi-nipype/running-install/lib/python2.7/site-packages/nipype/pipeline/engine.py", line 1504, in _run_comma
nd
    result = self._interface.run()
  File "/home2/data/Projects/CPAC_Regression_Test/nipype-installs/fcp-indi-nipype/running-install/lib/python2.7/site-packages/nipype/interfaces/base.py", line 895, in run
    runtime = self._run_interface(runtime)
  File "/home2/data/Projects/CPAC_Regression_Test/nipype-installs/fcp-indi-nipype/running-install/lib/python2.7/site-packages/nipype/interfaces/utility.py", line 412, in _run_int
erface
    out = function_handle(**args)
  File "&lt;string&gt;", line 16, in prepare_symbolic_links
  File "/home2/data/Projects/CPAC_Regression_Test/2013-05-30_cwas/C-PAC/CPAC/utils/utils.py", line 562, in create_symbolic_links
    ext = fname.split('.', 1)[1]
IndexError: list index out of range
Interface Function failed to run. 
</code></p>

<p>The error occurs because a path without a file is passed. It’s a little unclear why this link is even being run? Let’s get some background straight.</p>

<h1 id="what-is-sinked-and-linked">What is sinked and linked?</h1>

<p>So we can see what’s being synced with the following line of code towards the end:</p>

<p><code>python
for strat in strat_list:
	rp = strat.get_resource_pool()
</code></p>

<p>The variable <code>rp</code> is basically a dictionary with the key being a node name (e.g., <code>functional_mni</code>) and the value being a tuple with the actual node and the actual out_file. With my run of ALFF, below are the values of rp:</p>

<p><code>
{'alff_Z_img': (resting_preproc_0051466_session_1.alff_falff_0,
  'outputspec.alff_Z_img'),
 'alff_Z_smooth': (resting_preproc_0051466_session_1.alff_Z_smooth_0,
  'out_file'),
 'alff_Z_to_standard': (resting_preproc_0051466_session_1.alff_Z_to_standard_0,
  'outputspec.out_file'),
 'alff_Z_to_standard_smooth': (resting_preproc_0051466_session_1.alff_Z_to_standard_smooth_0,
  'out_file'),
 'alff_img': (resting_preproc_0051466_session_1.alff_falff_0,
  'outputspec.alff_img'),
 'anatomical_brain': (resting_preproc_0051466_session_1.anatomical_brain_gather_0,
  'outputspec.anat'),
 'anatomical_reorient': (anatomical_reorient_gather_0, 'outputspec.anat'),
 'anatomical_to_mni_nonlinear_xfm': (resting_preproc_0051466_session_1.anatomical_to_mni_nonlinear_xfm_gather_0,
  'outputspec.anat'),
 'ants_affine_xfm': (resting_preproc_0051466_session_1.ants_affine_xfm_gather_0,
  'outputspec.anat'),
 'falff_Z_img': (resting_preproc_0051466_session_1.alff_falff_0,
  'outputspec.falff_Z_img'),
 'falff_Z_smooth': (resting_preproc_0051466_session_1.falff_Z_smooth_0,
  'out_file'),
 'falff_Z_to_standard': (resting_preproc_0051466_session_1.falff_Z_to_standard_0,
  'outputspec.out_file'),
 'falff_Z_to_standard_smooth': (resting_preproc_0051466_session_1.falff_Z_to_standard_smooth_0,
  'out_file'),
 'falff_img': (resting_preproc_0051466_session_1.alff_falff_0,
  'outputspec.falff_img'),
 'functional_brain_mask': (resting_preproc_0051466_session_1.functional_brain_mask_gather_0,
  'outputspec.rest'),
 'functional_brain_mask_to_standard': (resting_preproc_0051466_session_1.functional_brain_mask_to_standard_gather_0,
  'outputspec.rest'),
 'functional_freq_filtered': (resting_preproc_0051466_session_1.functional_freq_filtered_gather_0,
  'outputspec.rest'),
 'functional_mni': (functional_mni_gather_0, 'outputspec.rest'),
 'functional_nuisance_residuals': (resting_preproc_0051466_session_1.functional_nuisance_residuals_gather_0,
  'outputspec.rest'),
 'functional_to_anat_linear_xfm': (resting_preproc_0051466_session_1.functional_to_anat_linear_xfm_gather_0,
  'outputspec.rest'),
 'mean_functional': (mean_functional_gather_0, 'outputspec.rest'),
 'mni_normalized_anatomical': (resting_preproc_0051466_session_1.mni_normalized_anatomical_gather_0,
  'outputspec.anat'),
 'preprocessed': (resting_preproc_0051466_session_1.preprocessed_gather_0,
  'outputspec.rest')}
</code></p>

<p>With that in mind, we can try to understand for the link error, what’s the related node. It appears that it is related to the functional_freq_filtered node name as the input path for linking is <code>/home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/ALFF_Output/pipeline_me/0051466_session_1/functional_freq_filtered/_scan_rest_1_rest/sinker_16</code>.</p>

<p>Since the actually input files like functional_freq_filtered are not copied into the working directory, the sink and link will not work as there is nothing to sink/link from the working directory. How can I tackle this issue?</p>

<ul>
  <li>I could create a workaround to not run sinking/linking for these particular strategies. For instance if preprocessing or what not were not turned on.</li>
  <li>In my get_func… function, I could do a soft-link inside the current directory. However this would need to be a second step, no?</li>
</ul>

<h1 id="issues">Issues</h1>

<h2 id="fix-error">Fix Error</h2>

<p>Spoke to Cameron and will be creating an exclusion or stop list. I’m thinking that as I’m adding the particular files to the strategy pool, I want to add the node name to an exclusion list as well. Yup these set of changes got it going.</p>

<h2 id="symlinks-error">Symlinks Error</h2>

<p>After fixing the prior issue, I then noticed that the symlinks directory is sort-of weird. There’s a lot of repetition. For instance, here are some of the main directories: </p>

<ul>
  <li>scan_rest<em>1_rest_rest</em>1_rest </li>
  <li>scan_rest<em>1_rest_rest</em>1<em>rest_rest</em>1_rest</li>
  <li>scan_rest<em>1_rest_rest</em>1<em>rest_rest</em>1<em>rest_rest</em>1_rest</li>
</ul>

<p>One of these directories as the alff/falff output for native space and with/without smoothing. The second is the output in standard space while the third is the same thing but with smoothing. A little obtuse.</p>

<p>The sink directory on the other hand is fairly normal and easy to work through. So my current thought is to simply use this going forward and make my own symlinks generation script.</p>

<h2 id="duplicate-montage-node-name">Duplicate Montage Node Name</h2>

<p>If I try a config file with all the preprocessing or even one for just alff, I get a similar error.</p>

<p>```
Traceback (most recent call last):
  File “/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86<em>64/lib/python2.7/multiprocessing/process.py”, line 258, in _bootstrap
    self.run()
  File “/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86</em>64/lib/python2.7/multiprocessing/process.py”, line 114, in run
    self.<em>target(*self._args, **self._kwargs)
  File “/home2/data/Projects/CPAC_Regression_Test/2013-05-30_cwas/C-PAC/CPAC/pipeline/cpac_pipeline.py”, line 3649, in prep_workflow
    montage_mni_anat, ‘inputspec.underlay’)
  File “/home2/data/Projects/CPAC_Regression_Test/nipype-installs/fcp-indi-nipype/running-install/lib/python2.7/site-packages/nipype/pipeline/engine.py”, line 306, in connect
    self._check_nodes(newnodes)
  File “/home2/data/Projects/CPAC_Regression_Test/nipype-installs/fcp-indi-nipype/running-install/lib/python2.7/site-packages/nipype/pipeline/engine.py”, line 772, in _check_nodes
    raise IOError(‘Duplicate node name %s found.’ % node.name)
IOError: Duplicate node name montage_mni_anat</em>0 found.
``</p>

<p>So let’s try to play with the code to see what is going on here. </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tuesday]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/05/tuesday/"/>
    <updated>2013-11-05T15:26:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/05/tuesday</id>
    <content type="html"><![CDATA[<h1 id="nilearn">NiLearn</h1>

<p>I need to regenerate the 4D output. I previously used data with nuisance co-variates removed, however I should have used the data prior to nuisance correction. Remember the data’s location<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>.</p>

<p>So again I want to turn off nuisance correction and bandpass filtering. In order to do this, I’m rerunning (on top of the old) the participants with nuisance and bandpass turned off.</p>

<p>Checking on the preprocessing, it initially skipped a bunch of things but now seems to be running some initial steps again. If this takes too long (i.e., is still going into tomorrow), then I might try to apply the warp myself tomorrow.</p>

<hr />

<h1 id="abide">ABIDE</h1>

<p>Since the QC pages have been fixed, I started to re-run the ABIDE analysis. This time I used 6mm of smoothing, added back the degree centrality, and running each subject with 4 cores on gelert. It is running strong (for now). There are 1102 subjects to run and currently 62 are being run in parallel.</p>

<hr />

<h1 id="emotional-bs">Emotional BS</h1>

<p>I ran the preprocessing for these participants before the QC issue for CPAC was fixed. Now I want to rebuild just the QC as follows:</p>

<p>``` python
import sys
sys.path.insert(0, ‘/home2/data/Projects/CPAC_Regression_Test/nipype-installs/fcp-indi-nipype/running-install/lib/python2.7/site-packages’)
sys.path.insert(1, “/home/milham/Downloads/cpac_master”)</p>

<p>import CPAC
CPAC.utils.create_all_qc.run(‘/home2/data/Projects/Emotional-BS/processed_data’)
```</p>

<p>This does work but I then later noticed that I used 4.5mm as the smoothness instead of 6mm. I may need to rerun this down the line? Since for now this only effects derivatives, which I’m not really using, then this shouldn’t be an issue.</p>

<p>I also looked into co-registering the two anatomicals. I ended up dropping this endeavor for now since the gain in SNR and benefit to any results should not be too great. In looking at the QC, one possibility is that we may want to replace one of our anatomicals with the other anatomical.</p>

<h2 id="multi-instance-learning">Multi-Instance Learning</h2>

<p>I did a basic search for relevant reading material. I found the following:</p>

<p><em>A paper outlining the approach in a seemingly easy manner.</em>
http://pages.ucsd.edu/~ztu/courses/2013_CS_spring/reading/bbabenko_re.pdf</p>

<p><em>Very detailed slides that appear to give a good overview.</em>
http://www.cs.columbia.edu/~andrews/pub/talk-proposal-july8-clean.pdf</p>

<p><em>Two links with relevant python code</em><br />
http://engr.case.edu/doran_gary/code.html<br />
https://github.com/garydoranjr/misvm</p>

<hr />

<h1 id="todo">TODO</h1>

<p><code>
[] NiLearn Test Data
	[x] Sent email confirming that band-pass filtering is required
	[] Get the pre-nuisance variables
[x] ABIDE Preprocessing
	[x] Restart the preprocessing using 4 cores (everything except eigen)
[x] Emotional BS
	[x] Try to regenerate the QC pages based on current output
	[x] Lookup some multi-instance learning stuff
	[] QC
[] QuickPack
	[] ? (not enough time today to make substantial progress)
</code></p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p><code>/home2/data/Projects/CPAC_Regression_Test/2013-10-04_v0-3-2/run/gael-o/package</code><a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Monday]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/04/monday/"/>
    <updated>2013-11-04T12:20:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/04/monday</id>
    <content type="html"><![CDATA[<h1 id="nilearn-test-data">NiLearn Test Data</h1>

<p>Note that the code to generate all this is as well as the actual package is in <code>/home/data/Projects/CPAC_Regression_Test/2013-10-04_v0-3-2/run/gael-o/package</code>.</p>

<p>I’ve already gotten the subject data and regressors. I am left to create the group phenotypic file. It appears that I don’t need to duplicate the previous setup so I can use what we have on motion per subject. With this new data packaged up, I uploaded it to my connectir repository as I still don’t have access to the ADHD200 repository.</p>

<p>The download links are below:</p>

<p>http://connectir.projects.nitrc.org/adhd40_p1.nii.gz
http://connectir.projects.nitrc.org/adhd40_p2.nii.gz
http://connectir.projects.nitrc.org/adhd40_p3.nii.gz
http://connectir.projects.nitrc.org/adhd40_p4.nii.gz</p>

<p>And I have contacted the other peeps about this. So DONE.</p>

<hr />

<h1 id="abide-preprocessing">ABIDE Preprocessing</h1>

<p>I found out that the QC pages are a little broken. So this will have to wait.</p>

<h2 id="run-config-setup">Run Config Setup</h2>

<p>I have adjusted this file to run one subject for every two nodes. I am using FSL5 now. Preprocessing will be run as well as all the derivatives except centrality (degree or eigen). This is because they both need a decent amount of RAM and computational time so I’ll wait to run them separately later.</p>

<hr />

<h1 id="quickpack">QuickPack</h1>

<p>So the first general issue that I’m experiencing is that if I run everything (no quick-pack), then I don’t get any of the sym links. However, in my Emotional-BS run, the symlinks all seem to be fine! A little weird. I’m not sure what is causing the problem.</p>

<h2 id="running-with-just-cpac">Running with just CPAC</h2>

<p>I’m focusing on why I’m getting symlink issues when I run internally with CPAC. I think the reasons that no files are being symlinked is because there might be single quotes in the strategy part of the path.</p>

<p><code>bash
ln -s /home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/All_Output/pipeline_RameyBorough/0051466_session_1/qc/mni_normalized_anatomical_a/mni_anat_a.png /home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/All_Output/sym_links/pipeline_RameyBorough/_compcor_'ncomponents_5_linear1.global1.motion1.quadratic1.compcor1.csf0'_CSF_0.96_GM_0.7_WM_0.96/0051466_session_1/scan/qc/mni_anat_a.png
</code></p>

<p>If I run this command that was spit out from CPAC, then I get a “No such file or directory” error. This error doesn’t appear with CPAC but this might explain why no symlinks were generated.</p>

<p>Now we must understand why those single quotes are there. We should note that there were actually 2 strategies that I had given but only one strategy folder in the symlinks directory folder was created. I wonder if this has to do with some error?</p>

<p>I tried to open the config file with the GUI and re-save it, seeing if that would help but it didn’t. Now I’ll try to run the pipeline from the GUI. Yup that fixed the issue and it isn’t related to the fact that I’m using Dan’s path and shiz. Running this one subject through the GUI.</p>

<h2 id="running-alff">Running ALFF</h2>

<p>There was one previous and confusing link issue. However, another issue that I didn’t notice was the ALFF output directory using single quotes in its symlinks strategy directory and only has one of the two strategies. This suggests a potential benefit in running this through the GUI.</p>

<h3 id="gui">GUI</h3>

<p>This run failed. The error message is below. I am not sure what’s going on.</p>

<p><code>
Invalid Connection: ALFF: 0  resource_pool:  {}
Process Process-3:
Traceback (most recent call last):
  File "/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86_64/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86_64/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "/home2/dlurie/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/CPAC/pipeline/cpac_pipeline.py", line 1198, in prep_workflow
    alff, 'inputspec.rest_res')
  File "/home2/dlurie/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/nipype/pipeline/engine.py", line 306, in connect
    self._check_nodes(newnodes)
  File "/home2/dlurie/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/nipype/pipeline/engine.py", line 769, in _check_nodes
    if node.name in node_names:
AttributeError: 'NoneType' object has no attribute 'name'
</code></p>

<h3 id="cli">CLI</h3>

<p>The error on the command-line is driven in part by two correction strategies (with and without global). Since my quick pack, can’t really do both of those strategies at once anyway, why not just have one strategy specified. Trying this with ALFF and that didn’t work! Ugh so it’s specifically related to the command-line, which is doing something differently than the GUI.</p>

<hr />

<h1 id="emotional-bs">Emotional-BS</h1>

<p>The preprocessing is done but the QC pages were not generated due to some issue in CPAC. I wish there was some way of running the QC scripts with the current outputs without me having to re-run all of CPAC. Since I ran via gelert using the <code>/tmp</code> directory, it will also need to redo all the processing.</p>

<hr />

<h1 id="random">Random</h1>

<p>I spent a bit of time getting the table of contents to work on these pages. Hopefully it was worth it (I might want to also add a go back to the top link for each header). At the very least it gives a sort-of summary list of what will be on the post.</p>

<p>I think I also had a bit of trouble keeping on track today. It was very frustrating and I’m hoping it was just re-adjusting to coming back to work after a brief break.</p>

<hr />

<h1 id="todo">TODO</h1>

<p><code>
[x] NiLearn Test Data
	[x] Generate group phenotype file
[] ABIDE Preprocessing
	[x] Email Cam about next steps
	[x] Setup the pipeline with gelert excluding centrality
	[] Start running the pipeline
[] QuickPack
	[x] Check that complete run was good
	[x] Try to figure out why the old runs did fine?
	[] Using the new complete run as input for QP
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Another Quick Pack Journey]]></title>
    <link href="http://czarrar.github.io/blog/2013/10/30/another-look-at-quick-pack-journey/"/>
    <updated>2013-10-30T15:55:00-04:00</updated>
    <id>http://czarrar.github.io/blog/2013/10/30/another-look-at-quick-pack-journey</id>
    <content type="html"><![CDATA[<p>Today, I’ve got through a few things. First, I finished compiling the <code>regressors.csv</code> file for </p>

<p>Some minor things, I helped Krishna with some ROI error he had in CPAC. It turned out that his issue was due to incorrectly binarizing the mask.</p>

<h1 id="quickpack">QuickPack</h1>

<p>Trying my hand again on quick pack today. Last time I got a bunch of different errors that I really couldn’t track. I am re-running the complete run to see what errors I get again and try to figure those out. I’m also rerunning the alff to see what happens there.</p>

<h2 id="complete-run">Complete Run</h2>

<p>This appears to be re-running smoothly (knock on wood). The last time I ran a complete (preprocessing + derivatives) run, it led to some unclear problems.</p>

<h2 id="alff-qp">ALFF QP</h2>

<p>I get the same link error as I did before. The error occurs on line 562 in <code>utils.py</code> within the <code>create_symbolic_links</code>. The line and error are</p>

<pre><code>ext = fname.split('.', 1)[1]
IndexError: list index out of range
</code></pre>

<p>Basically <code>fname</code> should be a filename but a directory is passed instead and so it fails. Specifically, it appears to be passing <code>/home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/ALFF_Output/pipeline_0/0051466_session_1/functional_freq_filtered/_scan_rest_1_rest/sinker_16</code>. I don’t have much sense about what is going on with this file.</p>

<p>Other bits of information:</p>

<ul>
  <li>The crash file is located <code>/home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/crash/crash-20131030-145001-milham-link_16.a0.np
z</code>.</li>
  <li>The directory in the working directory is <code>/data/Projects/ABIDE_Initiative/CPAC/test_qp/ALFF_Working/resting_preproc_0051466_session_1/_scan_rest_1_rest/link_16</code></li>
</ul>

<h3 id="steve-talk">Steve Talk</h3>

<p>I discussed this issue with Steve and it seems I will need to get down and dirty with nipype to understand the origin of this problem.</p>

<h2 id="reho-qp">REHO QP</h2>

<p>Here I got two errors. One of the errors was the same as with ALFF except instead of link<em>16 it is link</em>5. The other error is new and specific to REHO with the crash file: <code>/home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/crash/crash-20131030-164123-milham-reho_map.a0.a0.npz</code>. It appears the error is an empty input filename being passed to the <code>compute_reho</code> in <code>reho/utils</code>.</p>

<p>A snapshot of the error is given below:</p>

<pre><code>/data/Projects/ABIDE_Initiative/CPAC/test_qp/Reho_Working/resting_preproc_0051466_session_1/reho_0/_scan_rest_1_rest/_scan_rest_1_rest/reho_map/&lt;string&gt; in compute_reho(in_file, mask_file, cluster_size)

/home/data/PublicProgram/epd-7.2-2-rh5-x86_64/lib/python2.7/site-packages/nibabel/loadsave.pyc in load(filename)
     37     except KeyError:
     38         raise ImageFileError('Cannot work out file type of "%s"' %
---&gt; 39                              filename)
     40     if ext in ('.nii', '.mnc', '.mgh', '.mgz'):
     41         klass = class_map[img_type]['class']

ImageFileError: Cannot work out file type of ""
Interface Function failed to run. 
</code></pre>

<p>To run the crash file, see the code below.</p>

<p>``` python
import sys
sys.path.insert(0, ‘/home2/data/Projects/CPAC_Regression_Test/nipype-installs/fcp-indi-nipype/running-install/lib/python2.7/site-packages’)
sys.path.insert(1, “/home2/data/Projects/CPAC_Regression_Test/2013-05-30_cwas/C-PAC”)
import CPAC
import CPAC.reho.utils
from nipype.utils.filemanip import loadflat</p>

<p>crashinfo = loadflat(“/home2/data/Projects/ABIDE_Initiative/CPAC/test_qp/crash/crash-20131030-164123-milham-reho_map.a0.a0.npz”)
crashinfo[‘node’].run()
```</p>

<h1 id="preprocessing-emotionalbs">Preprocessing EmotionalBS</h1>

<p>I am going to preprocess the emotionalBS data again with CPAC+ANTS. Some relevant parameters are:</p>

<ul>
  <li>Used ANTS for registration</li>
  <li>Two nuisance correction strategies
      * compcor+motion+linear+quadratic
      * compcor+global+motion+linear+quadratic</li>
  <li>Both frequency filtering (0.01-0.1 Hz) and no filtering</li>
  <li>Smoothing of 4.5mm (only for derivatives)</li>
  <li>Generated some SCA and drSCA based derivates using ROIs/maps from the ABIDE preprocessing. Otherwise no other derivatives were created.</li>
</ul>

<h2 id="install-fsl-5-for-gelert">Install FSL 5 for Gelert</h2>

<p>I was a little confused on how to install fsl 5 to a custom path (<code>/home2/data/PublicProgram</code>). It seemed to go through neurodebian and apt-get, I needed certain admin privileges that even Mike’s account didn’t have. And even if I had those, it wasn’t clear how I would install to a custom directory. Then I realized I could just copy the current fsl5 in <code>/usr/share/fsl/5.o</code> into that directory. The reason for this (<code>/home2/data/PublicProgram</code>) directory is so </p>

<h1 id="new-journal">New Journal</h1>

<p>I also worked on setting up this blog/journal with github user account. The goal is to chronicle my work efforts with particular thought on any issues I encounter. A major side benefit will be easier communication with others in the lab (e.g., was able to easily show the error for ALFF quick pack to Steve). Previously, I had been using different journal pages for each of my projects. I ended up being a big confused where to post stuff and often lost track of the markdown pages that I had created before they were published. With one journal site, this should all be easier.</p>

<p>I’m also testing out this slightly different jekyll wrapper (octopress). I unfortunately spent a lot longer (2 hours) setting it up cuz of a git error then I had wanted. So I hope some of its benefits over jekyll bootstrap such as easier syncing, simpler interface, and simpler integration of more advanced plugins, will be useful in the long run. That way I can justify my time spent installing it!</p>
]]></content>
  </entry>
  
</feed>
