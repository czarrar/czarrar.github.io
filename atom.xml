<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Zarrar's Journaling]]></title>
  <link href="http://czarrar.github.io/atom.xml" rel="self"/>
  <link href="http://czarrar.github.io/"/>
  <updated>2013-12-09T20:11:39-05:00</updated>
  <id>http://czarrar.github.io/</id>
  <author>
    <name><![CDATA[Zarrar Shehzad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Week 50 - Monday]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/09/week-50-monday/"/>
    <updated>2013-12-09T20:03:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/09/week-50-monday</id>
    <content type="html"><![CDATA[<h2 id="plansummary">Plan/Summary</h2>

<ul>
  <li>[x] Email Maarten with the results</li>
  <li>[x] Add running Xinian’s filtering script to my Basecamp TODO</li>
  <li>[x] Integrate updated CWAS code into master CPAC branch</li>
  <li>[] Tagging (e 1 hr)</li>
  <li>[] QC (e 1 hr)</li>
  <li>[] Finish fast eigenvector centrality code (not workflow)</li>
</ul>

<p>While above were items that I had planned in the morning, items below reflect additions to this plan during the day.</p>

<ul>
  <li>[x] cleaned up the code to use regression test data to run a python CPAC and R version of CWAS.</li>
  <li>[x] wrote code to smooth 4D functional data in standard space</li>
  <li>[x] write code to use smoothed data for CWAS</li>
  <li>[x] write some additional code to use internal CPAC filepaths for CWAS</li>
  <li>[x] write code to parse group analysis model inputs for CWAS</li>
</ul>

<p>The coding for CWAS in CPAC went surprisingly well. Hopefully, this streak might continue tomorrow when testing.</p>

<h2 id="integrating-cwas-code-with-cpac">Integrating CWAS Code with CPAC</h2>

<p>My issue in using the main CPAC repository was that although I was a member, I needed to be made an owner. After figuring this out, I created a ‘cwas’ branch and added the relevant changes to the cwas folder.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CWAS Simulations]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/07/cwas-simulations/"/>
    <updated>2013-12-07T16:39:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/07/cwas-simulations</id>
    <content type="html"><![CDATA[<p>It seems that my previous set of simulations may go down the drain as they weren’t relevant to the reviewer’s concern.</p>

<p>So now, I am looking into a new set of simulations focused on examining properties of the MDMR approach using resting-state functional connectivity data.</p>

<h2 id="methods">Methods</h2>

<p>One question is how can we implement our simulations. Here are the general steps:</p>

<ol>
  <li>
    <p>Compute 1000 voxelwise connectivity maps at 4mm isotropic for all my 104 participants.</p>
  </li>
  <li>
    <p>Using GLM remove the effects of other variables to get residual connectivity maps</p>
  </li>
  <li>
    <p>Add in group effect and vary certain factors</p>
  </li>
</ol>

<p>Step three is crucial and is discussed later.</p>

<h3 id="factors">Factors</h3>

<p>What are the factors that we want to measure?</p>

<ul>
  <li>Number of ‘clusters’ that differ</li>
  <li>Extent of each ‘cluster’ that differ</li>
  <li>Effect size</li>
</ul>

<p>Another set of factors that could be examined is the variation in the effect size across the connectivity map. For now, I guess we shall assume a constant difference with some random noise.</p>

<p>Do we really need to be examining some of these steps? I’m thinking that we just focus on the effect size and simply generate a gaussian random field </p>

<h3 id="implementation">Implementation</h3>

<p>I’m assuming that I have a matrix with each participant’s connectivity map at one voxel. </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Week 49 - Friday]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/06/week-49-friday/"/>
    <updated>2013-12-06T10:35:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/06/week-49-friday</id>
    <content type="html"><![CDATA[<h2 id="plan">Plan</h2>

<ul>
  <li>ABIDE
    <ul>
      <li>[] QC (1 hour)</li>
    </ul>
  </li>
  <li>CPAC
    <ul>
      <li>[] Finish adapting fast eigenvector code (30 mins)</li>
      <li>[] Write SVD version of eigenvector (1 hour)</li>
      <li>[] Test new eigenvector code (2 hours)</li>
      <li>[] @Steve CWAS Regression Test (1 hour)</li>
    </ul>
  </li>
  <li>Tagging
    <ul>
      <li>[] Meeting to discuss criteria (1 hour)</li>
    </ul>
  </li>
</ul>

<p>Look into SVD (20 mins). But it’s slower.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">svd</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">scipy.sparse.linalg</span> <span class="kn">import</span> <span class="n">eigsh</span>
</span><span class="line"><span class="n">mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span><span class="mi">1000</span><span class="p">))</span>
</span><span class="line"><span class="o">%</span><span class="n">timeit</span> <span class="n">u</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">v</span> <span class="o">=</span> <span class="n">svd</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
</span><span class="line"><span class="o">%</span><span class="n">timeit</span> <span class="n">evalue</span><span class="p">,</span> <span class="n">evector</span> <span class="o">=</span> <span class="n">eigsh</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mat</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s">&#39;LM&#39;</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>In this case the current approach took 45.2ms and the SVD approach took 135ms.</p>

<h2 id="fast-eigenvector-centrality">Fast Eigenvector Centrality</h2>

<p>The plan for testing would be to first make sure that the code results are close to the original approach and possibly also compare to an SVD approach.</p>

<h2 id="brain-size-ala-maarten">Brain Size ala Maarten</h2>

<p>We ended up dropping the ball in getting Maarten some of these scripts.</p>

<p>I need to create an input file with <code>T1file,age,sex</code>. If I am using the NKI Rockland pilot dataset, then I believe I should be able to find the relevant path. For the most recent data, there have been additions so let me confirm that I have access to everything. Ok so first, pilot dataset.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="r"><span class="line">rawdir    <span class="o">&lt;-</span> <span class="s">&#39;/home2/data/Originals/Rockland/raw&#39;</span>
</span><span class="line">phenofile <span class="o">&lt;-</span> <span class="s">&#39;/home2/data/Originals/Rockland/NKI.1-39_phenotypic.csv&#39;</span>
</span><span class="line">
</span><span class="line">pheno     <span class="o">&lt;-</span> read.csv<span class="p">(</span>phenofile<span class="p">)</span>
</span><span class="line">df        <span class="o">&lt;-</span> pheno<span class="p">[,</span>c<span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">3</span><span class="p">,</span><span class="m">12</span><span class="p">)]</span>
</span><span class="line">df<span class="o">$</span>paths  <span class="o">&lt;-</span> file.path<span class="p">(</span>rawdir<span class="p">,</span> df<span class="o">$</span>Subject<span class="p">,</span> <span class="s">&quot;touse&quot;</span><span class="p">,</span> <span class="s">&quot;anat&quot;</span><span class="p">,</span> <span class="s">&quot;mprage.nii.gz&quot;</span><span class="p">)</span>
</span><span class="line">df<span class="o">$</span>exists <span class="o">&lt;-</span> file.exists<span class="p">(</span>df<span class="o">$</span>paths<span class="p">)</span><span class="o">*</span><span class="m">1</span>
</span><span class="line">
</span><span class="line">df.txt <span class="o">&lt;-</span> df<span class="p">[</span>df<span class="o">$</span>exists<span class="o">==</span><span class="m">1</span><span class="p">,</span>c<span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">3</span><span class="p">,</span><span class="m">2</span><span class="p">)]</span>
</span><span class="line">write.table<span class="p">(</span>df.txt<span class="p">,</span> file<span class="o">=</span><span class="s">&quot;rockland_pilot.txt&quot;</span><span class="p">,</span> row.names<span class="o">=</span><span class="k-Variable">F</span><span class="p">,</span> col.names<span class="o">=</span><span class="k-Variable">F</span><span class="p">,</span> quote<span class="o">=</span><span class="k-Variable">F</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I want to now look at the Enhanced dataset.</p>

<h2 id="thoughts">Thoughts</h2>

<h3 id="plan-better">Plan Better</h3>

<p>Last few days I feel that I haven’t gotten through as much as I could have. Possibly due to not sleeping as much and partly due to more socializing like with the speaker’s visit and a new office mate. Let’s try differently today.</p>

<h3 id="link-to-pages-by-week-and-month">Link to pages by week and month</h3>

<p>Another nice thing would be to have a summary page at the end of the week with links to all the other pages that week (might want to check if those exist?) and similarly at the end of the month could have a page with the lists for that month. I wonder if this could be within the pages section?</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Week 49 - Thursday]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/05/week-49-thursday/"/>
    <updated>2013-12-05T11:48:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/05/week-49-thursday</id>
    <content type="html"><![CDATA[<ul>
  <li>ABIDE
    <ul>
      <li>[x] QC (1 hour)</li>
    </ul>
  </li>
  <li>Fast Eigenvector Centrality
    <ul>
      <li>[x] Tried installing the C++ version (2 hrs)</li>
    </ul>
  </li>
  <li>Tagging</li>
</ul>

<h2 id="abide">ABIDE</h2>

<p>I am still waiting on Xinian to get back to me about his global signal regressed data (getting filtered vs non-filtered data).</p>

<p>I want to also get some QC done today.</p>

<h2 id="cpac">CPAC</h2>

<h3 id="fecm">fECM</h3>

<p>I was tried to install the fast eigenvector centrality code but had problems with importing all the C++11 headers on my mac. It wasn’t totally clear what was wrong to me except that the mac doesn’t really ship with the latest gear. So I contacted Stan to install a very recent version of gcc on rocky and I tried to install the latest version of gcc on my computer through home-brew. Still waiting on the first option and for the second, after some work, I was able to start the following command <code>brew install --enable-cxx --enable-fortran gcc49</code>. This failure is a little disappointing because it seems so simple but I’ve now hung up my towel for the moment and going to focus on my other tasks.</p>

<h2 id="tagging">Tagging</h2>

<p>I really want to get at least one hour of tagging into the mix here.</p>

<h2 id="general">General</h2>

<p>I emailed Dan Dickstein suggesting to use the R version of CWAS for now and pointed him to an install script that I have on github.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Week 49 - Wednesday]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/04/week-49-wednesday/"/>
    <updated>2013-12-04T10:02:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/04/week-49-wednesday</id>
    <content type="html"><![CDATA[<h2 id="summarytodo">Summary/TODO</h2>

<ul>
  <li>ABIDE
    <ul>
      <li>[x] QC (1 hour)</li>
    </ul>
  </li>
  <li>CPAC (related to ABIDE)
    <ul>
      <li>[] Functional Density Mapping</li>
      <li>[] Fast Eigenvector Centrality
        <ul>
          <li>[x] Respond to Wink</li>
          <li>[x] Tried installing the C++ version (1 hr)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>[] Tagging</li>
  <li>Dan Dickstein CWAS
    <ul>
      <li>[x] Get details from Dan</li>
      <li>[x] Phil Meeting about CWAS Paper. Discussed way to get confidence intervals for simulation power analysis. (1 hr)</li>
    </ul>
  </li>
  <li>CWAS with Steve. Worked out a plan to have a workable version of CWAS into the next release (30 min)</li>
  <li>Went to talk with Matt Hutchinson. Asked a super intelligent question. (1-2 hrs)</li>
</ul>

<h2 id="abide-qc">ABIDE QC</h2>

<p>Start at 10am till 11am.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Week 49 - Tuesday]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/03/week-49-tuesday/"/>
    <updated>2013-12-03T11:30:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/03/week-49-tuesday</id>
    <content type="html"><![CDATA[<p>testing draft</p>

<h2 id="summarytodo">Summary/TODO</h2>

<ul>
  <li>ABIDE
    <ul>
      <li>[x] Create quick pack for degree centrality</li>
      <li>[x] Email Xinian about GSR data and having filtered/non-filtered</li>
      <li>[x] Finish creating quick pack for LFCD</li>
    </ul>
  </li>
  <li>CPAC
    <ul>
      <li>[] Run an initial test of the functional node</li>
    </ul>
  </li>
  <li>Tag</li>
</ul>

<p>I also took the speaker (Matt) out to lunch with Zhen, and then showed him some of my slides about CWAS.</p>

<h2 id="quick-pack-for-centrality">Quick Pack for Centrality</h2>

<h3 id="cpac-preprocessed-data">CPAC Preprocessed Data</h3>

<p>The scripts for this analysis are located in <code>/data/Projects/ABIDE_Initiative/CPAC/abide/config/30_centrality</code>. I was able to first generate a subject list YAML file with <code>10_gen_quick_pack.py</code>. Below is a sample of what a file for one subject looks like:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class=""><span class="line">  functional_brain_mask: {filt_global: /home2/data/Projects/ABIDE_Initiative/CPAC/Output_2013-11-22/pipeline_MerrittIsland/0050642_session_1/functional_brain_mask/_scan_rest_1_rest/rest_3dc_tshift_RPI_3dv_automask.nii.gz,
</span><span class="line">    filt_noglobal: /home2/data/Projects/ABIDE_Initiative/CPAC/Output_2013-11-22/pipeline_MerrittIsland/0050642_session_1/functional_brain_mask/_scan_rest_1_rest/rest_3dc_tshift_RPI_3dv_automask.nii.gz,
</span><span class="line">    nofilt_global: /home2/data/Projects/ABIDE_Initiative/CPAC/Output_2013-11-22/pipeline_MerrittIsland/0050642_session_1/functional_brain_mask/_scan_rest_1_rest/rest_3dc_tshift_RPI_3dv_automask.nii.gz,
</span><span class="line">    nofilt_noglobal: /home2/data/Projects/ABIDE_Initiative/CPAC/Output_2013-11-22/pipeline_MerrittIsland/0050642_session_1/functional_brain_mask/_scan_rest_1_rest/rest_3dc_tshift_RPI_3dv_automask.nii.gz}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We can see that we have each filtering and global option as the input. This is a little bit of cheating since the different input keys are interpreted as different scans.</p>

<p>We can then use these different inputs in one CPAC run with <code>20_run_gen_quick_pack.py</code>. This script is currently running and the output/working directories can be found in <code>/data/Projects/ABIDE_Initiative/Derivatives/CPAC/Cent</code>.</p>

<h2 id="eigenvector-centrality">Eigenvector Centrality</h2>

<p>I emailed out to the author of the fast implementation paper to ask for his C++ code. However, if the author does not respond, the paper is fairly descriptive and I imagine it would be possible to write our own code based on the methods description.</p>

<h2 id="qc">QC</h2>

<p>I created the new excel file and looked through one subject. Should confirm the following: should have columns for anat, func, and reg with comments for each of those, and a final status column and final comments column. Do I need anything for the segmentations?</p>

<h2 id="functional-density-mapping">Functional Density Mapping</h2>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Week 49 - Monday]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/02/week-49-monday/"/>
    <updated>2013-12-02T13:25:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/02/week-49-monday</id>
    <content type="html"><![CDATA[<h2 id="summary">Summary</h2>

<ul>
  <li>Checked and finalized fixing a subject.</li>
  <li>Resolved questions with QC</li>
  <li>Discussed issue with VMHC</li>
  <li>Setup quick pack for Xinian’s dataset</li>
  <li>Read through the eigenvector centrality paper</li>
  <li>Went through a few articles for tagging with updated instructions</li>
</ul>

<h2 id="todos">TODOs</h2>

<ul>
  <li>ABIDE
    <ul>
      <li>Check on fixed subject</li>
      <li>Email about QC</li>
      <li>Look into VMHC issue on github</li>
      <li>Create quick packs for everyone</li>
      <li>Email to ask about eigenvector centrality</li>
    </ul>
  </li>
  <li>Check fixed subject</li>
  <li>Find out about tagging</li>
</ul>

<h2 id="abide---fixed-subject">ABIDE - Fixed Subject</h2>

<p>I reran one participant with missing derivatives. It appears that everything is good.</p>

<h2 id="abide---quick-packs">ABIDE - Quick Packs</h2>

<p>We are looking to have the following derivatives:</p>

<ul>
  <li>Timeseries for different regions of interest</li>
  <li>Dual Regression maps</li>
  <li>ALFF/fALFF</li>
  <li>REHO</li>
  <li>Degree/Eigen Centrality</li>
  <li>VMHC</li>
</ul>

<h3 id="cameron">Cameron</h3>

<p>For our data, we already have most of the above derivatives and need the following:</p>

<ul>
  <li>Degree/Eigen Centrality</li>
  <li>VMHC</li>
</ul>

<p>Degree I should be able to run now but need to wait on Eigen and VMHC.</p>

<h3 id="xinian">Xinian</h3>

<p>I started to create the quick pack for this dataset. The one issue is that it’s unclear whether the global signal correction was applied to either the filtered or non-filtered data. It should be applied to both so will need to contact some people in order to figure out how to resolve this issue. Hopefully no heads will roll.</p>

<p>The scripts and what not for this dataset quick pack are located in <code>/data/Projects/ABIDE_Initiative/CPAC/abide/config/32_xinian</code>. I still need to write some code to generate the standard brain mask and of course test it all as well.</p>

<h3 id="pierre">Pierre</h3>

<p>Pierre’s is already in standard space so I believe I should simply be able to turn off <code>runRegisterFuncToMNI</code> and also my custom option <code>applyRegisterFuncToMNI</code>. The regular outputs should be good enough.</p>

<h3 id="chao-gan">Chao-Gan</h3>

<p>Check with Chao-Gan.</p>

<h2 id="abide---qc">ABIDE - QC</h2>

<p>I should do a similar approach as Yang and reference an email from Pierre with some guidelines.</p>

<h2 id="eigenvector-centrality">Eigenvector Centrality</h2>

<p>I did a bit of reading last time about a faster approach to eigenvector centrality. Now I went through the main paper again. It appears that their speed-up is primarily by not having to compute the cross product between a matrix of time series (voxels x time points). They split this step up into two parts that reduces the complexity from N^2 to 2NT where N = voxels and T = time points. They make a claim that for voxelwise data, they see a speedup of 1000x.</p>

<h2 id="tagging">Tagging</h2>

<p>Spoke to Cameron. We discussed some potential changes and I emailed these out to him and Matt.</p>

<p>In terms of tagging, here were some important points to note:</p>

<ul>
  <li>use mike’s opinions as the standard</li>
  <li>basic neuroscience is a superset of brain/behavior</li>
  <li>only include resting-state fMRI studies!</li>
</ul>

<h2 id="cwas">CWAS</h2>

<p>I got emails from Xavier and Phil with comments. I should look to integrate them.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CWAS Simulations]]></title>
    <link href="http://czarrar.github.io/blog/2013/12/01/cwas-simulations/"/>
    <updated>2013-12-01T21:01:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/12/01/cwas-simulations</id>
    <content type="html"><![CDATA[<p>I should check the data that I am plotted. Right now, I am examining the mean -log10p value of the MDMR results but I think I might want to look at the percent of significant results with each effect size. Should double check.</p>

<p>The code for the simulation analysis is located in <code>/home/data/Projects/CWAS/share/simulations</code>. I have added multiprocessing functionality to speed up the ability to go through the different parameters and iterations.</p>

<p>I have started running one iteration of things including both variables of interest. Holy crap, it finished in like 30mins, super quick. I saved the values in an rda file within the same folder. The contents of that rda are as follows:</p>

<ul>
  <li>logp.cov1</li>
  <li>fstat.cov1</li>
  <li>logp.cov0</li>
  <li>fstat.cov0</li>
</ul>

<p>The <code>cov1</code> term means that the covariate was modeled whereas <code>cov0</code> means the covariate wasn’t modeled. For <code>cov1</code> matrices, the dimensions were 2 terms x 11 group difference x 11 correlation. For <code>cov0</code> matrices, the dimensions were 11 group difference x 11 correlation (only for the group difference term). The correlation was between the age covariate and the response.</p>

<h2 id="plot">Plot</h2>

<p>I generated plots of the results on my computer using the <code>.../pro42/scripts2add/simulations/20_plot.R</code> script. Since I used Rnotebook, I was able to publish those results online at http://rpubs.com/czarrar/cwas_simulations.</p>

<h2 id="status">Status</h2>

<p>So I had to rerun the analyses since I should have been calculating power as percent of significant iterations (p &lt; 05). I upped the number of iterations to 20, hopefully this is enough.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Week 49 - Sunday]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/30/week-49-sunday/"/>
    <updated>2013-11-30T21:47:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/30/week-49-sunday</id>
    <content type="html"><![CDATA[<p>This is a bit of a cheat since I started this on Saturday (week 48).</p>

<h2 id="todo">TODO</h2>

<p>I’m trying to use the workflow for this.</p>

<h2 id="comparing-distances">Comparing Distances</h2>

<p>I was able to finish making the figure as well as the text for the paper. I added all this information to the response letter.</p>

<h2 id="simulations">Simulations</h2>

<p>I ran this and need to double check. However, I do think that something went wrong here. I need to double check the code and spot check the effect sizes given. </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Plotting Images in R]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/30/plotting-images-in-r/"/>
    <updated>2013-11-30T19:37:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/30/plotting-images-in-r</id>
    <content type="html"><![CDATA[<p>Plotting images or 2D graphs are fairly easy in R. You can use the <code>image</code> function or for a nice legend, you can also use the <code>image.plot</code> function in the <code>fields</code> package. The issue I encountered for the <code>image.plot</code> function is when I want the labels from my x and y axis to be different due to a log transformation. This was not possible.</p>

<p>To get around this issue, I adapted the <code>image.plot</code> function as <code>image.zplot</code> and added the following lines that are run right after the image is generated.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="r"><span class="line"><span class="kr">if</span> <span class="p">(</span><span class="o">!</span>is.null<span class="p">(</span>axis.x<span class="p">))</span> <span class="p">{</span>
</span><span class="line">  axis.x<span class="o">$</span>side <span class="o">&lt;-</span> <span class="m">1</span>
</span><span class="line">  do.call<span class="p">(</span><span class="s">&quot;axis&quot;</span><span class="p">,</span> axis.x<span class="p">)</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line"><span class="kr">if</span> <span class="p">(</span><span class="o">!</span>is.null<span class="p">(</span>axis.y<span class="p">))</span> <span class="p">{</span>
</span><span class="line">  axis.y<span class="o">$</span>side <span class="o">&lt;-</span> <span class="m">2</span>
</span><span class="line">  do.call<span class="p">(</span><span class="s">&quot;axis&quot;</span><span class="p">,</span> axis.y<span class="p">)</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">box<span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This also adds the <code>axis.x</code> and <code>axis.y</code> options, which are options with list as the argument. The list entries are the key/value pairs when calling the particular axis function. This means I can use my custom log transformed x and y axis but provide the original numbers.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wednesday - Week 48]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/27/wednesday-week-48/"/>
    <updated>2013-11-27T11:42:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/27/wednesday-week-48</id>
    <content type="html"><![CDATA[<h2 id="summary">Summary</h2>

<p>I was able to get through at least at a partial level through the items in my TOOD list below. More details are of course below.</p>

<ul>
  <li>I checked the CPAC output and found that 1 subject had some missing preprocessing data due to a crash during nuisance. I am rerunning this subject on rocky so I can examine the working directory (it said a tissue prior was missing in the error). Everything else is good. It should be noted that there are 2 participants in OHSU that are missing the 1st scan but have 2 other scans.</li>
  <li>I reran the QC and the output pages now seem good. Need to decide the procedure for QC now.</li>
  <li>Looked into eigenvector centrality speedup. There is code to do the power iteration approach that is advocated in the Brain Connectivity paper with 1000x speedup in a fast eigenvector centrality approach. Could also email the people from that paper to ask for their code.</li>
  <li>Discussed updating VMHC to have Cameron’s fixes (register regular MNI brain to symmetric MNI brain) with Steve. Also, discussed having the registration step be done in the assigned standard anatomical resolution and then apply the warp from this registration to the native functional image into the assigned standard functional resolution. This is not currently done.</li>
  <li>Did tagging for about 30 minutes.</li>
  <li>Created TODO items for ABIDE.</li>
</ul>

<h2 id="plans">Plans</h2>

<ul>
  <li>ABIDE
    <ul>
      <li>Create TODOs</li>
      <li>Finish checking CPAC output</li>
      <li>Fix QC pages</li>
      <li>Look into fixing VMHC error when running at 3mm</li>
      <li>Look into eigenvector centrality</li>
      <li>Create quick packs for CPAC and Xinian</li>
    </ul>
  </li>
  <li>Do 1 hour of tagging</li>
</ul>

<h2 id="abide---todo">ABIDE - TODO</h2>

<p>This is a partial list and the next time I will need to make a more thorough list.</p>

<ul>
  <li>Do QC</li>
  <li>Check all outputs there for Pierre, Xinian, Chao-Gan</li>
  <li>Optimize the eigenvector centrality</li>
  <li>Make sure VMHC works properly when using 3mm output resolution</li>
  <li>Setup Quick Packs</li>
  <li>Run Quick Packs</li>
</ul>

<h2 id="abide---check-outputs">ABIDE - Check Outputs</h2>

<p>I made a script <code>20_check_subjects.R</code> in <code>.../config/24_check</code> that checks which subject is missing some preprocessing output given that the raw data exists.</p>

<p>For two subjects, we are missing the first rest scan. These subjects are from OHSU and are <code>0050155</code> and <code>0050165</code>.</p>

<p>We need to redo <code>0051275</code> since there was a failure in calculating the nuisance regression. I ran this subject again on rocky with the relevant info/scripts in <code>/data/Projects/ABIDE_Initiative/CPAC/abide/config/26_reprocess_3mm</code>.</p>

<h2 id="abide---qc">ABIDE - QC</h2>

<p>Simply rerunning the QC resolved some broken links and the QC pages appear fine. Need to set some system of getting through all these QC pages and confirm the protocol for checking.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">CPAC</span>
</span><span class="line"><span class="n">CPAC</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">create_all_qc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s">&#39;/path/to/output_directory&#39;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="vmhc">VMHC</h2>

<p>Currently there is a bit of an issue when we try to run VMHC at 3mm. Although it now works, the registration appears to to be done at that resolution (3mm). Talking to Steve, it seems a fix proposed by Cameron would resolve my issue as well. That is, the registration between the regular MNI to the symmetric MNI would be done in the anatomical resolution, which is usually specified at 1mm. Then, this warp would be combined with the previous one to standard space, and we would apply this warp to the functional in order to get to the specified standard functional space, which is in 3mm.</p>

<h2 id="eigen-vector-centrality">Eigen-Vector Centrality</h2>

<p>I did a brief search of faster implementations of eigenvector centrality, which is also sometimes referred to as google’s page rank.</p>

<h3 id="paper-with-fast-implementation">Paper with Fast Implementation</h3>

<p>The most relevant item is a paper recommended by Cameron: http://online.liebertpub.com.ezproxy.med.nyu.edu/doi/full/10.1089/brain.2012.0087. It appears their analyses were done in matlab so if we could also get their code, it would be fairly easy to integrate with CPAC. Below is a quote from the paper that indicates this approach is very fast, potentially faster than degree centrality! Note that they used a voxel size of 2mm.</p>

<blockquote>
  <p>The RS-fMRI data of each subject contain 195,704 in-brain voxels per volume and the time-series length is 200. The approximate gain in efficiency compared to the standard algorithm is a factor 1000. The computation times for ECM of the fMRI (excluding file I/O) are 39 sec on average (std. dev. 14 sec) on an Intel Xeon.</p>
</blockquote>

<p>So figure out if we should contact them or not?</p>

<h3 id="related-links">Related Links</h3>

<p>The above paper used the power iteration method to calculate the dominant eigenvector. I found various links on the code to run this power iteration method.</p>

<ul>
  <li>Some slides with well explained code. http://homepages.math.uic.edu/~jan/mcs507/numpyveclinalg.pdf</li>
  <li>Easy to follow function although needs to be fixed: https://github.com/seckcoder/mmd/blob/9aba8a224c69daebc6f3f800e3847362911f2225/dimension_reduction.py</li>
  <li>Super straightforward code: http://stackoverflow.com/questions/13739186/compute-eigenvector-using-a-dominant-eigenvalue</li>
  <li>4 ways to compute: http://glowingpython.blogspot.com/2011/05/four-ways-to-compute-google-pagerank.html</li>
</ul>

<h2 id="tagging">Tagging</h2>

<p>I think I should go through and check the first 45 that I did. I believe that I may not have properly added the type of connectivity (seed-based, unsupervised, etc) properly.</p>

<p>Sent out an email with some issues/guidance on tagging.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tuesday - Week 48]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/26/tuesday-week-48/"/>
    <updated>2013-11-26T10:41:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/26/tuesday-week-48</id>
    <content type="html"><![CDATA[<h2 id="summary">Summary</h2>

<p>I was able to get through a fair amount of what I planned (next section). Here is what I did do:</p>

<ul>
  <li>Finished updating centrality code to be memory efficient</li>
  <li>Created a command-line tool to run said centrality code</li>
  <li>Ran memory test on final centrality code. The memory limit that a user sets is roughly (within 50MB) the amount that is actually used.</li>
  <li>Found path to Yang’s QC report for ABIDE</li>
  <li>Finished writing script to check what CPAC outputs exist. Most of the data appear to have been written with only a few missing data.</li>
</ul>

<h2 id="plans">Plans</h2>

<ul>
  <li>Do final memory test for centrality code</li>
  <li>Create command-line centrality wrapper</li>
  <li>Finish 10mins (from yesterday) for CMI librarian tagging</li>
  <li>Find Yang’s old QCing and related PDF</li>
  <li>Get details on ABIDE preprocessing output (what completed and what didn’t)</li>
  <li>Start creating quick packs for different strategies (CPAC, Pierre, Xinian, and Chao-Gan).</li>
</ul>

<h3 id="questions">Questions</h3>

<ul>
  <li>How will quick pack work for Pierre’s data? Do we provide the identity matrix?</li>
</ul>

<h2 id="abide-processing-output">ABIDE Processing Output</h2>

<p>One thing to check is if all scans have been preprocessed (i.e., if a subject has 3 scans as is the case with OHSU). I could check to see what other’s have done. Hmm not a bad idea. First find subjects with more than one scan and then see what happens with these subjects. Yes, the OHSU data has more than 1 scan per participant (can be up to 3 scans). Our CPAC processing did produce output for all scans. Ok so Xinian and I think Chao-Gan did not actually preprocess the additional scans, but Pierre did. Also Xinian and Chao-Gan did not concatenate their data for those subjects as the number of time-points was the same.</p>

<p>I also finished creating a script that checks on the filtered and globally corrected data.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="r"><span class="line">df <span class="o">&lt;-</span> read.csv<span class="p">(</span><span class="s">&quot;zcheck_exists_filt_global.csv&quot;</span><span class="p">)</span>
</span><span class="line">rownames<span class="p">(</span>df<span class="p">)</span> <span class="o">&lt;-</span> df<span class="o">$</span>X
</span><span class="line">df <span class="o">&lt;-</span> df<span class="p">[,</span><span class="m">-1</span><span class="p">]</span>
</span><span class="line">nrow<span class="p">(</span>df<span class="p">)</span> <span class="o">-</span> colSums<span class="p">(</span>df<span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Here is the output below. We can see that we have most of the data but there are a couple of bad (possibly very bad) subject data.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class="r"><span class="line">                         alff                  anatomical_brain
</span><span class="line">                            <span class="m">3</span>                                 <span class="m">0</span>
</span><span class="line">          anatomical_reorient   anatomical_to_mni_nonlinear_xfm
</span><span class="line">                            <span class="m">0</span>                                 <span class="m">0</span>
</span><span class="line">              ants_affine_xfm                                dr
</span><span class="line">                            <span class="m">0</span>                                 <span class="m">3</span>
</span><span class="line">                        falff             functional_brain_mask
</span><span class="line">                            <span class="m">3</span>                                 <span class="m">2</span>
</span><span class="line">functional_brain_mask_to_standard          functional_freq_filtered
</span><span class="line">                            <span class="m">2</span>                                 <span class="m">3</span>
</span><span class="line">               functional_mni     functional_nuisance_residuals
</span><span class="line">                            <span class="m">3</span>                                 <span class="m">3</span>
</span><span class="line">functional_to_anat_linear_xfm                   mean_functional
</span><span class="line">                            <span class="m">2</span>                                 <span class="m">2</span>
</span><span class="line">    mni_normalized_anatomical                      preprocessed
</span><span class="line">                            <span class="m">0</span>                                 <span class="m">2</span>
</span><span class="line">                         reho
</span><span class="line">                            <span class="m">3</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="abide-qc">ABIDE QC</h2>

<p>I was able to find Yang’s QC report in <code>/data/Projects/ABIDE_Initiative/CPAC/QC</code>.</p>

<p>Didn’t get a chance to regenerate the QC outputs but I think I might want to do this after I resolve that all the subject’s have data output with CPAC.</p>

<h2 id="centrality">Centrality</h2>

<p>Details for the military tactics taken to resolving the memory overage issues in the centrality, see <a href="http://czarrar.github.io/blog/2013/11/25/updating-centrality">this page</a>.</p>

<h3 id="memory-test">Memory Test</h3>

<p>Below are the results of running <code>./memtopX ./x_test_memprof.py</code> in <code>.../config/test_quick_pack</code>. I set a 1GB memory limit here, which led to two blocks being used. It seems further down that it came close to the this limit with 1.05GB being used.</p>

<p>Note that I’ve used 4mm data in standard space here, although later I also test with similar results using 3mm data.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class="r"><span class="line">sorted nodes <span class="p">[</span><span class="m">0.0</span><span class="p">,</span> <span class="m">1.0</span><span class="p">]</span>
</span><span class="line">threshold_option <span class="o">--&gt;</span> <span class="m">0</span>
</span><span class="line">p_value <span class="o">-&gt;</span> <span class="m">0.266037302736</span>
</span><span class="line">r_value <span class="o">--&gt;</span>  <span class="m">0.0913878943802</span>
</span><span class="line">inside optimized_centraltity<span class="p">,</span> r_value <span class="o">-&gt;</span> <span class="m">0.0913878943802</span>
</span><span class="line">block_size <span class="o">-&gt;</span>  <span class="m">14236</span>
</span><span class="line">Setup Degree Output
</span><span class="line">Normalize TimeSeries
</span><span class="line">Computing centrality across <span class="m">18091</span> voxels
</span><span class="line">running block <span class="o">-&gt;</span> <span class="m">14236</span> <span class="m">0</span>
</span><span class="line">...correlating
</span><span class="line">...calculating degree
</span><span class="line">...removing correlation matrix
</span><span class="line">running block <span class="o">-&gt;</span> <span class="m">18091</span> <span class="m">14236</span>
</span><span class="line">...correlating
</span><span class="line">...calculating degree
</span><span class="line">...removing correlation matrix
</span><span class="line">timing<span class="o">:</span> <span class="m">32.47</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Here is the corresponding output of <code>mem.log</code> that recorded the memory usage for my script every 30 seconds.</p>

<p>File /Users/zarrar/Dropbox/Journal/octopress/source/downloads/code/cpac_centrality_memory.log could not be found</p>

<p>With the 3mm data, I set a memory limit of 4GB at first. This leads to a maximum usage of 4.03GB in the first block (it’s lower in the next block). Pretty nice! If I set a memory limit instead of 1GB, then I get a maximum usage of 1.03GB (consistent in most blocks). Score.</p>

<h3 id="command-line-tool">Command-Line Tool</h3>

<p>To make it easier to directly call the centrality code, I created a command-line interface. This is in <code>C-PAC/tools</code> and the script is called <code>cpac_centrality.py</code>. The auto-generated help is shown below.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
</pre></td><td class="code"><pre><code class="r"><span class="line">usage<span class="o">:</span> cpac_centrality.py <span class="p">[</span><span class="o">-</span>h<span class="p">]</span> <span class="o">-</span>i INPUT <span class="p">[</span><span class="o">-</span>m MASK<span class="p">]</span> <span class="p">[</span><span class="o">--</span>degree<span class="p">]</span> <span class="p">[</span><span class="o">--</span>eigen<span class="p">]</span>
</span><span class="line">                          <span class="p">[</span><span class="o">--</span>binarize<span class="p">]</span> <span class="p">[</span><span class="o">--</span>weighted<span class="p">]</span> <span class="p">[</span><span class="o">--</span>sparsity SPARSITY<span class="p">]</span>
</span><span class="line">                          <span class="p">[</span><span class="o">--</span>pvalue PVALUE<span class="p">]</span> <span class="p">[</span><span class="o">--</span>rho RHO<span class="p">]</span> <span class="p">[</span><span class="o">--</span>memlimit MEMLIMIT<span class="p">]</span>
</span><span class="line">                          <span class="p">[</span><span class="o">-</span>o OUTDIR<span class="p">]</span>
</span><span class="line">
</span><span class="line">Compute centrality <span class="kr">for</span> a given timeseries.
</span><span class="line">
</span><span class="line">optional arguments<span class="o">:</span>
</span><span class="line">  <span class="o">-</span>h<span class="p">,</span> <span class="o">--</span>help            show this help message and exit
</span><span class="line">  <span class="o">-</span>i INPUT<span class="p">,</span> <span class="o">--</span>input INPUT
</span><span class="line">                        Input timeseries data file
</span><span class="line">  <span class="o">-</span>m MASK<span class="p">,</span> <span class="o">--</span>mask MASK  Brain mask <span class="p">(</span>by default the program will create a mask
</span><span class="line">                        where voxels have non<span class="o">-</span>zero variance regardeless of the
</span><span class="line">                        user specified mask<span class="p">)</span>.
</span><span class="line">  <span class="o">--</span>degree              Calculate degree centrality
</span><span class="line">  <span class="o">--</span>eigen               Calculate eigen centrality
</span><span class="line">  <span class="o">--</span>binarize            For a given voxel<span class="p">,</span> save the number of connections that
</span><span class="line">                        pass a threshold
</span><span class="line">  <span class="o">--</span>weighted            For a given voxel<span class="p">,</span> save the sum of all connection
</span><span class="line">                        weights that pass a threshold.
</span><span class="line">  <span class="o">--</span>sparsity SPARSITY   Sparsity based threshold. <span class="p">(</span>Only one threshold option
</span><span class="line">                        can be specified.<span class="p">)</span>
</span><span class="line">  <span class="o">--</span>pvalue PVALUE       P<span class="o">-</span>value threshold <span class="kr">for</span> each connection. <span class="p">(</span>Only one
</span><span class="line">                        threshold option can be specified.<span class="p">)</span>
</span><span class="line">  <span class="o">--</span>rho RHO             Regular correlation threshold. <span class="p">(</span>Only one threshold
</span><span class="line">                        option can be specified.<span class="p">)</span>
</span><span class="line">  <span class="o">--</span>memlimit MEMLIMIT   Memory limit that should be set.
</span><span class="line">  <span class="o">-</span>o OUTDIR<span class="p">,</span> <span class="o">--</span>outdir OUTDIR
</span><span class="line">                        Output directory
</span></code></pre></td></tr></table></div></figure></notextile></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Updating Centrality]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/25/updating-centrality/"/>
    <updated>2013-11-25T10:26:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/25/updating-centrality</id>
    <content type="html"><![CDATA[<p>I’ll go through the different changes in the CPAC centrality code that I’ve implemented.</p>

<h2 id="normalized-timeseries">Normalized Timeseries</h2>

<p>The easiest change was to use normalized time series, which then allows one to compute the crossproduct of the time series to calculate the correlations. Below are the relevant code slices that call on functions used in CWAS.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">timeseries</span> <span class="o">=</span> <span class="n">norm_cols</span><span class="p">(</span><span class="n">timeseries</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span><span class="line"><span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">timeseries</span><span class="p">[:,</span><span class="n">j</span><span class="p">:</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">timeseries</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="block-size">Block Size</h2>

<p>We changed the block size to more accurately reflect the underlying operations. Although it should be noted that numpy has some poor memory handling as certain temporary objects will still be maintained in memory throwing off this calculation. However, later sections will show attempts to rectify this issue.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">nvoxs</span>   <span class="o">=</span> <span class="n">timeseries</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line"><span class="n">ntpts</span>   <span class="o">=</span> <span class="n">timeseries</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span><span class="line"><span class="n">nbytes</span>  <span class="o">=</span> <span class="n">timeseries</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">itemsize</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="n">memory_allocated</span><span class="p">:</span>
</span><span class="line">    <span class="n">memory_in_bytes</span> <span class="o">=</span> <span class="n">memory_allocated</span> <span class="o">*</span> <span class="mf">1024.0</span><span class="o">**</span><span class="mi">3</span>    <span class="c"># assume it is in GB</span>
</span><span class="line">    <span class="n">block_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span> <span class="n">memory_in_bytes</span><span class="o">/</span><span class="p">(</span><span class="n">nvoxs</span> <span class="o">*</span> <span class="n">nbytes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">ntpts</span><span class="o">*</span><span class="n">nbytes</span> <span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="n">block_size</span> <span class="o">&gt;</span> <span class="n">nvoxs</span><span class="p">:</span>
</span><span class="line">    <span class="n">block_size</span> <span class="o">=</span> <span class="n">nvoxs</span>
</span><span class="line"><span class="k">elif</span> <span class="n">block_size</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">    <span class="k">raise</span> <span class="ne">MemoryError</span><span class="p">(</span><span class="s">&quot; Not enough memory available to perform degree centrality&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Note that the calculation is also dynamic with the data type of the time series.</p>

<h2 id="no-more-nan-to-number">No More NaN to Number</h2>

<p>This function <code>np.nan_to_num</code> is applied on the correlation matrix to turn any NaNs to 0s. This will occur when a time-series is all 0s leading to an error of sorts when calculating the correlation as you will be dividing by zeros and what not. The easiest way to avoid this error is to remove any flat time-series. I do this in the <code>load</code> function that initially reads in the time-series and the mask data.</p>

<p>I create an additional mask that is specific to the data. This is then combined with the externally defined template mask for a final mask at a later step.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">datmask</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="o">!=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span><span class="line"><span class="n">mask</span>    <span class="o">=</span> <span class="n">nib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">template</span><span class="p">)</span><span class="o">.</span><span class="n">get_data</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="more-efficient-sum-of-thresholded-correlations">More Efficient Sum of Thresholded Correlations</h2>

<p>This particular step (below) is both slow and takes up a lot of memory. For instance the <code>corr_matrix &gt; r_value</code> must create a new copy of the matrix.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">corr_matrix</span> <span class="o">&gt;</span> <span class="n">r_value</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>One solution is to use cython. For now, I will do this without compiling the whole package and use <code>pyxthon</code> that will dynamically compile the new function at runtime. Below is the cython code.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">centrality_binarize_double</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">[</span><span class="n">double</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">]</span> <span class="n">cmat</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">[</span><span class="n">double</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">1</span><span class="p">]</span> <span class="n">cent</span><span class="p">,</span> <span class="n">double</span> <span class="n">thresh</span><span class="p">):</span>
</span><span class="line">    <span class="n">cdef</span> <span class="n">unsigned</span> <span class="nb">int</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">cmat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span><span class="line">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">cmat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
</span><span class="line">            <span class="n">cent</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">cent</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="o">*</span><span class="p">(</span><span class="n">cmat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thresh</span><span class="p">)</span>
</span><span class="line"><span class="c">#</span>
</span><span class="line"><span class="k">def</span> <span class="nf">centrality_weighted_double</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">[</span><span class="n">double</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">]</span> <span class="n">cmat</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">[</span><span class="n">double</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">1</span><span class="p">]</span> <span class="n">cent</span><span class="p">,</span> <span class="n">double</span> <span class="n">thresh</span><span class="p">):</span>
</span><span class="line">    <span class="n">cdef</span> <span class="n">unsigned</span> <span class="nb">int</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">cmat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span><span class="line">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">cmat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
</span><span class="line">            <span class="n">cent</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">cent</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">cmat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">cmat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thresh</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I created a wrapper centrality function that is fairly straightforward. It allows one to access all the different C functions through one python function. I also wrote 2 tests that both passed. Here’s one example to illustrate the new functionality (<code>comp</code>) relative to the old (<code>ref</code>).</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">test_centrality_binarize</span><span class="p">():</span>
</span><span class="line">    <span class="k">print</span> <span class="s">&quot;testing centrality binarize&quot;</span>
</span><span class="line">
</span><span class="line">    <span class="n">method</span>      <span class="o">=</span> <span class="s">&quot;binarize&quot;</span>
</span><span class="line">    <span class="n">nblock</span>      <span class="o">=</span> <span class="mi">20</span>
</span><span class="line">    <span class="n">nvoxs</span>       <span class="o">=</span> <span class="mi">100</span>
</span><span class="line">    <span class="n">r_value</span>     <span class="o">=</span> <span class="mf">0.2</span>
</span><span class="line">    <span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">nblock</span><span class="p">,</span> <span class="n">nvoxs</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="n">ref</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">&gt;</span><span class="n">r_value</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">    <span class="n">comp</span> <span class="o">=</span> <span class="n">centrality</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span> <span class="n">r_value</span><span class="p">,</span> <span class="n">method</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="n">assert_equal</span><span class="p">(</span><span class="n">ref</span><span class="p">,</span> <span class="n">comp</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="deleting-temporary-correlation-matrix">Deleting Temporary Correlation Matrix</h2>

<p>The correlation matrix created for each block loop will not be freed, hence there is a build up of these matrices in memory. A simple fix for this is to delete the correlation matrix at the end of each loop. It is then freed when the next loop iteration begins.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Monday - Week 48]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/25/monday-week-48/"/>
    <updated>2013-11-25T09:59:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/25/monday-week-48</id>
    <content type="html"><![CDATA[<h2 id="plans">Plans</h2>

<p>Today, I planned to work on…see summary section for what actually got done.</p>

<ul>
  <li>finish memory fixes to centrality code</li>
  <li>checking on the 3mm registration of ABIDE</li>
  <li>setup quick pack for Pierre &amp; Xinian’s data</li>
  <li>setup quick pack for ABIDE data (centrality and VMHC)</li>
</ul>

<h2 id="summary">Summary</h2>

<p>I wasn’t able to get through a lot that was planned. I worked on the following (described more later):</p>

<ul>
  <li>Centrality Coding</li>
  <li>Checking paths for ABIDE preprocessing</li>
</ul>

<h2 id="centrality-code">Centrality Code</h2>

<p>I updated the centrality code with several fixes and cleaned up some of the functions. Details of this work can be found on this <a href="http://czarrar.github.io/blog/2013/11/25/update-centrality">other post</a>.</p>

<h2 id="qc">QC</h2>

<p>The 3mm registration is done. I want to now take a closer look at the data. I want to take a look at a few things:</p>

<ul>
  <li>Are all the outputs there? Maybe make some type of table or record of what is good with what?</li>
  <li>Can we fix up the QC pages?</li>
  <li>Do the images look good?</li>
</ul>

<p>I began to create the code to find the needed paths in the preprocessing directory. This can be found in <code>/data/Projects/ABIDE_Initiative/CPAC/abide/config/24_check/10_check_path.py</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Timing Comparison]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/24/timing-comparison/"/>
    <updated>2013-11-24T15:21:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/24/timing-comparison</id>
    <content type="html"><![CDATA[<h2 id="overview">Overview</h2>

<p>I will be comparing the timing for the following functions:</p>

<ul>
  <li>Degree Centrality</li>
  <li>GLM</li>
  <li>MDMR</li>
  <li>SVM (linear)</li>
  <li>K-Means Clustering</li>
</ul>

<p>This order also reflects the speed result (first is faster).</p>

<h2 id="parameters">Parameters</h2>

<p>I examined the speed for examining connectivity-phenotype relationships for the following parameters:</p>

<ul>
  <li>number of nodes or connectivity maps: <strong>10</strong></li>
  <li>number of elements in each connectivity map: <strong>800</strong></li>
  <li>number of participants: <strong>100</strong></li>
  <li>number of iterations: <strong>100</strong></li>
</ul>

<p>In the case of SVM and K-Means, I did the analysis like I did with MDMR. I examined the fit between one connectivity map per participant and participant group membership. Thus, the analysis was repeated 10 times, one for each connectivity map.</p>

<p>This whole process was repeated 100 times.</p>

<h2 id="system">System</h2>

<p>The timing analysis was performed on a MacBook Pro OSX v10.8 using a 2.53GHz Intel Core 2 Duo with 4GB of RAM.</p>

<h2 id="results">Results</h2>

<p>A graph shows the average timing for each approach across 100 iterations below.</p>

<script type="text/javascript" src="https://www.google.com/jsapi"></script>

<script type="text/javascript">
  google.load("visualization", "1", {packages:["corechart"]});
  google.setOnLoadCallback(drawChart);
  function drawChart() {
    var data = google.visualization.arrayToDataTable([
      ['Method', 'Time'],
      ['Degree',  6],
      ['GLM',  		30],
      ['MDMR',  	52],
      ['SVM',  		229], 
		 ['K-Means', 788]
    ]);

    var options = {
      title: 'Average time (ms) to compute connectivity-phenotype association',
      hAxis: {title: 'Method'}, 
		 legend: {position: 'none'}
    };

    var chart = new google.visualization.ColumnChart(document.getElementById('chart_div'));
    chart.draw(data, options);
  }
</script>

<div id="chart_div" style="width: 500px; height: 300px;"></div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Similarity of IQ CWAS Across Scans]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/24/similarity-of-iq-cwas-across-scans/"/>
    <updated>2013-11-24T11:51:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/24/similarity-of-iq-cwas-across-scans</id>
    <content type="html"><![CDATA[<p>Get the overlap (dice) and similarity (spearman) of the IQ CWAS between scans, and estimate the significance. Below are the current results for a simple comparison.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">measures</th>
      <th>values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">dice</td>
      <td>0.2731</td>
    </tr>
    <tr>
      <td style="text-align: right">pearson</td>
      <td>0.2571</td>
    </tr>
    <tr>
      <td style="text-align: right">spearman</td>
      <td>0.2511</td>
    </tr>
    <tr>
      <td style="text-align: right">kendalls</td>
      <td>0.1695</td>
    </tr>
  </tbody>
</table>

<p>Let’s focus on just the dice and spearman for now. For a proper permutation test, I want to make sure that the permuted subject indices are the same for scan 1 and 2. This would allow one to test the hypothesis of how similar the CWAS results for IQ are between scans when given a random list of participants.</p>

<h2 id="re-running-mdmr-for-scan-2">Re-Running MDMR for Scan 2</h2>

<p>This understanding mean I need to rerun the MDMR for scan 2 using the permutation indices from scan 1. I have created a new script to run these modified MDMR analyses for scan 2.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">cd /home2/data/Projects/CWAS/share/nki/05_cwas
</span><span class="line">./30_mdmr_using_perms.bash</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="permutations-for-dice-and-spearman">Permutations for dice and spearman</h2>

<p>Now we need to take the extra time to gather p-values for everything so we can threshold and compare. I’ll use some of the false positive code <code>share/results/20_cwas_iq/40_false_positives.R</code> in getting the p-values of the permutations. The result of this inspiration can be found in <code>/home2/data/Projects/CWAS/share/figures/fig_03</code> where I created the <code>E2_compare_scans.R</code> file. This script does the following (among other things):</p>

<ol>
  <li>Read in the permuted Fstats</li>
  <li>Loop through each permutation so you can</li>
  <li>Calculate significance for both scans</li>
  <li>Compare the two scans with dice (p &lt; 0.05) and spearman</li>
  <li>Return this result and loop to #3 for the next permutation</li>
</ol>

<p>Note that I’m not clustering correcting for the dice comparison and I’m using p-values here instead of F-stats.</p>

<h2 id="results">Results</h2>

<p>Highly significant with dice = 0.2731411 (p &lt; 0.001 or p = 0.0003) and spearman rho = 0.2510524 (p &lt; 0.005 or p = 0.004).</p>

<h2 id="update-connectir">Update Connectir</h2>

<p>To do this analysis, I updated the MDMR script to accept an external permutation file. This is with the new option <code>--permfiles</code>. It accepts a list of descriptor files (comma separated) that should reflect the number of factors2perm.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Saturday - Week 47]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/23/saturday-week-47/"/>
    <updated>2013-11-23T13:54:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/23/saturday-week-47</id>
    <content type="html"><![CDATA[<p>This page covers both Saturday and Sunday (which is technically week 48).</p>

<h2 id="todo">TODO</h2>

<ul>
  <li><strong>Permutation test for dice and spearman with IQ CWAS</strong>
    <ul>
      <li><em>DONE</em></li>
    </ul>
  </li>
  <li><strong>Compare computational time for MDMR to other approaches</strong>
    <ul>
      <li><em>DONE</em></li>
    </ul>
  </li>
  <li><strong>Bootstrap analyses</strong>
    <ul>
      <li><em>DONE</em></li>
    </ul>
  </li>
  <li><strong>Compare different distances</strong>
    <ul>
      <li><em>DONE WITH TABLES</em></li>
      <li><em>GENERATE SURFACE RESULTS</em></li>
      <li><em>WRITE TEXT FOR PAPER</em></li>
      <li><em>UPDATE RESPONSE</em></li>
    </ul>
  </li>
  <li><strong>Simulations</strong>
    <ul>
      <li><em>WRITE CODE</em></li>
      <li><em>GENERATE PLOTS</em></li>
      <li><em>ADD TO RESPONSE LETTER</em></li>
    </ul>
  </li>
  <li><strong>Results</strong>
    <ul>
      <li><em>RE-COMPUTE WHERE NEEDED</em></li>
      <li><em>ADD TO PAPER</em></li>
    </ul>
  </li>
  <li><strong>Update paper with stuff from response letter</strong>
    <ul>
      <li><em>I guess waiting for feedback on response letter</em></li>
    </ul>
  </li>
</ul>

<h2 id="similarity-of-iq-cwas-across-scans">Similarity of IQ CWAS across scans</h2>

<p>See details at <a href="http://czarrar.github.io/2013/11/24/similarity-of-iq-cwas-across-scans/">my other post</a>.</p>

<p>Highly significant with dice = 0.2731411 (p &lt; 0.001 or p = 0.0003) and spearman rho = 0.2510524 (p &lt; 0.005 or p = 0.004).</p>

<h2 id="computational-time">Computational Time</h2>

<p>Below is the average time (ms) across 100 iterations. MDMR is a little slower than GLM but comparable.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">   method      time
1  Degree   5.76817
2     GLM  30.12167
3    MDMR  51.73251
4     SVM 228.89548
5 K-Means 788.41195</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Further details can be found on the <a href="http://czarrar.github.io/2013/11/24/timing-comparison/">following post</a>.</p>

<h2 id="comparing-distances">Comparing Distances</h2>

<p>Further details can be found on the <a href="http://czarrar.github.io/2013/11/09/comparing-distances">following post</a>.</p>

<h2 id="simulations">Simulations</h2>

<p>Getting a late start on the simulations. </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Friday - Week 47]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/22/friday-week-47/"/>
    <updated>2013-11-22T15:44:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/22/friday-week-47</id>
    <content type="html"><![CDATA[<h2 id="summary">Summary</h2>

<h3 id="centrality-and-memory">Centrality and Memory</h3>

<p>The conclusion from today’s work is that at three steps in the computation of centrality there is a huge spike in memory load that then stays there.</p>

<ol>
  <li>When computing the correlations, additional memory is allocated then that used for the output (up to 0.5x more).</li>
  <li>When converting any NaNs to 0s, more than 2x more than the current memory is used (I guess because a copy of the data is created but not wiped from memory at a later point).</li>
  <li>When calculating the degree centrality from the connectivity about 2x the amount of memory is used at this step.</li>
</ol>

<p>We actually can deal with step 2, since this only occurs because some time-series are flat or all zeros. If we mask those time series out in advance, then we don’t need to worry about that particular issue.</p>

<p>The third step might also be solved semi-easily with some cython. There is one nice tutorial/comparison here: http://technicaldiscovery.blogspot.com/2011/06/speeding-up-python-numpy-cython-and.html. We could then write a function that loops through the array, thresholds each element, and adds each element to a vector.</p>

<p>Further details are given below on examining memory issues in centrality using (1) 3mm brain and breakpoints and (2) a toy example to get more details.</p>

<h3 id="abide">ABIDE</h3>

<p>I began running CPAC to register the participant functional data and derivatives to 3mm space.</p>

<h2 id="memory-issues">Memory Issues</h2>

<p>There are a couple of nice pages on some memory issues with numpy. One suggestion was to upgrade to version 1.7.1+ where some memory leaks were fixed. I’m currently using version 1.6.1 so it’s possible using a more advanced version could help…however, I did check with a version of numpy Dan has and this didn’t solve my particular issues.</p>

<h2 id="detailed-memory-test">Detailed Memory Test</h2>

<p>I have put a few breakpoints in the centrality function to see where the memory blows up. I will record the memory usage based on <code>ps</code> at one of 4 points below. I really run the following command <code>./memtopX ./x_test_memprof.py</code>. I will be using 3mm data here.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">TODO</span><span class="p">:</span> <span class="n">breaks</span> <span class="n">pigments</span> <span class="n">will</span> <span class="nb">compile</span> <span class="n">elsewhere</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>block -&gt; 34382 0</p>

<ol>
  <li>93.88 MB</li>
  <li>139.12 MB</li>
  <li>6.45 GB (but note that before this point, memory goes up to 18.1GB)</li>
  <li>6.45 GB</li>
  <li>6.45 GB (but note that before this point, memory goes up to 13.75GB)</li>
</ol>

<p>block -&gt; 45262 34382</p>

<p>Also there was a second block so points 3-4 repeat. They both had 2.49 GB as that used.</p>

<p>Took about 324.71 secs or 5mins.</p>

<h2 id="for-our-sanity">For our Sanity</h2>

<p>So to confirm that we get sane numbers in python, one can run the following code and then monitor the memory externally with <code>./memtop</code>. Here, we will create a <code>corr_matrix</code> identical in size as in block 1 above. We find generally that our own calculation of this matrix is the same as that within python, which is the same as that using <code>memtop</code> (depends on <code>ps</code>). To get <code>memtop</code>, please follow this link: http://justinfranks.com/code/scripts/memtop.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="n">nvoxs</span> <span class="o">=</span> <span class="mi">34382</span>
</span><span class="line"><span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">nvoxs</span><span class="p">,</span> <span class="n">nvoxs</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float32&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="c"># Our assumption (4.4GB)</span>
</span><span class="line"><span class="n">size_us</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">nvoxs</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="c"># Using numpy (4.4GB)</span>
</span><span class="line"><span class="n">size_np</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="c"># From the command-line (4.45GB)</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">os</span>
</span><span class="line"><span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s">&quot;./memtop | grep ipython&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="understanding-memory-for-dot-product">Understanding memory for dot product</h2>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">sys</span>
</span><span class="line"><span class="n">sys</span><span class="o">.</span><span class="n">path</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/nibabel-1.3.0-py2.7.egg&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pydicom-0.9.8-py2.7.egg&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/networkx-1.7-py2.7.egg&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pysurfer-0.3.1-py2.7.egg&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86_64/lib/python27.zip&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86_64/lib/python2.7&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86_64/lib/python2.7/plat-linux2&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86_64/lib/python2.7/lib-tk&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86_64/lib/python2.7/lib-old&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86_64/lib/python2.7/lib-dynload&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Enthought/Canopy_64bit/User/lib/python2.7/site-packages&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Enthought/Canopy_64bit/System/lib/python2.7/site-packages&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Enthought/Canopy_64bit/System/lib/python2.7/site-packages/PIL&#39;</span><span class="p">,</span> <span class="s">&#39;/home2/dlurie/Canopy/appdata/canopy-1.0.3.1262.rh5-x86_64/lib/python2.7/site-packages&#39;</span><span class="p">]</span>
</span><span class="line">
</span><span class="line"><span class="c"># Note there is a base memory of 84.86  MB</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="n">npsize</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">nbytes</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="n">nvoxs</span> <span class="o">=</span> <span class="mi">10000</span>
</span><span class="line"><span class="n">ntpts</span> <span class="o">=</span> <span class="mi">150</span>
</span><span class="line">
</span><span class="line"><span class="c"># There is an increase to 100.7 MB with the time series</span>
</span><span class="line"><span class="c"># this is a little larger then it&#39;s 11.5MB size</span>
</span><span class="line"><span class="n">timeseries</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">nvoxs</span><span class="p">,</span><span class="n">ntpts</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</span><span class="line"><span class="n">timeseries</span><span class="p">[:,</span><span class="mi">100</span><span class="p">:</span><span class="mi">104</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line"><span class="c"># to 863.73 MB</span>
</span><span class="line"><span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nvoxs</span><span class="p">,</span><span class="n">nvoxs</span><span class="p">))</span>
</span><span class="line">
</span><span class="line"><span class="c"># to 1.35 GB (even after it is done)</span>
</span><span class="line"><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">timeseries</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="n">nvoxs</span><span class="p">],</span> <span class="n">timeseries</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">corr_matrix</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="c"># to 2.47 GB but stays at 2.09 GB</span>
</span><span class="line"><span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="c"># still at 2.09 GB</span>
</span><span class="line"><span class="n">degree_centrality_weighted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nvoxs</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="c"># to 3.68 GB but stays at 2.84 GB</span>
</span><span class="line"><span class="n">r_value</span> <span class="o">=</span> <span class="mf">0.2</span>
</span><span class="line"><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">*</span><span class="p">(</span><span class="n">corr_matrix</span> <span class="o">&gt;</span> <span class="n">r_value</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">degree_centrality_weighted</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="c"># Note that theoretically, we might have thought a limit of 0.75 GB</span>
</span><span class="line"><span class="p">(</span><span class="n">nvoxs</span><span class="o">*</span><span class="n">nvoxs</span> <span class="o">+</span> <span class="n">nvoxs</span> <span class="o">+</span> <span class="n">nvoxs</span><span class="o">*</span><span class="n">ntpts</span><span class="p">)</span><span class="o">*</span><span class="mi">8</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Below is the parallel commands that I ran to monitor the memory usage above.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">./memtop | grep ipython <span class="c"># get the pid</span>
</span><span class="line"><span class="nv">PID</span><span class="o">=</span>7873
</span><span class="line"><span class="k">while</span> <span class="o">[[</span> True <span class="o">]]</span>; <span class="k">do</span>
</span><span class="line">	./memtop -s -p <span class="nv">$PID</span>
</span><span class="line">	sleep 0.5
</span><span class="line"><span class="k">done</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="memory-leakage-links">Memory Leakage Links</h2>

<ul>
  <li>http://stackoverflow.com/questions/12422307/reducing-numpy-memory-footprint-in-long-running-application</li>
  <li>http://stackoverflow.com/questions/12461413/why-does-comparison-of-a-numpy-array-with-a-list-consume-so-much-memory</li>
  <li>http://stackoverflow.com/questions/15191391/how-to-avoid-this-four-line-memory-leak-with-numpymkl</li>
</ul>

<h2 id="running-centrality-on-abide">Running Centrality on ABIDE</h2>

<p>The usage approaches 18GB when I set a 12GB limit. On gelert, each node has about 16GB of RAM. The easiest thing might be to run one process on each node with a limit of 6GB to keep well within range.</p>

<h2 id="registering-data-to-3mm-space">Registering Data to 3mm Space</h2>

<p>I also want to redo the registration to be in 3mm standard space, whereas it currently is in 2mm. I’m working on the script for that task and have started running this on gelert now.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Thursday - Week 47]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/21/thursday-week-47/"/>
    <updated>2013-11-21T11:34:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/21/thursday-week-47</id>
    <content type="html"><![CDATA[<p>Today mostly revolved around memory profiling to work in python. Summary is as follows.</p>

<ul>
  <li>Looked into memory profiling
    <ul>
      <li>Had issues with memprof but got heapy to work (links below).</li>
      <li>Got memprof to work not through numpy</li>
      <li>Also applied memory_profiler (a little bit more accurate and useful than memprof).</li>
      <li>Finally also applied memtop (a wrapper around ps)</li>
    </ul>
  </li>
  <li>Attended lab meeting</li>
  <li>Discussion of ABIDE with Chao-Gan and then Cameron</li>
</ul>

<h2 id="memory-profiling">Memory Profiling</h2>

<p>Want to examine the memory usage of centrality in CPAC.</p>

<h3 id="choosing-a-package">Choosing a Package</h3>

<p>There are several python packages available with an extensive list available on stackoverflow: http://stackoverflow.com/questions/110259/which-python-memory-profiler-is-recommended.</p>

<p>Some that do not seem appropriate for our purposes:</p>

<ul>
  <li><a href="https://launchpad.net/meliae">Meliae</a>: No documentation.</li>
  <li><a href="http://pythonhosted.org/Pympler/muppy.html">Muppy</a>: Not that useful.</li>
</ul>

<p>One particular issue that I’ll be dealing with are how well it will work with nipype.</p>

<h3 id="path-issue">Path Issue</h3>

<p>The first problem I faced was the memory profiling not being on the path. This wasn’t so obvious because in the interactive shell of python and python, the memory profiling packages that were recently installed were available. However, when running python (the same python) from the command-line, then it couldn’t find it. The fix was simply to add my python site-packages to the path. Should know in the future that the paths for the interactive vs command-line python can be different.</p>

<h3 id="heapy">Heapy</h3>

<p>So when I actually ran memprof, it ended up failing. I went through all the other packages and was able to get <a href="http://smira.ru/wp-content/uploads/2011/08/heapy.html">heapy</a> to work. The approach taken with heapy is to call an individual function to measure the memory usage, whereas the other approaches all use a decorator of some kind. It worked, but the RAM values all seemed a bit low and not very convenient to read, so I tried another approach.</p>

<h3 id="memprof">Memprof</h3>

<p>Now I’ll be looking at <a href="http://jmdana.github.io/memprof">Memprof</a>, recommended by Cameron and also on the <a href="http://stackoverflow.com/questions/110259/which-python-memory-profiler-is-recommended">stackoverflow page</a>. Since it won’t work with nipype, I manually ran the <code>calc_centrality</code> function. This approach gave me some memory read outs. However, because it doesn’t actually record the memory usage of a particular command (when all the correlations are calculated), the maximum memory is about 160 + 11 or 171MB in total. This is a lot lower than I calculated and much lower than what I saw in top, which maxed out at around 4GB.</p>

<p>[add pics]</p>

<h3 id="memory-profiler">Memory Profiler</h3>

<p>The other approach <a href="https://pypi.python.org/pypi/memory_profiler">memory_profiler</a> was a bit more useful. It showed about 1.4gb being used at the peak (around the correlation calculation).</p>

<h3 id="memtop">Memtop</h3>

<p>I also used a command-line tool <a href="http://www.webhostingtalk.com/showthread.php?t=1256200">memtop</a> that wraps around the command-line tool <code>ps</code>. This showed 4.2gb being used at the peak.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Wednesday - Week 47]]></title>
    <link href="http://czarrar.github.io/blog/2013/11/20/wednesday-week-47/"/>
    <updated>2013-11-20T12:01:00-05:00</updated>
    <id>http://czarrar.github.io/blog/2013/11/20/wednesday-week-47</id>
    <content type="html"><![CDATA[<h2 id="todo">TODO</h2>

<h3 id="centrality">Centrality</h3>

<ul>
  <li>xx Look to using 4mm grey-matter mask</li>
  <li>xx Run speed test on new analysis</li>
  <li></li>
</ul>

<h2 id="centrality-1">Centrality</h2>

<p>There a few issues related to needing to changing the inputs to the centrality nipype node. Those where straightforward.</p>

<p>Now I’m having some weird bug where the centrality outputs are not being saved. Trying to debug at the <code>map_centrality_matrix</code> function. Ended up being a simple typo and returning the template_type instead of centrality file outputs from the <code>calc_centrality</code> command.</p>

<p>I ran a comparison with AFNI’s 3dTcorrMap. When looking at the binarized result, almost all the voxels were the same except 8 in this particular case. I have tried doing centrality manually for those 8 voxels and keep getting the values from CPAC, which are 1 higher than the values from AFNI. I tried different correlation approaches as well as removing any voxel’s with constant time-series. However, since the difference is so small (only 1 for the binarized results) and there are so few voxels involved (8/19+k), I’m leaving it right now as a to be continued.</p>

<p>I ‘ve also changed the way the correlations are calculated. Now the time-series will all be first normalized and then a dot product will be computed. This is in contrast, to doing both steps all at once for each block that the correlation is computed. So this will be a much greater speed boon when there are many blocks. In a small speed comparison using the normalization approach leads to a 2x improvement in speed.</p>

<p>Although the setting of the block size does seem off, I will leave it for now. The actual amount of RAM being used is less than the amount </p>

<blockquote>
  <p>There were initially a lot more differences but a small change in the code to correct for auto-correlations in centrality took care of that issue.</p>
</blockquote>
]]></content>
  </entry>
  
</feed>
